{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClipEncoder\n"
     ]
    }
   ],
   "source": [
    "import StableDiffusion.ModelConverter\n",
    "from StableDiffusion.DiffusionProcess import DiffusionProcess\n",
    "device = 'cuda'\n",
    "idleDevice = 'cpu'\n",
    "#diffusionDict = StableDiffusion.ModelConverter.load_from_standard_weights(input_file='./models/inkpunk-diffusion-v1.ckpt',\\\n",
    "#                                                            device = 'cuda')\n",
    "diffusionDict = StableDiffusion.ModelConverter.load_from_standard_weights(input_file='../models/sd15models/v1-5-pruned-emaonly.ckpt',\\\n",
    "                                                            device = 'cuda')\n",
    "clipWeights=diffusionDict['clip']\n",
    "diffusionWeights = diffusionDict['diffusion']\n",
    "vaeEncoderWeights = diffusionDict['encoder']\n",
    "vaeDecoderWeights = diffusionDict['decoder']\n",
    "\n",
    "\n",
    "\n",
    "import torch \n",
    "import StableDiffusion.VaeEncoder \n",
    "import StableDiffusion.VaeDecoder\n",
    "import StableDiffusion.ClipEncoder\n",
    "import StableDiffusion.DiffusionProcess\n",
    "import importlib\n",
    "importlib.reload(StableDiffusion.VaeEncoder)\n",
    "importlib.reload(StableDiffusion.VaeDecoder)\n",
    "importlib.reload(StableDiffusion.ClipEncoder)\n",
    "importlib.reload(StableDiffusion.DiffusionProcess)\n",
    "from StableDiffusion.VaeDecoder import VaeDecoder\n",
    "from StableDiffusion.VaeEncoder import VaeEncoder\n",
    "from StableDiffusion.ClipEncoder import ClipEncoder\n",
    "from StableDiffusion.DiffusionProcess import DiffusionProcess\n",
    "clipEncoder = ClipEncoder().to(device)\n",
    "vaeEncoder = VaeEncoder().to(device)\n",
    "vaeDecoder = VaeDecoder().to(device)\n",
    "diffusionProcess = DiffusionProcess().to(device)\n",
    "clipEncoder.load_state_dict(clipWeights,strict=True)\n",
    "vaeEncoder.load_state_dict(vaeEncoderWeights ,strict=True)\n",
    "vaeDecoder.load_state_dict(vaeDecoderWeights,strict=True)\n",
    "diffusionProcess.load_state_dict(diffusionWeights,strict=True)\n",
    "clipName = clipEncoder.__class__.__name__\n",
    "print(clipName)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clipEncoder = ClipEncoder().to(device)\n",
    "clipEncoderLora = ClipEncoder().to(device)\n",
    "clipEncoder.load_state_dict(clipWeights,strict=True)\n",
    "clipEncoderLora.load_state_dict(clipWeights, strict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import StableDiffusion.LoraLayer\n",
    "import importlib\n",
    "importlib.reload(StableDiffusion.LoraLayer)\n",
    "from StableDiffusion.LoraLayer import LoraLayer\n",
    "\n",
    "def injectLora(model,filterTuple=('in_proj','out_proj')):\n",
    "    for name,layer in model.named_modules():        \n",
    "        filterName =filterTuple\n",
    "        if name.endswith(filterName) and isinstance(layer,nn.Linear):            \n",
    "            partNames = name.split('.')\n",
    "            preName = '.'.join(partNames[:-1])\n",
    "            layerName = partNames[-1]\n",
    "            print(f'preName = {preName}')\n",
    "            if preName =='':\n",
    "                parent = model\n",
    "            else:\n",
    "                parent = model.get_submodule(preName)    \n",
    "            injectedLoraLayer = LoraLayer(layer,rank =8,alpha=16)\n",
    "            setattr(parent,layerName,injectedLoraLayer)\n",
    "    for name,params in model.named_parameters():\n",
    "        print(f' {name} {params.size()}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'lora' in 'loraA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " embedding.position_embedding torch.Size([77, 768])\n",
      " embedding.token_embedding.weight torch.Size([49408, 768])\n",
      " layers.0.layernorm_1.weight torch.Size([768])\n",
      " layers.0.layernorm_1.bias torch.Size([768])\n",
      " layers.0.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.0.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.0.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.0.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.0.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.0.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.0.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.0.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.0.layernorm_2.weight torch.Size([768])\n",
      " layers.0.layernorm_2.bias torch.Size([768])\n",
      " layers.0.linear_1.weight torch.Size([3072, 768])\n",
      " layers.0.linear_1.bias torch.Size([3072])\n",
      " layers.0.linear_2.weight torch.Size([768, 3072])\n",
      " layers.0.linear_2.bias torch.Size([768])\n",
      " layers.1.layernorm_1.weight torch.Size([768])\n",
      " layers.1.layernorm_1.bias torch.Size([768])\n",
      " layers.1.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.1.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.1.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.1.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.1.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.1.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.1.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.1.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.1.layernorm_2.weight torch.Size([768])\n",
      " layers.1.layernorm_2.bias torch.Size([768])\n",
      " layers.1.linear_1.weight torch.Size([3072, 768])\n",
      " layers.1.linear_1.bias torch.Size([3072])\n",
      " layers.1.linear_2.weight torch.Size([768, 3072])\n",
      " layers.1.linear_2.bias torch.Size([768])\n",
      " layers.2.layernorm_1.weight torch.Size([768])\n",
      " layers.2.layernorm_1.bias torch.Size([768])\n",
      " layers.2.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.2.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.2.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.2.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.2.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.2.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.2.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.2.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.2.layernorm_2.weight torch.Size([768])\n",
      " layers.2.layernorm_2.bias torch.Size([768])\n",
      " layers.2.linear_1.weight torch.Size([3072, 768])\n",
      " layers.2.linear_1.bias torch.Size([3072])\n",
      " layers.2.linear_2.weight torch.Size([768, 3072])\n",
      " layers.2.linear_2.bias torch.Size([768])\n",
      " layers.3.layernorm_1.weight torch.Size([768])\n",
      " layers.3.layernorm_1.bias torch.Size([768])\n",
      " layers.3.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.3.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.3.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.3.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.3.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.3.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.3.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.3.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.3.layernorm_2.weight torch.Size([768])\n",
      " layers.3.layernorm_2.bias torch.Size([768])\n",
      " layers.3.linear_1.weight torch.Size([3072, 768])\n",
      " layers.3.linear_1.bias torch.Size([3072])\n",
      " layers.3.linear_2.weight torch.Size([768, 3072])\n",
      " layers.3.linear_2.bias torch.Size([768])\n",
      " layers.4.layernorm_1.weight torch.Size([768])\n",
      " layers.4.layernorm_1.bias torch.Size([768])\n",
      " layers.4.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.4.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.4.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.4.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.4.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.4.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.4.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.4.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.4.layernorm_2.weight torch.Size([768])\n",
      " layers.4.layernorm_2.bias torch.Size([768])\n",
      " layers.4.linear_1.weight torch.Size([3072, 768])\n",
      " layers.4.linear_1.bias torch.Size([3072])\n",
      " layers.4.linear_2.weight torch.Size([768, 3072])\n",
      " layers.4.linear_2.bias torch.Size([768])\n",
      " layers.5.layernorm_1.weight torch.Size([768])\n",
      " layers.5.layernorm_1.bias torch.Size([768])\n",
      " layers.5.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.5.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.5.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.5.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.5.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.5.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.5.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.5.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.5.layernorm_2.weight torch.Size([768])\n",
      " layers.5.layernorm_2.bias torch.Size([768])\n",
      " layers.5.linear_1.weight torch.Size([3072, 768])\n",
      " layers.5.linear_1.bias torch.Size([3072])\n",
      " layers.5.linear_2.weight torch.Size([768, 3072])\n",
      " layers.5.linear_2.bias torch.Size([768])\n",
      " layers.6.layernorm_1.weight torch.Size([768])\n",
      " layers.6.layernorm_1.bias torch.Size([768])\n",
      " layers.6.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.6.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.6.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.6.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.6.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.6.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.6.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.6.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.6.layernorm_2.weight torch.Size([768])\n",
      " layers.6.layernorm_2.bias torch.Size([768])\n",
      " layers.6.linear_1.weight torch.Size([3072, 768])\n",
      " layers.6.linear_1.bias torch.Size([3072])\n",
      " layers.6.linear_2.weight torch.Size([768, 3072])\n",
      " layers.6.linear_2.bias torch.Size([768])\n",
      " layers.7.layernorm_1.weight torch.Size([768])\n",
      " layers.7.layernorm_1.bias torch.Size([768])\n",
      " layers.7.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.7.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.7.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.7.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.7.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.7.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.7.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.7.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.7.layernorm_2.weight torch.Size([768])\n",
      " layers.7.layernorm_2.bias torch.Size([768])\n",
      " layers.7.linear_1.weight torch.Size([3072, 768])\n",
      " layers.7.linear_1.bias torch.Size([3072])\n",
      " layers.7.linear_2.weight torch.Size([768, 3072])\n",
      " layers.7.linear_2.bias torch.Size([768])\n",
      " layers.8.layernorm_1.weight torch.Size([768])\n",
      " layers.8.layernorm_1.bias torch.Size([768])\n",
      " layers.8.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.8.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.8.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.8.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.8.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.8.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.8.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.8.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.8.layernorm_2.weight torch.Size([768])\n",
      " layers.8.layernorm_2.bias torch.Size([768])\n",
      " layers.8.linear_1.weight torch.Size([3072, 768])\n",
      " layers.8.linear_1.bias torch.Size([3072])\n",
      " layers.8.linear_2.weight torch.Size([768, 3072])\n",
      " layers.8.linear_2.bias torch.Size([768])\n",
      " layers.9.layernorm_1.weight torch.Size([768])\n",
      " layers.9.layernorm_1.bias torch.Size([768])\n",
      " layers.9.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.9.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.9.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.9.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.9.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.9.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.9.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.9.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.9.layernorm_2.weight torch.Size([768])\n",
      " layers.9.layernorm_2.bias torch.Size([768])\n",
      " layers.9.linear_1.weight torch.Size([3072, 768])\n",
      " layers.9.linear_1.bias torch.Size([3072])\n",
      " layers.9.linear_2.weight torch.Size([768, 3072])\n",
      " layers.9.linear_2.bias torch.Size([768])\n",
      " layers.10.layernorm_1.weight torch.Size([768])\n",
      " layers.10.layernorm_1.bias torch.Size([768])\n",
      " layers.10.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.10.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.10.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.10.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.10.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.10.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.10.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.10.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.10.layernorm_2.weight torch.Size([768])\n",
      " layers.10.layernorm_2.bias torch.Size([768])\n",
      " layers.10.linear_1.weight torch.Size([3072, 768])\n",
      " layers.10.linear_1.bias torch.Size([3072])\n",
      " layers.10.linear_2.weight torch.Size([768, 3072])\n",
      " layers.10.linear_2.bias torch.Size([768])\n",
      " layers.11.layernorm_1.weight torch.Size([768])\n",
      " layers.11.layernorm_1.bias torch.Size([768])\n",
      " layers.11.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.11.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.11.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.11.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.11.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.11.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.11.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.11.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.11.layernorm_2.weight torch.Size([768])\n",
      " layers.11.layernorm_2.bias torch.Size([768])\n",
      " layers.11.linear_1.weight torch.Size([3072, 768])\n",
      " layers.11.linear_1.bias torch.Size([3072])\n",
      " layers.11.linear_2.weight torch.Size([768, 3072])\n",
      " layers.11.linear_2.bias torch.Size([768])\n",
      " layernorm.weight torch.Size([768])\n",
      " layernorm.bias torch.Size([768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0139,  0.0147, -0.0089,  ..., -0.0349, -0.0042, -0.0188],\n",
       "        [-0.0586, -0.0059, -0.0179,  ...,  0.0012, -0.0068,  0.0254],\n",
       "        [-0.0211, -0.0321,  0.0308,  ..., -0.0189,  0.0091,  0.0066],\n",
       "        ...,\n",
       "        [-0.0217, -0.0089, -0.0143,  ..., -0.0153,  0.0053,  0.0016],\n",
       "        [-0.0086, -0.0083, -0.0049,  ...,  0.0208, -0.0048, -0.0041],\n",
       "        [-0.0087, -0.0024,  0.0105,  ..., -0.0037, -0.0148,  0.0030]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "injectLora(clipEncoderLora,('in_proj','out_proj'))\n",
    "clipLoraDict = dict(clipEncoderLora.named_parameters())\n",
    "clipDict = dict(clipEncoder.named_parameters())\n",
    "\n",
    "clipLoraDict['layers.0.attention.in_proj.originLinear.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0139,  0.0147, -0.0089,  ..., -0.0349, -0.0042, -0.0188],\n",
       "        [-0.0586, -0.0059, -0.0179,  ...,  0.0012, -0.0068,  0.0254],\n",
       "        [-0.0211, -0.0321,  0.0308,  ..., -0.0189,  0.0091,  0.0066],\n",
       "        ...,\n",
       "        [-0.0217, -0.0089, -0.0143,  ..., -0.0153,  0.0053,  0.0016],\n",
       "        [-0.0086, -0.0083, -0.0049,  ...,  0.0208, -0.0048, -0.0041],\n",
       "        [-0.0087, -0.0024,  0.0105,  ..., -0.0037, -0.0148,  0.0030]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clipDict['layers.0.attention.in_proj.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import StableDiffusion.LoraLayer\n",
    "import importlib\n",
    "importlib.reload(StableDiffusion.LoraLayer)\n",
    "from StableDiffusion.LoraLayer import LoraLayer\n",
    "\n",
    "\n",
    "\n",
    "clipLayersDict = dict(clipEncoder.named_modules())\n",
    "rawLinear = clipLayersDict['layers.11.attention.out_proj']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Parameter Name: loraA\n",
      "Shape: torch.Size([768, 8])\n",
      "Requires Grad: True\n",
      "------------------------------\n",
      "Parameter Name: loraB\n",
      "Shape: torch.Size([8, 768])\n",
      "Requires Grad: True\n",
      "------------------------------\n",
      "Parameter Name: originLinear.weight\n",
      "Shape: torch.Size([768, 768])\n",
      "Requires Grad: False\n",
      "------------------------------\n",
      "Parameter Name: originLinear.bias\n",
      "Shape: torch.Size([768])\n",
      "Requires Grad: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LoraLayer(\n",
       "  (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora1 = LoraLayer(rawLinear,rank = 8,alpha =16)\n",
    "for name, param in lora1.named_parameters():\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Parameter Name: {name}\")\n",
    "    print(f\"Shape: {param.shape}\")\n",
    "    print(f\"Requires Grad: {param.requires_grad}\")\n",
    "    # print(param.data) # Uncomment this if you want to see the actual numbers\n",
    "\n",
    "lora1.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "2.0\n",
      "lora1.loraA  Parameter containing:\n",
      "tensor([[ 0.0962,  0.0491, -0.1887,  ..., -0.3529, -0.0487,  0.2995],\n",
      "        [ 0.0673,  0.0200,  0.0351,  ...,  0.5498,  0.2075, -0.2122],\n",
      "        [ 0.4916, -0.4762,  0.4006,  ..., -0.2131, -0.0747,  0.2232],\n",
      "        ...,\n",
      "        [-0.1283,  0.1568, -0.2501,  ...,  0.0059,  0.2656, -0.1761],\n",
      "        [-0.0360,  0.3712, -0.1295,  ..., -0.2958, -0.2810,  0.1501],\n",
      "        [ 0.2324, -0.2738,  0.1563,  ...,  0.4901, -0.2719, -0.2658]],\n",
      "       device='cuda:0', requires_grad=True) torch.Size([768, 8])\n",
      "lora1.loraB  Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)  torch.Size([8, 768])\n"
     ]
    }
   ],
   "source": [
    "print(lora1.alpha)\n",
    "print(lora1.rank)\n",
    "print(lora1.scale)\n",
    "print(f'lora1.loraA  {lora1.loraA} {lora1.loraA.shape}')\n",
    "print(f'lora1.loraB  {lora1.loraB}  {lora1.loraB.shape}')\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:  | Type: <class 'StableDiffusion.ClipEncoder.ClipEncoder'>\n",
      "Name: embedding | Type: <class 'StableDiffusion.ClipEncoder.ClipEmbedding'>\n",
      "Name: embedding.token_embedding | Type: <class 'torch.nn.modules.sparse.Embedding'>\n",
      "Name: layers | Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "Name: layers.0 | Type: <class 'StableDiffusion.ClipEncoder.ClipEncoderLayer'>\n",
      "Name: layers.0.layernorm_1 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.0.attention | Type: <class 'StableDiffusion.Attention.MHSelfAttention'>\n",
      "Name: layers.0.attention.in_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.0.attention.in_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.0.attention.out_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.0.attention.out_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.0.layernorm_2 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.0.linear_1 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.0.linear_2 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.1 | Type: <class 'StableDiffusion.ClipEncoder.ClipEncoderLayer'>\n",
      "Name: layers.1.layernorm_1 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.1.attention | Type: <class 'StableDiffusion.Attention.MHSelfAttention'>\n",
      "Name: layers.1.attention.in_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.1.attention.in_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.1.attention.out_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.1.attention.out_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.1.layernorm_2 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.1.linear_1 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.1.linear_2 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.2 | Type: <class 'StableDiffusion.ClipEncoder.ClipEncoderLayer'>\n",
      "Name: layers.2.layernorm_1 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.2.attention | Type: <class 'StableDiffusion.Attention.MHSelfAttention'>\n",
      "Name: layers.2.attention.in_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.2.attention.in_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.2.attention.out_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.2.attention.out_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.2.layernorm_2 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.2.linear_1 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.2.linear_2 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.3 | Type: <class 'StableDiffusion.ClipEncoder.ClipEncoderLayer'>\n",
      "Name: layers.3.layernorm_1 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.3.attention | Type: <class 'StableDiffusion.Attention.MHSelfAttention'>\n",
      "Name: layers.3.attention.in_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.3.attention.in_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.3.attention.out_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.3.attention.out_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.3.layernorm_2 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.3.linear_1 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.3.linear_2 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.4 | Type: <class 'StableDiffusion.ClipEncoder.ClipEncoderLayer'>\n",
      "Name: layers.4.layernorm_1 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.4.attention | Type: <class 'StableDiffusion.Attention.MHSelfAttention'>\n",
      "Name: layers.4.attention.in_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.4.attention.in_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.4.attention.out_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.4.attention.out_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.4.layernorm_2 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.4.linear_1 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.4.linear_2 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.5 | Type: <class 'StableDiffusion.ClipEncoder.ClipEncoderLayer'>\n",
      "Name: layers.5.layernorm_1 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.5.attention | Type: <class 'StableDiffusion.Attention.MHSelfAttention'>\n",
      "Name: layers.5.attention.in_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.5.attention.in_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.5.attention.out_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.5.attention.out_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.5.layernorm_2 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.5.linear_1 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.5.linear_2 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.6 | Type: <class 'StableDiffusion.ClipEncoder.ClipEncoderLayer'>\n",
      "Name: layers.6.layernorm_1 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.6.attention | Type: <class 'StableDiffusion.Attention.MHSelfAttention'>\n",
      "Name: layers.6.attention.in_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.6.attention.in_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.6.attention.out_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.6.attention.out_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.6.layernorm_2 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.6.linear_1 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.6.linear_2 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.7 | Type: <class 'StableDiffusion.ClipEncoder.ClipEncoderLayer'>\n",
      "Name: layers.7.layernorm_1 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.7.attention | Type: <class 'StableDiffusion.Attention.MHSelfAttention'>\n",
      "Name: layers.7.attention.in_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.7.attention.in_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.7.attention.out_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.7.attention.out_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.7.layernorm_2 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.7.linear_1 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.7.linear_2 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.8 | Type: <class 'StableDiffusion.ClipEncoder.ClipEncoderLayer'>\n",
      "Name: layers.8.layernorm_1 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.8.attention | Type: <class 'StableDiffusion.Attention.MHSelfAttention'>\n",
      "Name: layers.8.attention.in_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.8.attention.in_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.8.attention.out_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.8.attention.out_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.8.layernorm_2 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.8.linear_1 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.8.linear_2 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.9 | Type: <class 'StableDiffusion.ClipEncoder.ClipEncoderLayer'>\n",
      "Name: layers.9.layernorm_1 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.9.attention | Type: <class 'StableDiffusion.Attention.MHSelfAttention'>\n",
      "Name: layers.9.attention.in_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.9.attention.in_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.9.attention.out_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.9.attention.out_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.9.layernorm_2 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.9.linear_1 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.9.linear_2 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.10 | Type: <class 'StableDiffusion.ClipEncoder.ClipEncoderLayer'>\n",
      "Name: layers.10.layernorm_1 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.10.attention | Type: <class 'StableDiffusion.Attention.MHSelfAttention'>\n",
      "Name: layers.10.attention.in_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.10.attention.in_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.10.attention.out_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.10.attention.out_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.10.layernorm_2 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.10.linear_1 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.10.linear_2 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.11 | Type: <class 'StableDiffusion.ClipEncoder.ClipEncoderLayer'>\n",
      "Name: layers.11.layernorm_1 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.11.attention | Type: <class 'StableDiffusion.Attention.MHSelfAttention'>\n",
      "Name: layers.11.attention.in_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.11.attention.in_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.11.attention.out_proj | Type: <class 'StableDiffusion.LoraLayer.LoraLayer'>\n",
      "Name: layers.11.attention.out_proj.originLinear | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.11.layernorm_2 | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "Name: layers.11.linear_1 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layers.11.linear_2 | Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Name: layernorm | Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n"
     ]
    }
   ],
   "source": [
    "for name, module in clipEncoder.named_modules():\n",
    "    # 'name' is the string path (e.g., 'layers.0.self_attn.q_proj')\n",
    "    # 'module' is the actual layer object\n",
    "    print(f\"Name: {name} | Type: {type(module)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "model = clipEncoder\n",
    "for name,layer in model.named_modules():\n",
    "    nameSplits=name.split('.')\n",
    "    filterName = ('in_proj','out_proj')\n",
    "    if name.endswith(filterName) and isinstance(layer,nn.Linear):\n",
    "        print(name)\n",
    "        partNames = name.split('.')\n",
    "        preName = '.'.join(partNames[:-1])\n",
    "        layerName = partNames[-1]\n",
    "        print(f'preName = {preName}')\n",
    "        if preName =='':\n",
    "            parent = model\n",
    "        else:\n",
    "            parent = model.get_submodule(preName)\n",
    "            print(f'parent = {parent} {type(parent)}')\n",
    "    \n",
    "        injectedLoraLayer = LoraLayer(layer,rank =8,alpha=16)\n",
    "        setattr(parent,layerName,injectedLoraLayer)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable (LoRA): 442,368\n",
      "Frozen (Original): 123,060,480\n",
      "Percentage: 0.3582%\n"
     ]
    }
   ],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    # Total trainable params\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    # Non-trainable (frozen) params\n",
    "    frozen_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    \n",
    "    print(f\"Trainable (LoRA): {trainable_params:,}\")\n",
    "    print(f\"Frozen (Original): {frozen_params:,}\")\n",
    "    print(f\"Percentage: {100 * trainable_params / (trainable_params + frozen_params):.4f}%\")\n",
    "\n",
    "count_trainable_parameters(clipEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " embedding.position_embedding torch.Size([77, 768])\n",
      " embedding.token_embedding.weight torch.Size([49408, 768])\n",
      " layers.0.layernorm_1.weight torch.Size([768])\n",
      " layers.0.layernorm_1.bias torch.Size([768])\n",
      " layers.0.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.0.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.0.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.0.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.0.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.0.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.0.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.0.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.0.layernorm_2.weight torch.Size([768])\n",
      " layers.0.layernorm_2.bias torch.Size([768])\n",
      " layers.0.linear_1.weight torch.Size([3072, 768])\n",
      " layers.0.linear_1.bias torch.Size([3072])\n",
      " layers.0.linear_2.weight torch.Size([768, 3072])\n",
      " layers.0.linear_2.bias torch.Size([768])\n",
      " layers.1.layernorm_1.weight torch.Size([768])\n",
      " layers.1.layernorm_1.bias torch.Size([768])\n",
      " layers.1.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.1.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.1.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.1.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.1.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.1.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.1.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.1.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.1.layernorm_2.weight torch.Size([768])\n",
      " layers.1.layernorm_2.bias torch.Size([768])\n",
      " layers.1.linear_1.weight torch.Size([3072, 768])\n",
      " layers.1.linear_1.bias torch.Size([3072])\n",
      " layers.1.linear_2.weight torch.Size([768, 3072])\n",
      " layers.1.linear_2.bias torch.Size([768])\n",
      " layers.2.layernorm_1.weight torch.Size([768])\n",
      " layers.2.layernorm_1.bias torch.Size([768])\n",
      " layers.2.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.2.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.2.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.2.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.2.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.2.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.2.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.2.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.2.layernorm_2.weight torch.Size([768])\n",
      " layers.2.layernorm_2.bias torch.Size([768])\n",
      " layers.2.linear_1.weight torch.Size([3072, 768])\n",
      " layers.2.linear_1.bias torch.Size([3072])\n",
      " layers.2.linear_2.weight torch.Size([768, 3072])\n",
      " layers.2.linear_2.bias torch.Size([768])\n",
      " layers.3.layernorm_1.weight torch.Size([768])\n",
      " layers.3.layernorm_1.bias torch.Size([768])\n",
      " layers.3.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.3.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.3.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.3.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.3.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.3.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.3.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.3.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.3.layernorm_2.weight torch.Size([768])\n",
      " layers.3.layernorm_2.bias torch.Size([768])\n",
      " layers.3.linear_1.weight torch.Size([3072, 768])\n",
      " layers.3.linear_1.bias torch.Size([3072])\n",
      " layers.3.linear_2.weight torch.Size([768, 3072])\n",
      " layers.3.linear_2.bias torch.Size([768])\n",
      " layers.4.layernorm_1.weight torch.Size([768])\n",
      " layers.4.layernorm_1.bias torch.Size([768])\n",
      " layers.4.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.4.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.4.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.4.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.4.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.4.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.4.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.4.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.4.layernorm_2.weight torch.Size([768])\n",
      " layers.4.layernorm_2.bias torch.Size([768])\n",
      " layers.4.linear_1.weight torch.Size([3072, 768])\n",
      " layers.4.linear_1.bias torch.Size([3072])\n",
      " layers.4.linear_2.weight torch.Size([768, 3072])\n",
      " layers.4.linear_2.bias torch.Size([768])\n",
      " layers.5.layernorm_1.weight torch.Size([768])\n",
      " layers.5.layernorm_1.bias torch.Size([768])\n",
      " layers.5.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.5.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.5.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.5.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.5.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.5.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.5.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.5.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.5.layernorm_2.weight torch.Size([768])\n",
      " layers.5.layernorm_2.bias torch.Size([768])\n",
      " layers.5.linear_1.weight torch.Size([3072, 768])\n",
      " layers.5.linear_1.bias torch.Size([3072])\n",
      " layers.5.linear_2.weight torch.Size([768, 3072])\n",
      " layers.5.linear_2.bias torch.Size([768])\n",
      " layers.6.layernorm_1.weight torch.Size([768])\n",
      " layers.6.layernorm_1.bias torch.Size([768])\n",
      " layers.6.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.6.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.6.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.6.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.6.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.6.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.6.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.6.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.6.layernorm_2.weight torch.Size([768])\n",
      " layers.6.layernorm_2.bias torch.Size([768])\n",
      " layers.6.linear_1.weight torch.Size([3072, 768])\n",
      " layers.6.linear_1.bias torch.Size([3072])\n",
      " layers.6.linear_2.weight torch.Size([768, 3072])\n",
      " layers.6.linear_2.bias torch.Size([768])\n",
      " layers.7.layernorm_1.weight torch.Size([768])\n",
      " layers.7.layernorm_1.bias torch.Size([768])\n",
      " layers.7.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.7.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.7.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.7.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.7.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.7.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.7.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.7.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.7.layernorm_2.weight torch.Size([768])\n",
      " layers.7.layernorm_2.bias torch.Size([768])\n",
      " layers.7.linear_1.weight torch.Size([3072, 768])\n",
      " layers.7.linear_1.bias torch.Size([3072])\n",
      " layers.7.linear_2.weight torch.Size([768, 3072])\n",
      " layers.7.linear_2.bias torch.Size([768])\n",
      " layers.8.layernorm_1.weight torch.Size([768])\n",
      " layers.8.layernorm_1.bias torch.Size([768])\n",
      " layers.8.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.8.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.8.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.8.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.8.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.8.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.8.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.8.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.8.layernorm_2.weight torch.Size([768])\n",
      " layers.8.layernorm_2.bias torch.Size([768])\n",
      " layers.8.linear_1.weight torch.Size([3072, 768])\n",
      " layers.8.linear_1.bias torch.Size([3072])\n",
      " layers.8.linear_2.weight torch.Size([768, 3072])\n",
      " layers.8.linear_2.bias torch.Size([768])\n",
      " layers.9.layernorm_1.weight torch.Size([768])\n",
      " layers.9.layernorm_1.bias torch.Size([768])\n",
      " layers.9.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.9.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.9.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.9.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.9.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.9.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.9.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.9.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.9.layernorm_2.weight torch.Size([768])\n",
      " layers.9.layernorm_2.bias torch.Size([768])\n",
      " layers.9.linear_1.weight torch.Size([3072, 768])\n",
      " layers.9.linear_1.bias torch.Size([3072])\n",
      " layers.9.linear_2.weight torch.Size([768, 3072])\n",
      " layers.9.linear_2.bias torch.Size([768])\n",
      " layers.10.layernorm_1.weight torch.Size([768])\n",
      " layers.10.layernorm_1.bias torch.Size([768])\n",
      " layers.10.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.10.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.10.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.10.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.10.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.10.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.10.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.10.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.10.layernorm_2.weight torch.Size([768])\n",
      " layers.10.layernorm_2.bias torch.Size([768])\n",
      " layers.10.linear_1.weight torch.Size([3072, 768])\n",
      " layers.10.linear_1.bias torch.Size([3072])\n",
      " layers.10.linear_2.weight torch.Size([768, 3072])\n",
      " layers.10.linear_2.bias torch.Size([768])\n",
      " layers.11.layernorm_1.weight torch.Size([768])\n",
      " layers.11.layernorm_1.bias torch.Size([768])\n",
      " layers.11.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.11.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.11.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.11.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.11.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.11.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.11.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.11.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.11.layernorm_2.weight torch.Size([768])\n",
      " layers.11.layernorm_2.bias torch.Size([768])\n",
      " layers.11.linear_1.weight torch.Size([3072, 768])\n",
      " layers.11.linear_1.bias torch.Size([3072])\n",
      " layers.11.linear_2.weight torch.Size([768, 3072])\n",
      " layers.11.linear_2.bias torch.Size([768])\n",
      " layernorm.weight torch.Size([768])\n",
      " layernorm.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for name,params in model.named_parameters():\n",
    "    print(f' {name} {params.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ClipEncoder(\n",
      "  (embedding): ClipEmbedding(\n",
      "    (token_embedding): Embedding(49408, 768)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-11): 12 x ClipEncoderLayer(\n",
      "      (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention): MHSelfAttention(\n",
      "        (in_proj): LoraLayer(\n",
      "          (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        )\n",
      "        (out_proj): LoraLayer(\n",
      "          (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      " embedding ClipEmbedding(\n",
      "  (token_embedding): Embedding(49408, 768)\n",
      ")\n",
      " embedding.token_embedding Embedding(49408, 768)\n",
      " layers ModuleList(\n",
      "  (0-11): 12 x ClipEncoderLayer(\n",
      "    (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MHSelfAttention(\n",
      "      (in_proj): LoraLayer(\n",
      "        (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      )\n",
      "      (out_proj): LoraLayer(\n",
      "        (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      ")\n",
      " layers.0 ClipEncoderLayer(\n",
      "  (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attention): MHSelfAttention(\n",
      "    (in_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    )\n",
      "    (out_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      " layers.0.layernorm_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.0.attention MHSelfAttention(\n",
      "  (in_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  )\n",
      "  (out_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      ")\n",
      " layers.0.attention.in_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      ")\n",
      " layers.0.attention.in_proj.originLinear Linear(in_features=768, out_features=2304, bias=True)\n",
      " layers.0.attention.out_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      " layers.0.attention.out_proj.originLinear Linear(in_features=768, out_features=768, bias=True)\n",
      " layers.0.layernorm_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.0.linear_1 Linear(in_features=768, out_features=3072, bias=True)\n",
      " layers.0.linear_2 Linear(in_features=3072, out_features=768, bias=True)\n",
      " layers.1 ClipEncoderLayer(\n",
      "  (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attention): MHSelfAttention(\n",
      "    (in_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    )\n",
      "    (out_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      " layers.1.layernorm_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.1.attention MHSelfAttention(\n",
      "  (in_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  )\n",
      "  (out_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      ")\n",
      " layers.1.attention.in_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      ")\n",
      " layers.1.attention.in_proj.originLinear Linear(in_features=768, out_features=2304, bias=True)\n",
      " layers.1.attention.out_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      " layers.1.attention.out_proj.originLinear Linear(in_features=768, out_features=768, bias=True)\n",
      " layers.1.layernorm_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.1.linear_1 Linear(in_features=768, out_features=3072, bias=True)\n",
      " layers.1.linear_2 Linear(in_features=3072, out_features=768, bias=True)\n",
      " layers.2 ClipEncoderLayer(\n",
      "  (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attention): MHSelfAttention(\n",
      "    (in_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    )\n",
      "    (out_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      " layers.2.layernorm_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.2.attention MHSelfAttention(\n",
      "  (in_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  )\n",
      "  (out_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      ")\n",
      " layers.2.attention.in_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      ")\n",
      " layers.2.attention.in_proj.originLinear Linear(in_features=768, out_features=2304, bias=True)\n",
      " layers.2.attention.out_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      " layers.2.attention.out_proj.originLinear Linear(in_features=768, out_features=768, bias=True)\n",
      " layers.2.layernorm_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.2.linear_1 Linear(in_features=768, out_features=3072, bias=True)\n",
      " layers.2.linear_2 Linear(in_features=3072, out_features=768, bias=True)\n",
      " layers.3 ClipEncoderLayer(\n",
      "  (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attention): MHSelfAttention(\n",
      "    (in_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    )\n",
      "    (out_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      " layers.3.layernorm_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.3.attention MHSelfAttention(\n",
      "  (in_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  )\n",
      "  (out_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      ")\n",
      " layers.3.attention.in_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      ")\n",
      " layers.3.attention.in_proj.originLinear Linear(in_features=768, out_features=2304, bias=True)\n",
      " layers.3.attention.out_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      " layers.3.attention.out_proj.originLinear Linear(in_features=768, out_features=768, bias=True)\n",
      " layers.3.layernorm_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.3.linear_1 Linear(in_features=768, out_features=3072, bias=True)\n",
      " layers.3.linear_2 Linear(in_features=3072, out_features=768, bias=True)\n",
      " layers.4 ClipEncoderLayer(\n",
      "  (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attention): MHSelfAttention(\n",
      "    (in_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    )\n",
      "    (out_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      " layers.4.layernorm_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.4.attention MHSelfAttention(\n",
      "  (in_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  )\n",
      "  (out_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      ")\n",
      " layers.4.attention.in_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      ")\n",
      " layers.4.attention.in_proj.originLinear Linear(in_features=768, out_features=2304, bias=True)\n",
      " layers.4.attention.out_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      " layers.4.attention.out_proj.originLinear Linear(in_features=768, out_features=768, bias=True)\n",
      " layers.4.layernorm_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.4.linear_1 Linear(in_features=768, out_features=3072, bias=True)\n",
      " layers.4.linear_2 Linear(in_features=3072, out_features=768, bias=True)\n",
      " layers.5 ClipEncoderLayer(\n",
      "  (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attention): MHSelfAttention(\n",
      "    (in_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    )\n",
      "    (out_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      " layers.5.layernorm_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.5.attention MHSelfAttention(\n",
      "  (in_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  )\n",
      "  (out_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      ")\n",
      " layers.5.attention.in_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      ")\n",
      " layers.5.attention.in_proj.originLinear Linear(in_features=768, out_features=2304, bias=True)\n",
      " layers.5.attention.out_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      " layers.5.attention.out_proj.originLinear Linear(in_features=768, out_features=768, bias=True)\n",
      " layers.5.layernorm_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.5.linear_1 Linear(in_features=768, out_features=3072, bias=True)\n",
      " layers.5.linear_2 Linear(in_features=3072, out_features=768, bias=True)\n",
      " layers.6 ClipEncoderLayer(\n",
      "  (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attention): MHSelfAttention(\n",
      "    (in_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    )\n",
      "    (out_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      " layers.6.layernorm_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.6.attention MHSelfAttention(\n",
      "  (in_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  )\n",
      "  (out_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      ")\n",
      " layers.6.attention.in_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      ")\n",
      " layers.6.attention.in_proj.originLinear Linear(in_features=768, out_features=2304, bias=True)\n",
      " layers.6.attention.out_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      " layers.6.attention.out_proj.originLinear Linear(in_features=768, out_features=768, bias=True)\n",
      " layers.6.layernorm_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.6.linear_1 Linear(in_features=768, out_features=3072, bias=True)\n",
      " layers.6.linear_2 Linear(in_features=3072, out_features=768, bias=True)\n",
      " layers.7 ClipEncoderLayer(\n",
      "  (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attention): MHSelfAttention(\n",
      "    (in_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    )\n",
      "    (out_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      " layers.7.layernorm_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.7.attention MHSelfAttention(\n",
      "  (in_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  )\n",
      "  (out_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      ")\n",
      " layers.7.attention.in_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      ")\n",
      " layers.7.attention.in_proj.originLinear Linear(in_features=768, out_features=2304, bias=True)\n",
      " layers.7.attention.out_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      " layers.7.attention.out_proj.originLinear Linear(in_features=768, out_features=768, bias=True)\n",
      " layers.7.layernorm_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.7.linear_1 Linear(in_features=768, out_features=3072, bias=True)\n",
      " layers.7.linear_2 Linear(in_features=3072, out_features=768, bias=True)\n",
      " layers.8 ClipEncoderLayer(\n",
      "  (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attention): MHSelfAttention(\n",
      "    (in_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    )\n",
      "    (out_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      " layers.8.layernorm_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.8.attention MHSelfAttention(\n",
      "  (in_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  )\n",
      "  (out_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      ")\n",
      " layers.8.attention.in_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      ")\n",
      " layers.8.attention.in_proj.originLinear Linear(in_features=768, out_features=2304, bias=True)\n",
      " layers.8.attention.out_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      " layers.8.attention.out_proj.originLinear Linear(in_features=768, out_features=768, bias=True)\n",
      " layers.8.layernorm_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.8.linear_1 Linear(in_features=768, out_features=3072, bias=True)\n",
      " layers.8.linear_2 Linear(in_features=3072, out_features=768, bias=True)\n",
      " layers.9 ClipEncoderLayer(\n",
      "  (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attention): MHSelfAttention(\n",
      "    (in_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    )\n",
      "    (out_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      " layers.9.layernorm_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.9.attention MHSelfAttention(\n",
      "  (in_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  )\n",
      "  (out_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      ")\n",
      " layers.9.attention.in_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      ")\n",
      " layers.9.attention.in_proj.originLinear Linear(in_features=768, out_features=2304, bias=True)\n",
      " layers.9.attention.out_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      " layers.9.attention.out_proj.originLinear Linear(in_features=768, out_features=768, bias=True)\n",
      " layers.9.layernorm_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.9.linear_1 Linear(in_features=768, out_features=3072, bias=True)\n",
      " layers.9.linear_2 Linear(in_features=3072, out_features=768, bias=True)\n",
      " layers.10 ClipEncoderLayer(\n",
      "  (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attention): MHSelfAttention(\n",
      "    (in_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    )\n",
      "    (out_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      " layers.10.layernorm_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.10.attention MHSelfAttention(\n",
      "  (in_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  )\n",
      "  (out_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      ")\n",
      " layers.10.attention.in_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      ")\n",
      " layers.10.attention.in_proj.originLinear Linear(in_features=768, out_features=2304, bias=True)\n",
      " layers.10.attention.out_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      " layers.10.attention.out_proj.originLinear Linear(in_features=768, out_features=768, bias=True)\n",
      " layers.10.layernorm_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.10.linear_1 Linear(in_features=768, out_features=3072, bias=True)\n",
      " layers.10.linear_2 Linear(in_features=3072, out_features=768, bias=True)\n",
      " layers.11 ClipEncoderLayer(\n",
      "  (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attention): MHSelfAttention(\n",
      "    (in_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    )\n",
      "    (out_proj): LoraLayer(\n",
      "      (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      " layers.11.layernorm_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.11.attention MHSelfAttention(\n",
      "  (in_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  )\n",
      "  (out_proj): LoraLayer(\n",
      "    (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      ")\n",
      " layers.11.attention.in_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=2304, bias=True)\n",
      ")\n",
      " layers.11.attention.in_proj.originLinear Linear(in_features=768, out_features=2304, bias=True)\n",
      " layers.11.attention.out_proj LoraLayer(\n",
      "  (originLinear): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      " layers.11.attention.out_proj.originLinear Linear(in_features=768, out_features=768, bias=True)\n",
      " layers.11.layernorm_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " layers.11.linear_1 Linear(in_features=768, out_features=3072, bias=True)\n",
      " layers.11.linear_2 Linear(in_features=3072, out_features=768, bias=True)\n",
      " layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      " embedding.position_embedding torch.Size([77, 768])\n",
      " embedding.token_embedding.weight torch.Size([49408, 768])\n",
      " layers.0.layernorm_1.weight torch.Size([768])\n",
      " layers.0.layernorm_1.bias torch.Size([768])\n",
      " layers.0.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.0.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.0.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.0.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.0.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.0.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.0.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.0.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.0.layernorm_2.weight torch.Size([768])\n",
      " layers.0.layernorm_2.bias torch.Size([768])\n",
      " layers.0.linear_1.weight torch.Size([3072, 768])\n",
      " layers.0.linear_1.bias torch.Size([3072])\n",
      " layers.0.linear_2.weight torch.Size([768, 3072])\n",
      " layers.0.linear_2.bias torch.Size([768])\n",
      " layers.1.layernorm_1.weight torch.Size([768])\n",
      " layers.1.layernorm_1.bias torch.Size([768])\n",
      " layers.1.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.1.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.1.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.1.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.1.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.1.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.1.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.1.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.1.layernorm_2.weight torch.Size([768])\n",
      " layers.1.layernorm_2.bias torch.Size([768])\n",
      " layers.1.linear_1.weight torch.Size([3072, 768])\n",
      " layers.1.linear_1.bias torch.Size([3072])\n",
      " layers.1.linear_2.weight torch.Size([768, 3072])\n",
      " layers.1.linear_2.bias torch.Size([768])\n",
      " layers.2.layernorm_1.weight torch.Size([768])\n",
      " layers.2.layernorm_1.bias torch.Size([768])\n",
      " layers.2.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.2.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.2.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.2.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.2.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.2.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.2.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.2.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.2.layernorm_2.weight torch.Size([768])\n",
      " layers.2.layernorm_2.bias torch.Size([768])\n",
      " layers.2.linear_1.weight torch.Size([3072, 768])\n",
      " layers.2.linear_1.bias torch.Size([3072])\n",
      " layers.2.linear_2.weight torch.Size([768, 3072])\n",
      " layers.2.linear_2.bias torch.Size([768])\n",
      " layers.3.layernorm_1.weight torch.Size([768])\n",
      " layers.3.layernorm_1.bias torch.Size([768])\n",
      " layers.3.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.3.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.3.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.3.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.3.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.3.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.3.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.3.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.3.layernorm_2.weight torch.Size([768])\n",
      " layers.3.layernorm_2.bias torch.Size([768])\n",
      " layers.3.linear_1.weight torch.Size([3072, 768])\n",
      " layers.3.linear_1.bias torch.Size([3072])\n",
      " layers.3.linear_2.weight torch.Size([768, 3072])\n",
      " layers.3.linear_2.bias torch.Size([768])\n",
      " layers.4.layernorm_1.weight torch.Size([768])\n",
      " layers.4.layernorm_1.bias torch.Size([768])\n",
      " layers.4.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.4.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.4.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.4.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.4.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.4.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.4.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.4.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.4.layernorm_2.weight torch.Size([768])\n",
      " layers.4.layernorm_2.bias torch.Size([768])\n",
      " layers.4.linear_1.weight torch.Size([3072, 768])\n",
      " layers.4.linear_1.bias torch.Size([3072])\n",
      " layers.4.linear_2.weight torch.Size([768, 3072])\n",
      " layers.4.linear_2.bias torch.Size([768])\n",
      " layers.5.layernorm_1.weight torch.Size([768])\n",
      " layers.5.layernorm_1.bias torch.Size([768])\n",
      " layers.5.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.5.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.5.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.5.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.5.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.5.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.5.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.5.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.5.layernorm_2.weight torch.Size([768])\n",
      " layers.5.layernorm_2.bias torch.Size([768])\n",
      " layers.5.linear_1.weight torch.Size([3072, 768])\n",
      " layers.5.linear_1.bias torch.Size([3072])\n",
      " layers.5.linear_2.weight torch.Size([768, 3072])\n",
      " layers.5.linear_2.bias torch.Size([768])\n",
      " layers.6.layernorm_1.weight torch.Size([768])\n",
      " layers.6.layernorm_1.bias torch.Size([768])\n",
      " layers.6.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.6.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.6.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.6.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.6.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.6.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.6.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.6.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.6.layernorm_2.weight torch.Size([768])\n",
      " layers.6.layernorm_2.bias torch.Size([768])\n",
      " layers.6.linear_1.weight torch.Size([3072, 768])\n",
      " layers.6.linear_1.bias torch.Size([3072])\n",
      " layers.6.linear_2.weight torch.Size([768, 3072])\n",
      " layers.6.linear_2.bias torch.Size([768])\n",
      " layers.7.layernorm_1.weight torch.Size([768])\n",
      " layers.7.layernorm_1.bias torch.Size([768])\n",
      " layers.7.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.7.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.7.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.7.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.7.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.7.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.7.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.7.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.7.layernorm_2.weight torch.Size([768])\n",
      " layers.7.layernorm_2.bias torch.Size([768])\n",
      " layers.7.linear_1.weight torch.Size([3072, 768])\n",
      " layers.7.linear_1.bias torch.Size([3072])\n",
      " layers.7.linear_2.weight torch.Size([768, 3072])\n",
      " layers.7.linear_2.bias torch.Size([768])\n",
      " layers.8.layernorm_1.weight torch.Size([768])\n",
      " layers.8.layernorm_1.bias torch.Size([768])\n",
      " layers.8.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.8.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.8.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.8.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.8.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.8.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.8.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.8.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.8.layernorm_2.weight torch.Size([768])\n",
      " layers.8.layernorm_2.bias torch.Size([768])\n",
      " layers.8.linear_1.weight torch.Size([3072, 768])\n",
      " layers.8.linear_1.bias torch.Size([3072])\n",
      " layers.8.linear_2.weight torch.Size([768, 3072])\n",
      " layers.8.linear_2.bias torch.Size([768])\n",
      " layers.9.layernorm_1.weight torch.Size([768])\n",
      " layers.9.layernorm_1.bias torch.Size([768])\n",
      " layers.9.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.9.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.9.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.9.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.9.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.9.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.9.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.9.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.9.layernorm_2.weight torch.Size([768])\n",
      " layers.9.layernorm_2.bias torch.Size([768])\n",
      " layers.9.linear_1.weight torch.Size([3072, 768])\n",
      " layers.9.linear_1.bias torch.Size([3072])\n",
      " layers.9.linear_2.weight torch.Size([768, 3072])\n",
      " layers.9.linear_2.bias torch.Size([768])\n",
      " layers.10.layernorm_1.weight torch.Size([768])\n",
      " layers.10.layernorm_1.bias torch.Size([768])\n",
      " layers.10.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.10.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.10.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.10.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.10.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.10.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.10.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.10.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.10.layernorm_2.weight torch.Size([768])\n",
      " layers.10.layernorm_2.bias torch.Size([768])\n",
      " layers.10.linear_1.weight torch.Size([3072, 768])\n",
      " layers.10.linear_1.bias torch.Size([3072])\n",
      " layers.10.linear_2.weight torch.Size([768, 3072])\n",
      " layers.10.linear_2.bias torch.Size([768])\n",
      " layers.11.layernorm_1.weight torch.Size([768])\n",
      " layers.11.layernorm_1.bias torch.Size([768])\n",
      " layers.11.attention.in_proj.loraA torch.Size([768, 8])\n",
      " layers.11.attention.in_proj.loraB torch.Size([8, 2304])\n",
      " layers.11.attention.in_proj.originLinear.weight torch.Size([2304, 768])\n",
      " layers.11.attention.in_proj.originLinear.bias torch.Size([2304])\n",
      " layers.11.attention.out_proj.loraA torch.Size([768, 8])\n",
      " layers.11.attention.out_proj.loraB torch.Size([8, 768])\n",
      " layers.11.attention.out_proj.originLinear.weight torch.Size([768, 768])\n",
      " layers.11.attention.out_proj.originLinear.bias torch.Size([768])\n",
      " layers.11.layernorm_2.weight torch.Size([768])\n",
      " layers.11.layernorm_2.bias torch.Size([768])\n",
      " layers.11.linear_1.weight torch.Size([3072, 768])\n",
      " layers.11.linear_1.bias torch.Size([3072])\n",
      " layers.11.linear_2.weight torch.Size([768, 3072])\n",
      " layers.11.linear_2.bias torch.Size([768])\n",
      " layernorm.weight torch.Size([768])\n",
      " layernorm.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for name,layer in model.named_modules():\n",
    "    print(f' {name} {layer}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''   \n",
    "    filterName = 'k_proj'\n",
    "    if name.endswith(filterName):\n",
    "        print(name)\n",
    "        \n",
    "    filterName = 'v_proj'\n",
    "    if name.endswith(filterName):\n",
    "        print(name)\n",
    "    \n",
    "    filterName = 'q_proj'\n",
    "    if name.endswith(filterName):\n",
    "        print(name)\n",
    "    \n",
    "'''    \n",
    "\n",
    "    #print(name_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ClipEncoder' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m name \u001b[39m=\u001b[39m clipEncoder\n\u001b[0;32m----> 2\u001b[0m name_cols\u001b[39m=\u001b[39mname\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# linearmodule\u001b[39;00m\n\u001b[1;32m      5\u001b[0m children\u001b[39m=\u001b[39mname_cols[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/external-libraries/lib/python3.10/site-packages/torch/nn/modules/module.py:1965\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1963\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1964\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1965\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m   1966\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1967\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ClipEncoder' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "name = clipEncoder\n",
    "name_cols=name.split('.')\n",
    "\n",
    "# linearmodule\n",
    "children=name_cols[:-1]\n",
    "cur_layer=model \n",
    "for child in children:\n",
    "    cur_layer=getattr(cur_layer,child)\n",
    "\n",
    "#print(layer==getattr(cur_layer,name_cols[-1]))\n",
    "#lora_layer=LoraLayer(layer,layer.in_features,layer.out_features,LORA_R,LORA_ALPHA)\n",
    "#setattr(cur_layer,name_cols[-1],lora_layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('MyEnvForSD': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "647a70d9ee79436e1ee2ec7ca1cc35c4dc1bf6626b891860add3bc52abf89db9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
