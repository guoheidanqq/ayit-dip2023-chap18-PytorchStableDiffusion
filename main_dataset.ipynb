{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized: xlgfx56_01.jpg\n",
      "Resized: xlgfx30_06.jpg\n",
      "Resized: xlgfx54_05 - Copy.jpg\n",
      "Resized: xlgfx52_07.jpg\n",
      "Resized: xlgfx42_07.jpg\n",
      "Resized: xlgfx20_08.jpg\n",
      "Resized: xlgfx23_06.jpg\n",
      "Resized: xlgfx53_02.jpg\n",
      "Resized: xlgfx23_10.jpg\n",
      "Resized: xlgfx19_08.jpg\n",
      "Resized: xlgfx31_08.jpg\n",
      "Resized: xlgfx49_06.jpg\n",
      "Resized: xlgfx52_06.jpg\n",
      "Resized: xlgfx38_05.jpg\n",
      "Resized: xlgfx42_05.jpg\n",
      "Resized: xlgfx08_13.jpg\n",
      "Resized: xlgfx56_06.jpg\n",
      "Resized: xlgfx13_09.jpg\n",
      "Resized: xlgfx45_05.jpg\n",
      "Resized: xlgfx52_05.jpg\n",
      "Resized: xlgfx51_08.jpg\n",
      "Resized: xlgfx08_03.jpg\n",
      "Resized: xlgfx08_01.jpg\n",
      "Resized: xlgfx56_04.jpg\n",
      "Resized: xlgfx43_03.jpg\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "input_folder = \"images/xfx_512_512\"\n",
    "output_folder = \"images/xfx_512_512_resized\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".webp\")):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "        image = Image.open(input_path).convert(\"RGB\")\n",
    "        image = image.resize((512, 512), Image.LANCZOS)\n",
    "\n",
    "        image.save(output_path)\n",
    "\n",
    "        print(f\"Resized: {filename}\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Runtime version: 1.23.2\n",
      "['general', 'sensitive', 'questionable', 'explicit', '1girl', 'solo', 'long_hair', 'breasts', 'looking_at_viewer', 'blush']\n",
      "['xlgfx56_06.jpg', 'xlgfx56_04.jpg', 'xlgfx08_01.jpg', 'xlgfx08_13.jpg', 'xlgfx23_06.jpg', 'xlgfx08_03.jpg', 'xlgfx42_05.jpg', 'xlgfx49_06.jpg', 'xlgfx52_06.jpg', 'xlgfx54_05.jpg', 'xlgfx43_03.jpg', 'xlgfx31_08.jpg', 'xlgfx19_08.jpg', 'xlgfx51_08.jpg', 'xlgfx56_01.jpg', 'xlgfx13_09.jpg', 'xlgfx45_05.jpg', 'xlgfx53_02.jpg', 'xlgfx52_07.jpg', 'xlgfx52_05.jpg', 'xlgfx42_07.jpg', 'xlgfx38_05.jpg', 'xlgfx30_06.jpg', 'xlgfx23_10.jpg', 'xlgfx20_08.jpg']\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx56_06.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[6.2523067e-01 3.5084054e-01 3.4222007e-03 ... 1.0818243e-05 3.8146973e-06\n",
      " 2.0563602e-06]\n",
      "xiaofeixiang,general,sensitive,1girl,solo,long hair,looking at viewer,smile,bangs,shirt,black hair,animal ears,white shirt,upper body,outdoors,teeth,collared shirt,cat ears,grin,black eyes,colored skin,fangs,portrait,realistic,photo background\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx56_04.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[6.8181598e-01 3.0335802e-01 2.7686059e-03 ... 1.1354685e-05 1.3411045e-06\n",
      " 1.3113022e-06]\n",
      "xiaofeixiang,general,sensitive,1girl,solo,long hair,looking at viewer,smile,bangs,shirt,black hair,bow,animal ears,school uniform,upper body,outdoors,teeth,cat ears,bowtie,grin,black eyes,colored skin,portrait,realistic,horror (theme)\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx08_01.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[2.0815495e-01 8.1353295e-01 7.9714358e-03 ... 4.6491623e-06 1.4901161e-07\n",
      " 2.3841858e-07]\n",
      "xiaofeixiang,general,sensitive,1girl,solo,long hair,skirt,shirt,black hair,hair ornament,sitting,school uniform,monochrome,outdoors,necktie,hairclip,lips,colored skin,blue necktie,stairs,realistic,railing,purple theme,purple skin,sitting on stairs\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx08_13.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[7.1421951e-02 9.2013955e-01 1.5496671e-02 ... 1.0013580e-05 2.6524067e-06\n",
      " 2.2351742e-06]\n",
      "xiaofeixiang,sensitive,1girl,solo,long hair,looking at viewer,skirt,shirt,black hair,hair ornament,sitting,school uniform,necktie,socks,hairclip,colored skin,pale skin,blue necktie,realistic,fence\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx23_06.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[7.9067087e-01 1.9065797e-01 2.7866364e-03 ... 1.6570091e-05 1.1026859e-06\n",
      " 1.0728836e-06]\n",
      "xiaofeixiang,general,1girl,solo,long hair,looking at viewer,smile,bangs,simple background,black hair,jewelry,closed mouth,blurry,black eyes,lips,half-closed eyes,ring,portrait\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx08_03.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[4.9630195e-02 9.3929446e-01 1.0294944e-02 ... 3.1590462e-06 1.4901161e-07\n",
      " 1.7881393e-07]\n",
      "xiaofeixiang,sensitive,1girl,solo,long hair,skirt,black hair,hair ornament,school uniform,necktie,hairclip,colored skin,crossed arms,realistic,blue skin,purple skin\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx42_05.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[6.2405717e-01 3.2456696e-01 2.2199154e-03 ... 9.2387199e-07 1.8477440e-06\n",
      " 1.3113022e-06]\n",
      "xiaofeixiang,general,sensitive,1girl,solo,long hair,looking at viewer,smile,bangs,brown hair,black hair,long sleeves,closed mouth,upper body,outdoors,hand up,water,scarf,black eyes,sweater,lips,coat,realistic,winter clothes\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx49_06.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[1.6469881e-01 8.3593082e-01 2.3016632e-03 ... 3.4272671e-06 2.0861626e-07\n",
      " 5.9604645e-08]\n",
      "xiaofeixiang,sensitive,1girl,solo,long hair,looking at viewer,bangs,brown hair,black hair,dress,bare shoulders,brown eyes,closed mouth,upper body,flower,outdoors,sky,sleeveless,day,hair over one eye,lips,plant,white flower,bouquet,realistic,yellow dress,faux traditional media\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx52_06.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[3.9862844e-01 5.6991011e-01 2.1647513e-03 ... 6.9737434e-06 5.9604645e-07\n",
      " 4.1723251e-07]\n",
      "xiaofeixiang,general,sensitive,1girl,solo,long hair,looking at viewer,bangs,black hair,animal ears,closed mouth,upper body,outdoors,day,cat ears,water,black eyes,tree,lips,colored skin,ground vehicle,realistic,river,purple lips,photo background,faux traditional media\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx54_05.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[6.5681171e-01 3.2413009e-01 2.7695000e-03 ... 1.5646219e-05 1.3709068e-06\n",
      " 1.4901161e-06]\n",
      "xiaofeixiang,general,sensitive,1girl,solo,long hair,looking at viewer,bangs,shirt,black hair,bow,animal ears,closed mouth,school uniform,upper body,outdoors,day,cat ears,bowtie,black eyes,tree,lips,fake animal ears,colored skin,realistic,fence,chain-link fence,purple lips\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx43_03.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[3.8003397e-01 5.8024091e-01 4.3048263e-03 ... 2.6434660e-05 2.0861626e-07\n",
      " 1.1920929e-07]\n",
      "xiaofeixiang,general,sensitive,1girl,solo,long hair,skirt,black hair,bow,animal ears,school uniform,jacket,outdoors,cat ears,water,bag,plaid,colored skin,plaid skirt,parody,building,realistic\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx31_08.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[5.0341076e-01 5.2054536e-01 3.3604801e-03 ... 1.5199184e-06 1.6391277e-06\n",
      " 1.2516975e-06]\n",
      "xiaofeixiang,general,sensitive,1girl,solo,long hair,simple background,black hair,parted lips,teeth,black eyes,arm up,lips,fingernails,colored skin,looking away,portrait,close-up,realistic,nose,blue skin,hand on own head,grey skin,purple skin,purple lips,shading eyes,hand on forehead\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx19_08.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[5.4764384e-01 4.2220870e-01 1.3172626e-03 ... 3.1292439e-06 9.2387199e-07\n",
      " 9.2387199e-07]\n",
      "xiaofeixiang,general,sensitive,1girl,solo,long hair,bangs,black hair,1boy,closed mouth,jacket,closed eyes,upper body,male focus,earrings,outdoors,blurry,from side,lips,black jacket,profile,makeup,depth of field,blurry background,colored skin,formal,suit,lipstick,portrait,realistic,nose,purple lips\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx51_08.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[7.5515169e-01 2.4612230e-01 1.5863180e-03 ... 1.1026859e-06 6.2584877e-07\n",
      " 5.9604645e-07]\n",
      "xiaofeixiang,general,sensitive,1girl,solo,long hair,looking at viewer,smile,bangs,brown hair,black hair,long sleeves,animal ears,jewelry,closed mouth,earrings,outdoors,cat ears,hand up,blurry,black eyes,lips,fingernails,blurry background,colored skin,half-closed eyes,bug,portrait,blue skin,purple skin,purple lips,silk,spider web\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx56_01.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[7.8996813e-01 2.2408703e-01 1.7690659e-03 ... 1.1622906e-05 1.2516975e-06\n",
      " 9.2387199e-07]\n",
      "xiaofeixiang,general,sensitive,1girl,solo,long hair,looking at viewer,bangs,shirt,black hair,bow,animal ears,closed mouth,school uniform,upper body,outdoors,collared shirt,virtual youtuber,cat ears,bowtie,black eyes,lips,fake animal ears,colored skin,portrait,realistic,purple lips,photo background\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx13_09.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[8.9719152e-01 9.9560767e-02 1.3326406e-03 ... 3.0994415e-06 5.6624413e-07\n",
      " 6.8545341e-07]\n",
      "xiaofeixiang,general,1girl,solo,long hair,bangs,shirt,black hair,closed mouth,jacket,white shirt,upper body,outdoors,necktie,collared shirt,blurry,black eyes,lips,black jacket,makeup,depth of field,blurry background,colored skin,looking away,formal,suit,looking up,lipstick,building,portrait,black necktie,city,realistic,nose,purple lips\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx45_05.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[3.7440681e-01 6.0138500e-01 1.3722181e-03 ... 7.3611736e-06 1.7881393e-07\n",
      " 8.9406967e-08]\n",
      "xiaofeixiang,general,sensitive,1girl,solo,long hair,smile,bangs,shirt,black hair,long sleeves,bow,holding,animal ears,closed mouth,school uniform,closed eyes,upper body,outdoors,collared shirt,cat ears,bowtie,sweater,tree,colored skin,facing viewer,purple bow,realistic\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx53_02.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[5.5113596e-01 4.2398432e-01 2.7441978e-03 ... 3.8444996e-06 2.8610229e-06\n",
      " 2.4139881e-06]\n",
      "xiaofeixiang,general,sensitive,1girl,solo,long hair,looking at viewer,smile,bangs,black hair,closed mouth,upper body,outdoors,blunt bangs,blurry,black eyes,tree,lips,portrait,realistic\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx52_07.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[7.2127700e-01 2.7858388e-01 2.2938550e-03 ... 4.5299530e-06 5.9604645e-07\n",
      " 3.8743019e-07]\n",
      "xiaofeixiang,general,sensitive,1girl,solo,long hair,looking at viewer,smile,bangs,shirt,black hair,bow,animal ears,school uniform,blue hair,parted lips,teeth,virtual youtuber,cat ears,bowtie,blurry,black eyes,lips,parody,portrait,realistic,purple lips\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx52_05.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[7.8903300e-01 2.1558350e-01 1.5112460e-03 ... 5.4538250e-06 1.1622906e-06\n",
      " 9.2387199e-07]\n",
      "xiaofeixiang,general,sensitive,1girl,solo,long hair,looking at viewer,bangs,black hair,animal ears,closed mouth,outdoors,cat ears,blurry,black eyes,lips,fingernails,blurry background,colored skin,lipstick,portrait,close-up,realistic,nose,purple skin,purple lips\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx42_07.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[5.0922161e-01 4.7159907e-01 1.5057623e-03 ... 3.6656857e-06 5.4836273e-06\n",
      " 7.5399876e-06]\n",
      "xiaofeixiang,general,sensitive,1girl,solo,long hair,looking at viewer,short hair,bangs,black hair,brown eyes,closed mouth,upper body,outdoors,scarf,black eyes,tree,lips,makeup,colored skin,expressionless,lipstick,portrait,nature,forest,realistic,winter,bare tree\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx38_05.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[5.4682231e-01 4.8385924e-01 1.1765957e-03 ... 6.6041946e-05 2.0861626e-07\n",
      " 1.7881393e-07]\n",
      "xiaofeixiang,general,sensitive,1girl,solo,long hair,bangs,shirt,black hair,long sleeves,closed mouth,school uniform,jacket,closed eyes,white shirt,upper body,necktie,collared shirt,blurry,blurry background,colored skin,realistic,blue skin,striped necktie,purple skin,purple lips\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx30_06.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[3.9942071e-01 6.2449801e-01 3.4799576e-03 ... 4.0620565e-05 3.5762787e-07\n",
      " 2.0861626e-07]\n",
      "xiaofeixiang,general,sensitive,1girl,solo,long hair,looking at viewer,smile,skirt,shirt,black hair,long sleeves,school uniform,jacket,outdoors,necktie,coat,plaid,colored skin,cardigan,pale skin,realistic,grey skin,striped necktie,photo background\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx23_10.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[8.7655336e-01 1.3767326e-01 8.6551905e-04 ... 2.1100044e-05 8.3446503e-07\n",
      " 6.2584877e-07]\n",
      "xiaofeixiang,general,1girl,solo,long hair,simple background,shirt,black hair,closed mouth,closed eyes,lips,colored skin,portrait,close-up,realistic,blue skin,grey skin,purple skin,purple lips\n",
      "/home/aistudio/PytorchStableDiffusion/images/xfx_512_512/xlgfx20_08.jpg\n",
      "<class 'list'> 1\n",
      "(1, 9083)\n",
      "[6.6967165e-01 3.0428368e-01 3.3559799e-03 ... 9.1791153e-06 7.4505806e-07\n",
      " 3.2782555e-07]\n",
      "xiaofeixiang,general,sensitive,1girl,solo,long hair,looking at viewer,bangs,shirt,black hair,long sleeves,closed mouth,jacket,white shirt,upper body,flower,outdoors,necktie,collared shirt,blurry,lips,coat,black jacket,blurry background,colored skin,leaf,formal,suit,plant,black necktie,black coat,realistic,purple flower\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import onnxruntime as ort\n",
    "import pandas as pd\n",
    "print(\"ONNX Runtime version:\", ort.__version__)\n",
    "ImagePath ='/home/aistudio/PytorchStableDiffusion/images/xfx_512_512'\n",
    "WD14TaggerPath ='/home/aistudio/models/wd-v1-4-convnextv2-tagger-v2/model.onnx'\n",
    "TagCSVPath = '/home/aistudio/models/wd-v1-4-convnextv2-tagger-v2/selected_tags.csv'\n",
    "TriggerWords = 'xiaofeixiang'\n",
    "session = ort.InferenceSession(WD14TaggerPath)\n",
    "tags = pd.read_csv(TagCSVPath)\n",
    "tagsList =tags['name'].tolist()\n",
    "print(tagsList[:10])\n",
    "fileList = os.listdir(ImagePath)\n",
    "fileImageList = []\n",
    "for fileName in fileList:\n",
    "    if fileName.endswith('.jpg'):\n",
    "        fileImageList.append(fileName)\n",
    "fileList = fileImageList\n",
    "print(fileList)\n",
    "for fileName in fileList:    \n",
    "    inputPath = os.path.join(ImagePath, fileName)\n",
    "    print(inputPath)\n",
    "    image = cv2.imread(inputPath)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = image.astype(np.float32)\n",
    "    image = cv2.resize(image, (448, 448))\n",
    "    #plt.imshow(image/255.0)\n",
    "    #plt.show()\n",
    "    #image = image.transpose((2, 0, 1))\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    #print(image, image.shape)\n",
    "    inputName = session.get_inputs()[0].name\n",
    "    outPut = session.run(None,{inputName:image})\n",
    "    print(type(outPut), len(outPut))\n",
    "    print(outPut[0].shape)\n",
    "    outPutIndexList =outPut[0][0]\n",
    "    print(outPutIndexList)\n",
    "    tagOutTextList = []\n",
    "    for i,tagConfidene in enumerate(outPutIndexList):\n",
    "        if tagConfidene > 0.2:\n",
    "            tagOutTextList.append(tagsList[i].replace('_',' '))\n",
    "    tagStr = TriggerWords\n",
    "    for tag in tagOutTextList:\n",
    "        tagStr = tagStr + ',' + tag       \n",
    "    textPath = inputPath.split('.')[0] + '.txt'\n",
    "    with open(textPath, 'w') as f:\n",
    "        f.write(tagStr)\n",
    "    print(tagStr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aistudio/external-libraries/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPTokenizer\n",
    "class LoraDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        super(LoraDataSet,self).__init__()\n",
    "        self.dataPath = '/home/aistudio/PytorchStableDiffusion/images/xfx_512_512'\n",
    "        self.imageFileList = []\n",
    "        self.textFileList = []\n",
    "        for fileName in os.listdir(self.dataPath):\n",
    "            if fileName.endswith('.jpg'):\n",
    "                self.imageFileList.append(os.path.join(self.dataPath,fileName))\n",
    "            \n",
    "            if fileName.endswith('.txt'):\n",
    "                self.textFileList.append(os.path.join(self.dataPath,fileName))\n",
    "        \n",
    "        self.preprocess = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((512,512)),\n",
    "                transforms.ToTensor(),   # (0 255) -> (-1, 1)\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])# \n",
    "                ]) \n",
    "        self.promptTokenizer = CLIPTokenizer(vocab_file='../models/sd15models/vocab.json',\n",
    "                                        merges_file='../models/sd15models/merges.txt')\n",
    "\n",
    "    def __getitem__(self,index):    \n",
    "        filePath = self.imageFileList[index]    \n",
    "        img1 = cv2.imread(filePath)\n",
    "        img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "        img1 = self.preprocess(img1)\n",
    "        \n",
    "        with open(self.textFileList[index],'r',encoding='utf-8') as f:\n",
    "            prompt = f.read()\n",
    "        promptTokens = self.promptTokenizer(prompt,padding='max_length',max_length=77,truncation=True,return_tensors='pt')['input_ids']\n",
    "        attentionMask = self.promptTokenizer(prompt,padding='max_length',max_length=77,truncation=True,return_tensors='pt')['attention_mask']\n",
    "        return img1,promptTokens,attentionMask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imageFileList)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aistudio/external-libraries/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPTokenizer\n",
    "from StableDiffusion.Utils import *\n",
    "class LoraDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        super(LoraDataSet,self).__init__()\n",
    "        self.dataPath = '/home/aistudio/PytorchStableDiffusion/images/xfx_512_512'\n",
    "        self.preprocess = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((512,512)),\n",
    "                transforms.ToTensor(),   # (0 255) -> (-1, 1)\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])# \n",
    "                ])\n",
    "        self.promptTokenizer = CLIPTokenizer(vocab_file='../models/sd15models/vocab.json',\n",
    "                                        merges_file='../models/sd15models/merges.txt')\n",
    "        self.imageFileList = []\n",
    "        self.textFileList = []\n",
    "        for fileName in os.listdir(self.dataPath):\n",
    "            if fileName.endswith('.jpg'):\n",
    "                self.imageFileList.append(os.path.join(self.dataPath,fileName))\n",
    "            \n",
    "            if fileName.endswith('.txt'):\n",
    "                self.textFileList.append(os.path.join(self.dataPath,fileName))\n",
    "    \n",
    "    def loadImageBatch(self,filePath:str,device='cuda')->torch.Tensor:\n",
    "        img1 = cv2.imread(filePath)\n",
    "        img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        imgTensor = self.preprocess(img1)\n",
    "        imgBatch = imgTensor[None, :, :, :]\n",
    "        return imgBatch[0].to(device)   \n",
    "    \n",
    "    def getPromptTokens(self,prompt:str,device='cuda')->Tuple[torch.LongTensor,torch.LongTensor]:\n",
    "\n",
    "        promptTokens = self.promptTokenizer(prompt,padding='max_length',\n",
    "                                            max_length=77,truncation=True,return_tensors='pt')['input_ids']\n",
    "        attentionMask = self.promptTokenizer(prompt,padding='max_length',\n",
    "                                             max_length=77,truncation=True,return_tensors='pt')['attention_mask']\n",
    "        return promptTokens[0].to(device),attentionMask[0].to(device)\n",
    "    \n",
    "    def __getitem__(self,index):    \n",
    "        filePath = self.imageFileList[index]\n",
    "        with open(self.textFileList[index],'r',encoding='utf-8') as f:\n",
    "            prompt = f.read()\n",
    "        imgTensor = self.loadImageBatch(filePath)\n",
    "        promptTokens,attentionMask = self.getPromptTokens(prompt)\n",
    "\n",
    "        return imgTensor,promptTokens,attentionMask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imageFileList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aistudio/external-libraries/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClipEncoder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import StableDiffusion.LoraDataSet\n",
    "import StableDiffusion.DdpmSamplerTorch\n",
    "import StableDiffusion.Utils\n",
    "import importlib\n",
    "importlib.reload(StableDiffusion.DdpmSamplerTorch)\n",
    "importlib.reload(StableDiffusion.LoraDataSet)\n",
    "importlib.reload(StableDiffusion.Utils)\n",
    "from StableDiffusion.DdpmSamplerTorch import DdpmSamplerTorch\n",
    "from StableDiffusion.LoraDataSet import LoraDataSet\n",
    "from StableDiffusion.Utils import Utils\n",
    "vaeEncoder,vaeDecoder,clipEncoder,diffusionProcess =Utils.loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " time_embedding.linear_1.weight torch.Size([1280, 320])\n",
      " time_embedding.linear_1.bias torch.Size([1280])\n",
      " time_embedding.linear_2.weight torch.Size([1280, 1280])\n",
      " time_embedding.linear_2.bias torch.Size([1280])\n",
      " unet.encoders.0.0.weight torch.Size([320, 4, 3, 3])\n",
      " unet.encoders.0.0.bias torch.Size([320])\n",
      " unet.encoders.1.0.groupnorm_feature.weight torch.Size([320])\n",
      " unet.encoders.1.0.groupnorm_feature.bias torch.Size([320])\n",
      " unet.encoders.1.0.conv_feature.weight torch.Size([320, 320, 3, 3])\n",
      " unet.encoders.1.0.conv_feature.bias torch.Size([320])\n",
      " unet.encoders.1.0.linear_time.weight torch.Size([320, 1280])\n",
      " unet.encoders.1.0.linear_time.bias torch.Size([320])\n",
      " unet.encoders.1.0.groupnorm_merged.weight torch.Size([320])\n",
      " unet.encoders.1.0.groupnorm_merged.bias torch.Size([320])\n",
      " unet.encoders.1.0.conv_merged.weight torch.Size([320, 320, 3, 3])\n",
      " unet.encoders.1.0.conv_merged.bias torch.Size([320])\n",
      " unet.encoders.1.1.groupnorm.weight torch.Size([320])\n",
      " unet.encoders.1.1.groupnorm.bias torch.Size([320])\n",
      " unet.encoders.1.1.conv_input.weight torch.Size([320, 320, 1, 1])\n",
      " unet.encoders.1.1.conv_input.bias torch.Size([320])\n",
      " unet.encoders.1.1.layernorm_1.weight torch.Size([320])\n",
      " unet.encoders.1.1.layernorm_1.bias torch.Size([320])\n",
      " unet.encoders.1.1.attention_1.in_proj.originLinear.weight torch.Size([960, 320])\n",
      " unet.encoders.1.1.attention_1.in_proj.loraA.weight torch.Size([8, 320])\n",
      " unet.encoders.1.1.attention_1.in_proj.loraB.weight torch.Size([960, 8])\n",
      " unet.encoders.1.1.attention_1.out_proj.originLinear.weight torch.Size([320, 320])\n",
      " unet.encoders.1.1.attention_1.out_proj.originLinear.bias torch.Size([320])\n",
      " unet.encoders.1.1.attention_1.out_proj.loraA.weight torch.Size([8, 320])\n",
      " unet.encoders.1.1.attention_1.out_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.encoders.1.1.layernorm_2.weight torch.Size([320])\n",
      " unet.encoders.1.1.layernorm_2.bias torch.Size([320])\n",
      " unet.encoders.1.1.attention_2.q_proj.originLinear.weight torch.Size([320, 320])\n",
      " unet.encoders.1.1.attention_2.q_proj.loraA.weight torch.Size([8, 320])\n",
      " unet.encoders.1.1.attention_2.q_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.encoders.1.1.attention_2.k_proj.originLinear.weight torch.Size([320, 768])\n",
      " unet.encoders.1.1.attention_2.k_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.encoders.1.1.attention_2.k_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.encoders.1.1.attention_2.v_proj.originLinear.weight torch.Size([320, 768])\n",
      " unet.encoders.1.1.attention_2.v_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.encoders.1.1.attention_2.v_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.encoders.1.1.attention_2.out_proj.originLinear.weight torch.Size([320, 320])\n",
      " unet.encoders.1.1.attention_2.out_proj.originLinear.bias torch.Size([320])\n",
      " unet.encoders.1.1.attention_2.out_proj.loraA.weight torch.Size([8, 320])\n",
      " unet.encoders.1.1.attention_2.out_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.encoders.1.1.layernorm_3.weight torch.Size([320])\n",
      " unet.encoders.1.1.layernorm_3.bias torch.Size([320])\n",
      " unet.encoders.1.1.linear_geglu_1.weight torch.Size([2560, 320])\n",
      " unet.encoders.1.1.linear_geglu_1.bias torch.Size([2560])\n",
      " unet.encoders.1.1.linear_geglu_2.weight torch.Size([320, 1280])\n",
      " unet.encoders.1.1.linear_geglu_2.bias torch.Size([320])\n",
      " unet.encoders.1.1.conv_output.weight torch.Size([320, 320, 1, 1])\n",
      " unet.encoders.1.1.conv_output.bias torch.Size([320])\n",
      " unet.encoders.2.0.groupnorm_feature.weight torch.Size([320])\n",
      " unet.encoders.2.0.groupnorm_feature.bias torch.Size([320])\n",
      " unet.encoders.2.0.conv_feature.weight torch.Size([320, 320, 3, 3])\n",
      " unet.encoders.2.0.conv_feature.bias torch.Size([320])\n",
      " unet.encoders.2.0.linear_time.weight torch.Size([320, 1280])\n",
      " unet.encoders.2.0.linear_time.bias torch.Size([320])\n",
      " unet.encoders.2.0.groupnorm_merged.weight torch.Size([320])\n",
      " unet.encoders.2.0.groupnorm_merged.bias torch.Size([320])\n",
      " unet.encoders.2.0.conv_merged.weight torch.Size([320, 320, 3, 3])\n",
      " unet.encoders.2.0.conv_merged.bias torch.Size([320])\n",
      " unet.encoders.2.1.groupnorm.weight torch.Size([320])\n",
      " unet.encoders.2.1.groupnorm.bias torch.Size([320])\n",
      " unet.encoders.2.1.conv_input.weight torch.Size([320, 320, 1, 1])\n",
      " unet.encoders.2.1.conv_input.bias torch.Size([320])\n",
      " unet.encoders.2.1.layernorm_1.weight torch.Size([320])\n",
      " unet.encoders.2.1.layernorm_1.bias torch.Size([320])\n",
      " unet.encoders.2.1.attention_1.in_proj.originLinear.weight torch.Size([960, 320])\n",
      " unet.encoders.2.1.attention_1.in_proj.loraA.weight torch.Size([8, 320])\n",
      " unet.encoders.2.1.attention_1.in_proj.loraB.weight torch.Size([960, 8])\n",
      " unet.encoders.2.1.attention_1.out_proj.originLinear.weight torch.Size([320, 320])\n",
      " unet.encoders.2.1.attention_1.out_proj.originLinear.bias torch.Size([320])\n",
      " unet.encoders.2.1.attention_1.out_proj.loraA.weight torch.Size([8, 320])\n",
      " unet.encoders.2.1.attention_1.out_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.encoders.2.1.layernorm_2.weight torch.Size([320])\n",
      " unet.encoders.2.1.layernorm_2.bias torch.Size([320])\n",
      " unet.encoders.2.1.attention_2.q_proj.originLinear.weight torch.Size([320, 320])\n",
      " unet.encoders.2.1.attention_2.q_proj.loraA.weight torch.Size([8, 320])\n",
      " unet.encoders.2.1.attention_2.q_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.encoders.2.1.attention_2.k_proj.originLinear.weight torch.Size([320, 768])\n",
      " unet.encoders.2.1.attention_2.k_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.encoders.2.1.attention_2.k_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.encoders.2.1.attention_2.v_proj.originLinear.weight torch.Size([320, 768])\n",
      " unet.encoders.2.1.attention_2.v_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.encoders.2.1.attention_2.v_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.encoders.2.1.attention_2.out_proj.originLinear.weight torch.Size([320, 320])\n",
      " unet.encoders.2.1.attention_2.out_proj.originLinear.bias torch.Size([320])\n",
      " unet.encoders.2.1.attention_2.out_proj.loraA.weight torch.Size([8, 320])\n",
      " unet.encoders.2.1.attention_2.out_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.encoders.2.1.layernorm_3.weight torch.Size([320])\n",
      " unet.encoders.2.1.layernorm_3.bias torch.Size([320])\n",
      " unet.encoders.2.1.linear_geglu_1.weight torch.Size([2560, 320])\n",
      " unet.encoders.2.1.linear_geglu_1.bias torch.Size([2560])\n",
      " unet.encoders.2.1.linear_geglu_2.weight torch.Size([320, 1280])\n",
      " unet.encoders.2.1.linear_geglu_2.bias torch.Size([320])\n",
      " unet.encoders.2.1.conv_output.weight torch.Size([320, 320, 1, 1])\n",
      " unet.encoders.2.1.conv_output.bias torch.Size([320])\n",
      " unet.encoders.3.0.weight torch.Size([320, 320, 3, 3])\n",
      " unet.encoders.3.0.bias torch.Size([320])\n",
      " unet.encoders.4.0.groupnorm_feature.weight torch.Size([320])\n",
      " unet.encoders.4.0.groupnorm_feature.bias torch.Size([320])\n",
      " unet.encoders.4.0.conv_feature.weight torch.Size([640, 320, 3, 3])\n",
      " unet.encoders.4.0.conv_feature.bias torch.Size([640])\n",
      " unet.encoders.4.0.linear_time.weight torch.Size([640, 1280])\n",
      " unet.encoders.4.0.linear_time.bias torch.Size([640])\n",
      " unet.encoders.4.0.groupnorm_merged.weight torch.Size([640])\n",
      " unet.encoders.4.0.groupnorm_merged.bias torch.Size([640])\n",
      " unet.encoders.4.0.conv_merged.weight torch.Size([640, 640, 3, 3])\n",
      " unet.encoders.4.0.conv_merged.bias torch.Size([640])\n",
      " unet.encoders.4.0.residual_layer.weight torch.Size([640, 320, 1, 1])\n",
      " unet.encoders.4.0.residual_layer.bias torch.Size([640])\n",
      " unet.encoders.4.1.groupnorm.weight torch.Size([640])\n",
      " unet.encoders.4.1.groupnorm.bias torch.Size([640])\n",
      " unet.encoders.4.1.conv_input.weight torch.Size([640, 640, 1, 1])\n",
      " unet.encoders.4.1.conv_input.bias torch.Size([640])\n",
      " unet.encoders.4.1.layernorm_1.weight torch.Size([640])\n",
      " unet.encoders.4.1.layernorm_1.bias torch.Size([640])\n",
      " unet.encoders.4.1.attention_1.in_proj.originLinear.weight torch.Size([1920, 640])\n",
      " unet.encoders.4.1.attention_1.in_proj.loraA.weight torch.Size([8, 640])\n",
      " unet.encoders.4.1.attention_1.in_proj.loraB.weight torch.Size([1920, 8])\n",
      " unet.encoders.4.1.attention_1.out_proj.originLinear.weight torch.Size([640, 640])\n",
      " unet.encoders.4.1.attention_1.out_proj.originLinear.bias torch.Size([640])\n",
      " unet.encoders.4.1.attention_1.out_proj.loraA.weight torch.Size([8, 640])\n",
      " unet.encoders.4.1.attention_1.out_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.encoders.4.1.layernorm_2.weight torch.Size([640])\n",
      " unet.encoders.4.1.layernorm_2.bias torch.Size([640])\n",
      " unet.encoders.4.1.attention_2.q_proj.originLinear.weight torch.Size([640, 640])\n",
      " unet.encoders.4.1.attention_2.q_proj.loraA.weight torch.Size([8, 640])\n",
      " unet.encoders.4.1.attention_2.q_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.encoders.4.1.attention_2.k_proj.originLinear.weight torch.Size([640, 768])\n",
      " unet.encoders.4.1.attention_2.k_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.encoders.4.1.attention_2.k_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.encoders.4.1.attention_2.v_proj.originLinear.weight torch.Size([640, 768])\n",
      " unet.encoders.4.1.attention_2.v_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.encoders.4.1.attention_2.v_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.encoders.4.1.attention_2.out_proj.originLinear.weight torch.Size([640, 640])\n",
      " unet.encoders.4.1.attention_2.out_proj.originLinear.bias torch.Size([640])\n",
      " unet.encoders.4.1.attention_2.out_proj.loraA.weight torch.Size([8, 640])\n",
      " unet.encoders.4.1.attention_2.out_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.encoders.4.1.layernorm_3.weight torch.Size([640])\n",
      " unet.encoders.4.1.layernorm_3.bias torch.Size([640])\n",
      " unet.encoders.4.1.linear_geglu_1.weight torch.Size([5120, 640])\n",
      " unet.encoders.4.1.linear_geglu_1.bias torch.Size([5120])\n",
      " unet.encoders.4.1.linear_geglu_2.weight torch.Size([640, 2560])\n",
      " unet.encoders.4.1.linear_geglu_2.bias torch.Size([640])\n",
      " unet.encoders.4.1.conv_output.weight torch.Size([640, 640, 1, 1])\n",
      " unet.encoders.4.1.conv_output.bias torch.Size([640])\n",
      " unet.encoders.5.0.groupnorm_feature.weight torch.Size([640])\n",
      " unet.encoders.5.0.groupnorm_feature.bias torch.Size([640])\n",
      " unet.encoders.5.0.conv_feature.weight torch.Size([640, 640, 3, 3])\n",
      " unet.encoders.5.0.conv_feature.bias torch.Size([640])\n",
      " unet.encoders.5.0.linear_time.weight torch.Size([640, 1280])\n",
      " unet.encoders.5.0.linear_time.bias torch.Size([640])\n",
      " unet.encoders.5.0.groupnorm_merged.weight torch.Size([640])\n",
      " unet.encoders.5.0.groupnorm_merged.bias torch.Size([640])\n",
      " unet.encoders.5.0.conv_merged.weight torch.Size([640, 640, 3, 3])\n",
      " unet.encoders.5.0.conv_merged.bias torch.Size([640])\n",
      " unet.encoders.5.1.groupnorm.weight torch.Size([640])\n",
      " unet.encoders.5.1.groupnorm.bias torch.Size([640])\n",
      " unet.encoders.5.1.conv_input.weight torch.Size([640, 640, 1, 1])\n",
      " unet.encoders.5.1.conv_input.bias torch.Size([640])\n",
      " unet.encoders.5.1.layernorm_1.weight torch.Size([640])\n",
      " unet.encoders.5.1.layernorm_1.bias torch.Size([640])\n",
      " unet.encoders.5.1.attention_1.in_proj.originLinear.weight torch.Size([1920, 640])\n",
      " unet.encoders.5.1.attention_1.in_proj.loraA.weight torch.Size([8, 640])\n",
      " unet.encoders.5.1.attention_1.in_proj.loraB.weight torch.Size([1920, 8])\n",
      " unet.encoders.5.1.attention_1.out_proj.originLinear.weight torch.Size([640, 640])\n",
      " unet.encoders.5.1.attention_1.out_proj.originLinear.bias torch.Size([640])\n",
      " unet.encoders.5.1.attention_1.out_proj.loraA.weight torch.Size([8, 640])\n",
      " unet.encoders.5.1.attention_1.out_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.encoders.5.1.layernorm_2.weight torch.Size([640])\n",
      " unet.encoders.5.1.layernorm_2.bias torch.Size([640])\n",
      " unet.encoders.5.1.attention_2.q_proj.originLinear.weight torch.Size([640, 640])\n",
      " unet.encoders.5.1.attention_2.q_proj.loraA.weight torch.Size([8, 640])\n",
      " unet.encoders.5.1.attention_2.q_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.encoders.5.1.attention_2.k_proj.originLinear.weight torch.Size([640, 768])\n",
      " unet.encoders.5.1.attention_2.k_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.encoders.5.1.attention_2.k_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.encoders.5.1.attention_2.v_proj.originLinear.weight torch.Size([640, 768])\n",
      " unet.encoders.5.1.attention_2.v_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.encoders.5.1.attention_2.v_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.encoders.5.1.attention_2.out_proj.originLinear.weight torch.Size([640, 640])\n",
      " unet.encoders.5.1.attention_2.out_proj.originLinear.bias torch.Size([640])\n",
      " unet.encoders.5.1.attention_2.out_proj.loraA.weight torch.Size([8, 640])\n",
      " unet.encoders.5.1.attention_2.out_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.encoders.5.1.layernorm_3.weight torch.Size([640])\n",
      " unet.encoders.5.1.layernorm_3.bias torch.Size([640])\n",
      " unet.encoders.5.1.linear_geglu_1.weight torch.Size([5120, 640])\n",
      " unet.encoders.5.1.linear_geglu_1.bias torch.Size([5120])\n",
      " unet.encoders.5.1.linear_geglu_2.weight torch.Size([640, 2560])\n",
      " unet.encoders.5.1.linear_geglu_2.bias torch.Size([640])\n",
      " unet.encoders.5.1.conv_output.weight torch.Size([640, 640, 1, 1])\n",
      " unet.encoders.5.1.conv_output.bias torch.Size([640])\n",
      " unet.encoders.6.0.weight torch.Size([640, 640, 3, 3])\n",
      " unet.encoders.6.0.bias torch.Size([640])\n",
      " unet.encoders.7.0.groupnorm_feature.weight torch.Size([640])\n",
      " unet.encoders.7.0.groupnorm_feature.bias torch.Size([640])\n",
      " unet.encoders.7.0.conv_feature.weight torch.Size([1280, 640, 3, 3])\n",
      " unet.encoders.7.0.conv_feature.bias torch.Size([1280])\n",
      " unet.encoders.7.0.linear_time.weight torch.Size([1280, 1280])\n",
      " unet.encoders.7.0.linear_time.bias torch.Size([1280])\n",
      " unet.encoders.7.0.groupnorm_merged.weight torch.Size([1280])\n",
      " unet.encoders.7.0.groupnorm_merged.bias torch.Size([1280])\n",
      " unet.encoders.7.0.conv_merged.weight torch.Size([1280, 1280, 3, 3])\n",
      " unet.encoders.7.0.conv_merged.bias torch.Size([1280])\n",
      " unet.encoders.7.0.residual_layer.weight torch.Size([1280, 640, 1, 1])\n",
      " unet.encoders.7.0.residual_layer.bias torch.Size([1280])\n",
      " unet.encoders.7.1.groupnorm.weight torch.Size([1280])\n",
      " unet.encoders.7.1.groupnorm.bias torch.Size([1280])\n",
      " unet.encoders.7.1.conv_input.weight torch.Size([1280, 1280, 1, 1])\n",
      " unet.encoders.7.1.conv_input.bias torch.Size([1280])\n",
      " unet.encoders.7.1.layernorm_1.weight torch.Size([1280])\n",
      " unet.encoders.7.1.layernorm_1.bias torch.Size([1280])\n",
      " unet.encoders.7.1.attention_1.in_proj.originLinear.weight torch.Size([3840, 1280])\n",
      " unet.encoders.7.1.attention_1.in_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.encoders.7.1.attention_1.in_proj.loraB.weight torch.Size([3840, 8])\n",
      " unet.encoders.7.1.attention_1.out_proj.originLinear.weight torch.Size([1280, 1280])\n",
      " unet.encoders.7.1.attention_1.out_proj.originLinear.bias torch.Size([1280])\n",
      " unet.encoders.7.1.attention_1.out_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.encoders.7.1.attention_1.out_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.encoders.7.1.layernorm_2.weight torch.Size([1280])\n",
      " unet.encoders.7.1.layernorm_2.bias torch.Size([1280])\n",
      " unet.encoders.7.1.attention_2.q_proj.originLinear.weight torch.Size([1280, 1280])\n",
      " unet.encoders.7.1.attention_2.q_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.encoders.7.1.attention_2.q_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.encoders.7.1.attention_2.k_proj.originLinear.weight torch.Size([1280, 768])\n",
      " unet.encoders.7.1.attention_2.k_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.encoders.7.1.attention_2.k_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.encoders.7.1.attention_2.v_proj.originLinear.weight torch.Size([1280, 768])\n",
      " unet.encoders.7.1.attention_2.v_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.encoders.7.1.attention_2.v_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.encoders.7.1.attention_2.out_proj.originLinear.weight torch.Size([1280, 1280])\n",
      " unet.encoders.7.1.attention_2.out_proj.originLinear.bias torch.Size([1280])\n",
      " unet.encoders.7.1.attention_2.out_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.encoders.7.1.attention_2.out_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.encoders.7.1.layernorm_3.weight torch.Size([1280])\n",
      " unet.encoders.7.1.layernorm_3.bias torch.Size([1280])\n",
      " unet.encoders.7.1.linear_geglu_1.weight torch.Size([10240, 1280])\n",
      " unet.encoders.7.1.linear_geglu_1.bias torch.Size([10240])\n",
      " unet.encoders.7.1.linear_geglu_2.weight torch.Size([1280, 5120])\n",
      " unet.encoders.7.1.linear_geglu_2.bias torch.Size([1280])\n",
      " unet.encoders.7.1.conv_output.weight torch.Size([1280, 1280, 1, 1])\n",
      " unet.encoders.7.1.conv_output.bias torch.Size([1280])\n",
      " unet.encoders.8.0.groupnorm_feature.weight torch.Size([1280])\n",
      " unet.encoders.8.0.groupnorm_feature.bias torch.Size([1280])\n",
      " unet.encoders.8.0.conv_feature.weight torch.Size([1280, 1280, 3, 3])\n",
      " unet.encoders.8.0.conv_feature.bias torch.Size([1280])\n",
      " unet.encoders.8.0.linear_time.weight torch.Size([1280, 1280])\n",
      " unet.encoders.8.0.linear_time.bias torch.Size([1280])\n",
      " unet.encoders.8.0.groupnorm_merged.weight torch.Size([1280])\n",
      " unet.encoders.8.0.groupnorm_merged.bias torch.Size([1280])\n",
      " unet.encoders.8.0.conv_merged.weight torch.Size([1280, 1280, 3, 3])\n",
      " unet.encoders.8.0.conv_merged.bias torch.Size([1280])\n",
      " unet.encoders.8.1.groupnorm.weight torch.Size([1280])\n",
      " unet.encoders.8.1.groupnorm.bias torch.Size([1280])\n",
      " unet.encoders.8.1.conv_input.weight torch.Size([1280, 1280, 1, 1])\n",
      " unet.encoders.8.1.conv_input.bias torch.Size([1280])\n",
      " unet.encoders.8.1.layernorm_1.weight torch.Size([1280])\n",
      " unet.encoders.8.1.layernorm_1.bias torch.Size([1280])\n",
      " unet.encoders.8.1.attention_1.in_proj.originLinear.weight torch.Size([3840, 1280])\n",
      " unet.encoders.8.1.attention_1.in_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.encoders.8.1.attention_1.in_proj.loraB.weight torch.Size([3840, 8])\n",
      " unet.encoders.8.1.attention_1.out_proj.originLinear.weight torch.Size([1280, 1280])\n",
      " unet.encoders.8.1.attention_1.out_proj.originLinear.bias torch.Size([1280])\n",
      " unet.encoders.8.1.attention_1.out_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.encoders.8.1.attention_1.out_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.encoders.8.1.layernorm_2.weight torch.Size([1280])\n",
      " unet.encoders.8.1.layernorm_2.bias torch.Size([1280])\n",
      " unet.encoders.8.1.attention_2.q_proj.originLinear.weight torch.Size([1280, 1280])\n",
      " unet.encoders.8.1.attention_2.q_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.encoders.8.1.attention_2.q_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.encoders.8.1.attention_2.k_proj.originLinear.weight torch.Size([1280, 768])\n",
      " unet.encoders.8.1.attention_2.k_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.encoders.8.1.attention_2.k_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.encoders.8.1.attention_2.v_proj.originLinear.weight torch.Size([1280, 768])\n",
      " unet.encoders.8.1.attention_2.v_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.encoders.8.1.attention_2.v_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.encoders.8.1.attention_2.out_proj.originLinear.weight torch.Size([1280, 1280])\n",
      " unet.encoders.8.1.attention_2.out_proj.originLinear.bias torch.Size([1280])\n",
      " unet.encoders.8.1.attention_2.out_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.encoders.8.1.attention_2.out_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.encoders.8.1.layernorm_3.weight torch.Size([1280])\n",
      " unet.encoders.8.1.layernorm_3.bias torch.Size([1280])\n",
      " unet.encoders.8.1.linear_geglu_1.weight torch.Size([10240, 1280])\n",
      " unet.encoders.8.1.linear_geglu_1.bias torch.Size([10240])\n",
      " unet.encoders.8.1.linear_geglu_2.weight torch.Size([1280, 5120])\n",
      " unet.encoders.8.1.linear_geglu_2.bias torch.Size([1280])\n",
      " unet.encoders.8.1.conv_output.weight torch.Size([1280, 1280, 1, 1])\n",
      " unet.encoders.8.1.conv_output.bias torch.Size([1280])\n",
      " unet.encoders.9.0.weight torch.Size([1280, 1280, 3, 3])\n",
      " unet.encoders.9.0.bias torch.Size([1280])\n",
      " unet.encoders.10.0.groupnorm_feature.weight torch.Size([1280])\n",
      " unet.encoders.10.0.groupnorm_feature.bias torch.Size([1280])\n",
      " unet.encoders.10.0.conv_feature.weight torch.Size([1280, 1280, 3, 3])\n",
      " unet.encoders.10.0.conv_feature.bias torch.Size([1280])\n",
      " unet.encoders.10.0.linear_time.weight torch.Size([1280, 1280])\n",
      " unet.encoders.10.0.linear_time.bias torch.Size([1280])\n",
      " unet.encoders.10.0.groupnorm_merged.weight torch.Size([1280])\n",
      " unet.encoders.10.0.groupnorm_merged.bias torch.Size([1280])\n",
      " unet.encoders.10.0.conv_merged.weight torch.Size([1280, 1280, 3, 3])\n",
      " unet.encoders.10.0.conv_merged.bias torch.Size([1280])\n",
      " unet.encoders.11.0.groupnorm_feature.weight torch.Size([1280])\n",
      " unet.encoders.11.0.groupnorm_feature.bias torch.Size([1280])\n",
      " unet.encoders.11.0.conv_feature.weight torch.Size([1280, 1280, 3, 3])\n",
      " unet.encoders.11.0.conv_feature.bias torch.Size([1280])\n",
      " unet.encoders.11.0.linear_time.weight torch.Size([1280, 1280])\n",
      " unet.encoders.11.0.linear_time.bias torch.Size([1280])\n",
      " unet.encoders.11.0.groupnorm_merged.weight torch.Size([1280])\n",
      " unet.encoders.11.0.groupnorm_merged.bias torch.Size([1280])\n",
      " unet.encoders.11.0.conv_merged.weight torch.Size([1280, 1280, 3, 3])\n",
      " unet.encoders.11.0.conv_merged.bias torch.Size([1280])\n",
      " unet.bottleneck.0.groupnorm_feature.weight torch.Size([1280])\n",
      " unet.bottleneck.0.groupnorm_feature.bias torch.Size([1280])\n",
      " unet.bottleneck.0.conv_feature.weight torch.Size([1280, 1280, 3, 3])\n",
      " unet.bottleneck.0.conv_feature.bias torch.Size([1280])\n",
      " unet.bottleneck.0.linear_time.weight torch.Size([1280, 1280])\n",
      " unet.bottleneck.0.linear_time.bias torch.Size([1280])\n",
      " unet.bottleneck.0.groupnorm_merged.weight torch.Size([1280])\n",
      " unet.bottleneck.0.groupnorm_merged.bias torch.Size([1280])\n",
      " unet.bottleneck.0.conv_merged.weight torch.Size([1280, 1280, 3, 3])\n",
      " unet.bottleneck.0.conv_merged.bias torch.Size([1280])\n",
      " unet.bottleneck.1.groupnorm.weight torch.Size([1280])\n",
      " unet.bottleneck.1.groupnorm.bias torch.Size([1280])\n",
      " unet.bottleneck.1.conv_input.weight torch.Size([1280, 1280, 1, 1])\n",
      " unet.bottleneck.1.conv_input.bias torch.Size([1280])\n",
      " unet.bottleneck.1.layernorm_1.weight torch.Size([1280])\n",
      " unet.bottleneck.1.layernorm_1.bias torch.Size([1280])\n",
      " unet.bottleneck.1.attention_1.in_proj.originLinear.weight torch.Size([3840, 1280])\n",
      " unet.bottleneck.1.attention_1.in_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.bottleneck.1.attention_1.in_proj.loraB.weight torch.Size([3840, 8])\n",
      " unet.bottleneck.1.attention_1.out_proj.originLinear.weight torch.Size([1280, 1280])\n",
      " unet.bottleneck.1.attention_1.out_proj.originLinear.bias torch.Size([1280])\n",
      " unet.bottleneck.1.attention_1.out_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.bottleneck.1.attention_1.out_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.bottleneck.1.layernorm_2.weight torch.Size([1280])\n",
      " unet.bottleneck.1.layernorm_2.bias torch.Size([1280])\n",
      " unet.bottleneck.1.attention_2.q_proj.originLinear.weight torch.Size([1280, 1280])\n",
      " unet.bottleneck.1.attention_2.q_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.bottleneck.1.attention_2.q_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.bottleneck.1.attention_2.k_proj.originLinear.weight torch.Size([1280, 768])\n",
      " unet.bottleneck.1.attention_2.k_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.bottleneck.1.attention_2.k_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.bottleneck.1.attention_2.v_proj.originLinear.weight torch.Size([1280, 768])\n",
      " unet.bottleneck.1.attention_2.v_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.bottleneck.1.attention_2.v_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.bottleneck.1.attention_2.out_proj.originLinear.weight torch.Size([1280, 1280])\n",
      " unet.bottleneck.1.attention_2.out_proj.originLinear.bias torch.Size([1280])\n",
      " unet.bottleneck.1.attention_2.out_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.bottleneck.1.attention_2.out_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.bottleneck.1.layernorm_3.weight torch.Size([1280])\n",
      " unet.bottleneck.1.layernorm_3.bias torch.Size([1280])\n",
      " unet.bottleneck.1.linear_geglu_1.weight torch.Size([10240, 1280])\n",
      " unet.bottleneck.1.linear_geglu_1.bias torch.Size([10240])\n",
      " unet.bottleneck.1.linear_geglu_2.weight torch.Size([1280, 5120])\n",
      " unet.bottleneck.1.linear_geglu_2.bias torch.Size([1280])\n",
      " unet.bottleneck.1.conv_output.weight torch.Size([1280, 1280, 1, 1])\n",
      " unet.bottleneck.1.conv_output.bias torch.Size([1280])\n",
      " unet.bottleneck.2.groupnorm_feature.weight torch.Size([1280])\n",
      " unet.bottleneck.2.groupnorm_feature.bias torch.Size([1280])\n",
      " unet.bottleneck.2.conv_feature.weight torch.Size([1280, 1280, 3, 3])\n",
      " unet.bottleneck.2.conv_feature.bias torch.Size([1280])\n",
      " unet.bottleneck.2.linear_time.weight torch.Size([1280, 1280])\n",
      " unet.bottleneck.2.linear_time.bias torch.Size([1280])\n",
      " unet.bottleneck.2.groupnorm_merged.weight torch.Size([1280])\n",
      " unet.bottleneck.2.groupnorm_merged.bias torch.Size([1280])\n",
      " unet.bottleneck.2.conv_merged.weight torch.Size([1280, 1280, 3, 3])\n",
      " unet.bottleneck.2.conv_merged.bias torch.Size([1280])\n",
      " unet.decoders.0.0.groupnorm_feature.weight torch.Size([2560])\n",
      " unet.decoders.0.0.groupnorm_feature.bias torch.Size([2560])\n",
      " unet.decoders.0.0.conv_feature.weight torch.Size([1280, 2560, 3, 3])\n",
      " unet.decoders.0.0.conv_feature.bias torch.Size([1280])\n",
      " unet.decoders.0.0.linear_time.weight torch.Size([1280, 1280])\n",
      " unet.decoders.0.0.linear_time.bias torch.Size([1280])\n",
      " unet.decoders.0.0.groupnorm_merged.weight torch.Size([1280])\n",
      " unet.decoders.0.0.groupnorm_merged.bias torch.Size([1280])\n",
      " unet.decoders.0.0.conv_merged.weight torch.Size([1280, 1280, 3, 3])\n",
      " unet.decoders.0.0.conv_merged.bias torch.Size([1280])\n",
      " unet.decoders.0.0.residual_layer.weight torch.Size([1280, 2560, 1, 1])\n",
      " unet.decoders.0.0.residual_layer.bias torch.Size([1280])\n",
      " unet.decoders.1.0.groupnorm_feature.weight torch.Size([2560])\n",
      " unet.decoders.1.0.groupnorm_feature.bias torch.Size([2560])\n",
      " unet.decoders.1.0.conv_feature.weight torch.Size([1280, 2560, 3, 3])\n",
      " unet.decoders.1.0.conv_feature.bias torch.Size([1280])\n",
      " unet.decoders.1.0.linear_time.weight torch.Size([1280, 1280])\n",
      " unet.decoders.1.0.linear_time.bias torch.Size([1280])\n",
      " unet.decoders.1.0.groupnorm_merged.weight torch.Size([1280])\n",
      " unet.decoders.1.0.groupnorm_merged.bias torch.Size([1280])\n",
      " unet.decoders.1.0.conv_merged.weight torch.Size([1280, 1280, 3, 3])\n",
      " unet.decoders.1.0.conv_merged.bias torch.Size([1280])\n",
      " unet.decoders.1.0.residual_layer.weight torch.Size([1280, 2560, 1, 1])\n",
      " unet.decoders.1.0.residual_layer.bias torch.Size([1280])\n",
      " unet.decoders.2.0.groupnorm_feature.weight torch.Size([2560])\n",
      " unet.decoders.2.0.groupnorm_feature.bias torch.Size([2560])\n",
      " unet.decoders.2.0.conv_feature.weight torch.Size([1280, 2560, 3, 3])\n",
      " unet.decoders.2.0.conv_feature.bias torch.Size([1280])\n",
      " unet.decoders.2.0.linear_time.weight torch.Size([1280, 1280])\n",
      " unet.decoders.2.0.linear_time.bias torch.Size([1280])\n",
      " unet.decoders.2.0.groupnorm_merged.weight torch.Size([1280])\n",
      " unet.decoders.2.0.groupnorm_merged.bias torch.Size([1280])\n",
      " unet.decoders.2.0.conv_merged.weight torch.Size([1280, 1280, 3, 3])\n",
      " unet.decoders.2.0.conv_merged.bias torch.Size([1280])\n",
      " unet.decoders.2.0.residual_layer.weight torch.Size([1280, 2560, 1, 1])\n",
      " unet.decoders.2.0.residual_layer.bias torch.Size([1280])\n",
      " unet.decoders.2.1.conv.weight torch.Size([1280, 1280, 3, 3])\n",
      " unet.decoders.2.1.conv.bias torch.Size([1280])\n",
      " unet.decoders.3.0.groupnorm_feature.weight torch.Size([2560])\n",
      " unet.decoders.3.0.groupnorm_feature.bias torch.Size([2560])\n",
      " unet.decoders.3.0.conv_feature.weight torch.Size([1280, 2560, 3, 3])\n",
      " unet.decoders.3.0.conv_feature.bias torch.Size([1280])\n",
      " unet.decoders.3.0.linear_time.weight torch.Size([1280, 1280])\n",
      " unet.decoders.3.0.linear_time.bias torch.Size([1280])\n",
      " unet.decoders.3.0.groupnorm_merged.weight torch.Size([1280])\n",
      " unet.decoders.3.0.groupnorm_merged.bias torch.Size([1280])\n",
      " unet.decoders.3.0.conv_merged.weight torch.Size([1280, 1280, 3, 3])\n",
      " unet.decoders.3.0.conv_merged.bias torch.Size([1280])\n",
      " unet.decoders.3.0.residual_layer.weight torch.Size([1280, 2560, 1, 1])\n",
      " unet.decoders.3.0.residual_layer.bias torch.Size([1280])\n",
      " unet.decoders.3.1.groupnorm.weight torch.Size([1280])\n",
      " unet.decoders.3.1.groupnorm.bias torch.Size([1280])\n",
      " unet.decoders.3.1.conv_input.weight torch.Size([1280, 1280, 1, 1])\n",
      " unet.decoders.3.1.conv_input.bias torch.Size([1280])\n",
      " unet.decoders.3.1.layernorm_1.weight torch.Size([1280])\n",
      " unet.decoders.3.1.layernorm_1.bias torch.Size([1280])\n",
      " unet.decoders.3.1.attention_1.in_proj.originLinear.weight torch.Size([3840, 1280])\n",
      " unet.decoders.3.1.attention_1.in_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.decoders.3.1.attention_1.in_proj.loraB.weight torch.Size([3840, 8])\n",
      " unet.decoders.3.1.attention_1.out_proj.originLinear.weight torch.Size([1280, 1280])\n",
      " unet.decoders.3.1.attention_1.out_proj.originLinear.bias torch.Size([1280])\n",
      " unet.decoders.3.1.attention_1.out_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.decoders.3.1.attention_1.out_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.decoders.3.1.layernorm_2.weight torch.Size([1280])\n",
      " unet.decoders.3.1.layernorm_2.bias torch.Size([1280])\n",
      " unet.decoders.3.1.attention_2.q_proj.originLinear.weight torch.Size([1280, 1280])\n",
      " unet.decoders.3.1.attention_2.q_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.decoders.3.1.attention_2.q_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.decoders.3.1.attention_2.k_proj.originLinear.weight torch.Size([1280, 768])\n",
      " unet.decoders.3.1.attention_2.k_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.decoders.3.1.attention_2.k_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.decoders.3.1.attention_2.v_proj.originLinear.weight torch.Size([1280, 768])\n",
      " unet.decoders.3.1.attention_2.v_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.decoders.3.1.attention_2.v_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.decoders.3.1.attention_2.out_proj.originLinear.weight torch.Size([1280, 1280])\n",
      " unet.decoders.3.1.attention_2.out_proj.originLinear.bias torch.Size([1280])\n",
      " unet.decoders.3.1.attention_2.out_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.decoders.3.1.attention_2.out_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.decoders.3.1.layernorm_3.weight torch.Size([1280])\n",
      " unet.decoders.3.1.layernorm_3.bias torch.Size([1280])\n",
      " unet.decoders.3.1.linear_geglu_1.weight torch.Size([10240, 1280])\n",
      " unet.decoders.3.1.linear_geglu_1.bias torch.Size([10240])\n",
      " unet.decoders.3.1.linear_geglu_2.weight torch.Size([1280, 5120])\n",
      " unet.decoders.3.1.linear_geglu_2.bias torch.Size([1280])\n",
      " unet.decoders.3.1.conv_output.weight torch.Size([1280, 1280, 1, 1])\n",
      " unet.decoders.3.1.conv_output.bias torch.Size([1280])\n",
      " unet.decoders.4.0.groupnorm_feature.weight torch.Size([2560])\n",
      " unet.decoders.4.0.groupnorm_feature.bias torch.Size([2560])\n",
      " unet.decoders.4.0.conv_feature.weight torch.Size([1280, 2560, 3, 3])\n",
      " unet.decoders.4.0.conv_feature.bias torch.Size([1280])\n",
      " unet.decoders.4.0.linear_time.weight torch.Size([1280, 1280])\n",
      " unet.decoders.4.0.linear_time.bias torch.Size([1280])\n",
      " unet.decoders.4.0.groupnorm_merged.weight torch.Size([1280])\n",
      " unet.decoders.4.0.groupnorm_merged.bias torch.Size([1280])\n",
      " unet.decoders.4.0.conv_merged.weight torch.Size([1280, 1280, 3, 3])\n",
      " unet.decoders.4.0.conv_merged.bias torch.Size([1280])\n",
      " unet.decoders.4.0.residual_layer.weight torch.Size([1280, 2560, 1, 1])\n",
      " unet.decoders.4.0.residual_layer.bias torch.Size([1280])\n",
      " unet.decoders.4.1.groupnorm.weight torch.Size([1280])\n",
      " unet.decoders.4.1.groupnorm.bias torch.Size([1280])\n",
      " unet.decoders.4.1.conv_input.weight torch.Size([1280, 1280, 1, 1])\n",
      " unet.decoders.4.1.conv_input.bias torch.Size([1280])\n",
      " unet.decoders.4.1.layernorm_1.weight torch.Size([1280])\n",
      " unet.decoders.4.1.layernorm_1.bias torch.Size([1280])\n",
      " unet.decoders.4.1.attention_1.in_proj.originLinear.weight torch.Size([3840, 1280])\n",
      " unet.decoders.4.1.attention_1.in_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.decoders.4.1.attention_1.in_proj.loraB.weight torch.Size([3840, 8])\n",
      " unet.decoders.4.1.attention_1.out_proj.originLinear.weight torch.Size([1280, 1280])\n",
      " unet.decoders.4.1.attention_1.out_proj.originLinear.bias torch.Size([1280])\n",
      " unet.decoders.4.1.attention_1.out_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.decoders.4.1.attention_1.out_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.decoders.4.1.layernorm_2.weight torch.Size([1280])\n",
      " unet.decoders.4.1.layernorm_2.bias torch.Size([1280])\n",
      " unet.decoders.4.1.attention_2.q_proj.originLinear.weight torch.Size([1280, 1280])\n",
      " unet.decoders.4.1.attention_2.q_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.decoders.4.1.attention_2.q_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.decoders.4.1.attention_2.k_proj.originLinear.weight torch.Size([1280, 768])\n",
      " unet.decoders.4.1.attention_2.k_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.decoders.4.1.attention_2.k_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.decoders.4.1.attention_2.v_proj.originLinear.weight torch.Size([1280, 768])\n",
      " unet.decoders.4.1.attention_2.v_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.decoders.4.1.attention_2.v_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.decoders.4.1.attention_2.out_proj.originLinear.weight torch.Size([1280, 1280])\n",
      " unet.decoders.4.1.attention_2.out_proj.originLinear.bias torch.Size([1280])\n",
      " unet.decoders.4.1.attention_2.out_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.decoders.4.1.attention_2.out_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.decoders.4.1.layernorm_3.weight torch.Size([1280])\n",
      " unet.decoders.4.1.layernorm_3.bias torch.Size([1280])\n",
      " unet.decoders.4.1.linear_geglu_1.weight torch.Size([10240, 1280])\n",
      " unet.decoders.4.1.linear_geglu_1.bias torch.Size([10240])\n",
      " unet.decoders.4.1.linear_geglu_2.weight torch.Size([1280, 5120])\n",
      " unet.decoders.4.1.linear_geglu_2.bias torch.Size([1280])\n",
      " unet.decoders.4.1.conv_output.weight torch.Size([1280, 1280, 1, 1])\n",
      " unet.decoders.4.1.conv_output.bias torch.Size([1280])\n",
      " unet.decoders.5.0.groupnorm_feature.weight torch.Size([1920])\n",
      " unet.decoders.5.0.groupnorm_feature.bias torch.Size([1920])\n",
      " unet.decoders.5.0.conv_feature.weight torch.Size([1280, 1920, 3, 3])\n",
      " unet.decoders.5.0.conv_feature.bias torch.Size([1280])\n",
      " unet.decoders.5.0.linear_time.weight torch.Size([1280, 1280])\n",
      " unet.decoders.5.0.linear_time.bias torch.Size([1280])\n",
      " unet.decoders.5.0.groupnorm_merged.weight torch.Size([1280])\n",
      " unet.decoders.5.0.groupnorm_merged.bias torch.Size([1280])\n",
      " unet.decoders.5.0.conv_merged.weight torch.Size([1280, 1280, 3, 3])\n",
      " unet.decoders.5.0.conv_merged.bias torch.Size([1280])\n",
      " unet.decoders.5.0.residual_layer.weight torch.Size([1280, 1920, 1, 1])\n",
      " unet.decoders.5.0.residual_layer.bias torch.Size([1280])\n",
      " unet.decoders.5.1.groupnorm.weight torch.Size([1280])\n",
      " unet.decoders.5.1.groupnorm.bias torch.Size([1280])\n",
      " unet.decoders.5.1.conv_input.weight torch.Size([1280, 1280, 1, 1])\n",
      " unet.decoders.5.1.conv_input.bias torch.Size([1280])\n",
      " unet.decoders.5.1.layernorm_1.weight torch.Size([1280])\n",
      " unet.decoders.5.1.layernorm_1.bias torch.Size([1280])\n",
      " unet.decoders.5.1.attention_1.in_proj.originLinear.weight torch.Size([3840, 1280])\n",
      " unet.decoders.5.1.attention_1.in_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.decoders.5.1.attention_1.in_proj.loraB.weight torch.Size([3840, 8])\n",
      " unet.decoders.5.1.attention_1.out_proj.originLinear.weight torch.Size([1280, 1280])\n",
      " unet.decoders.5.1.attention_1.out_proj.originLinear.bias torch.Size([1280])\n",
      " unet.decoders.5.1.attention_1.out_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.decoders.5.1.attention_1.out_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.decoders.5.1.layernorm_2.weight torch.Size([1280])\n",
      " unet.decoders.5.1.layernorm_2.bias torch.Size([1280])\n",
      " unet.decoders.5.1.attention_2.q_proj.originLinear.weight torch.Size([1280, 1280])\n",
      " unet.decoders.5.1.attention_2.q_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.decoders.5.1.attention_2.q_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.decoders.5.1.attention_2.k_proj.originLinear.weight torch.Size([1280, 768])\n",
      " unet.decoders.5.1.attention_2.k_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.decoders.5.1.attention_2.k_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.decoders.5.1.attention_2.v_proj.originLinear.weight torch.Size([1280, 768])\n",
      " unet.decoders.5.1.attention_2.v_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.decoders.5.1.attention_2.v_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.decoders.5.1.attention_2.out_proj.originLinear.weight torch.Size([1280, 1280])\n",
      " unet.decoders.5.1.attention_2.out_proj.originLinear.bias torch.Size([1280])\n",
      " unet.decoders.5.1.attention_2.out_proj.loraA.weight torch.Size([8, 1280])\n",
      " unet.decoders.5.1.attention_2.out_proj.loraB.weight torch.Size([1280, 8])\n",
      " unet.decoders.5.1.layernorm_3.weight torch.Size([1280])\n",
      " unet.decoders.5.1.layernorm_3.bias torch.Size([1280])\n",
      " unet.decoders.5.1.linear_geglu_1.weight torch.Size([10240, 1280])\n",
      " unet.decoders.5.1.linear_geglu_1.bias torch.Size([10240])\n",
      " unet.decoders.5.1.linear_geglu_2.weight torch.Size([1280, 5120])\n",
      " unet.decoders.5.1.linear_geglu_2.bias torch.Size([1280])\n",
      " unet.decoders.5.1.conv_output.weight torch.Size([1280, 1280, 1, 1])\n",
      " unet.decoders.5.1.conv_output.bias torch.Size([1280])\n",
      " unet.decoders.5.2.conv.weight torch.Size([1280, 1280, 3, 3])\n",
      " unet.decoders.5.2.conv.bias torch.Size([1280])\n",
      " unet.decoders.6.0.groupnorm_feature.weight torch.Size([1920])\n",
      " unet.decoders.6.0.groupnorm_feature.bias torch.Size([1920])\n",
      " unet.decoders.6.0.conv_feature.weight torch.Size([640, 1920, 3, 3])\n",
      " unet.decoders.6.0.conv_feature.bias torch.Size([640])\n",
      " unet.decoders.6.0.linear_time.weight torch.Size([640, 1280])\n",
      " unet.decoders.6.0.linear_time.bias torch.Size([640])\n",
      " unet.decoders.6.0.groupnorm_merged.weight torch.Size([640])\n",
      " unet.decoders.6.0.groupnorm_merged.bias torch.Size([640])\n",
      " unet.decoders.6.0.conv_merged.weight torch.Size([640, 640, 3, 3])\n",
      " unet.decoders.6.0.conv_merged.bias torch.Size([640])\n",
      " unet.decoders.6.0.residual_layer.weight torch.Size([640, 1920, 1, 1])\n",
      " unet.decoders.6.0.residual_layer.bias torch.Size([640])\n",
      " unet.decoders.6.1.groupnorm.weight torch.Size([640])\n",
      " unet.decoders.6.1.groupnorm.bias torch.Size([640])\n",
      " unet.decoders.6.1.conv_input.weight torch.Size([640, 640, 1, 1])\n",
      " unet.decoders.6.1.conv_input.bias torch.Size([640])\n",
      " unet.decoders.6.1.layernorm_1.weight torch.Size([640])\n",
      " unet.decoders.6.1.layernorm_1.bias torch.Size([640])\n",
      " unet.decoders.6.1.attention_1.in_proj.originLinear.weight torch.Size([1920, 640])\n",
      " unet.decoders.6.1.attention_1.in_proj.loraA.weight torch.Size([8, 640])\n",
      " unet.decoders.6.1.attention_1.in_proj.loraB.weight torch.Size([1920, 8])\n",
      " unet.decoders.6.1.attention_1.out_proj.originLinear.weight torch.Size([640, 640])\n",
      " unet.decoders.6.1.attention_1.out_proj.originLinear.bias torch.Size([640])\n",
      " unet.decoders.6.1.attention_1.out_proj.loraA.weight torch.Size([8, 640])\n",
      " unet.decoders.6.1.attention_1.out_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.decoders.6.1.layernorm_2.weight torch.Size([640])\n",
      " unet.decoders.6.1.layernorm_2.bias torch.Size([640])\n",
      " unet.decoders.6.1.attention_2.q_proj.originLinear.weight torch.Size([640, 640])\n",
      " unet.decoders.6.1.attention_2.q_proj.loraA.weight torch.Size([8, 640])\n",
      " unet.decoders.6.1.attention_2.q_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.decoders.6.1.attention_2.k_proj.originLinear.weight torch.Size([640, 768])\n",
      " unet.decoders.6.1.attention_2.k_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.decoders.6.1.attention_2.k_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.decoders.6.1.attention_2.v_proj.originLinear.weight torch.Size([640, 768])\n",
      " unet.decoders.6.1.attention_2.v_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.decoders.6.1.attention_2.v_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.decoders.6.1.attention_2.out_proj.originLinear.weight torch.Size([640, 640])\n",
      " unet.decoders.6.1.attention_2.out_proj.originLinear.bias torch.Size([640])\n",
      " unet.decoders.6.1.attention_2.out_proj.loraA.weight torch.Size([8, 640])\n",
      " unet.decoders.6.1.attention_2.out_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.decoders.6.1.layernorm_3.weight torch.Size([640])\n",
      " unet.decoders.6.1.layernorm_3.bias torch.Size([640])\n",
      " unet.decoders.6.1.linear_geglu_1.weight torch.Size([5120, 640])\n",
      " unet.decoders.6.1.linear_geglu_1.bias torch.Size([5120])\n",
      " unet.decoders.6.1.linear_geglu_2.weight torch.Size([640, 2560])\n",
      " unet.decoders.6.1.linear_geglu_2.bias torch.Size([640])\n",
      " unet.decoders.6.1.conv_output.weight torch.Size([640, 640, 1, 1])\n",
      " unet.decoders.6.1.conv_output.bias torch.Size([640])\n",
      " unet.decoders.7.0.groupnorm_feature.weight torch.Size([1280])\n",
      " unet.decoders.7.0.groupnorm_feature.bias torch.Size([1280])\n",
      " unet.decoders.7.0.conv_feature.weight torch.Size([640, 1280, 3, 3])\n",
      " unet.decoders.7.0.conv_feature.bias torch.Size([640])\n",
      " unet.decoders.7.0.linear_time.weight torch.Size([640, 1280])\n",
      " unet.decoders.7.0.linear_time.bias torch.Size([640])\n",
      " unet.decoders.7.0.groupnorm_merged.weight torch.Size([640])\n",
      " unet.decoders.7.0.groupnorm_merged.bias torch.Size([640])\n",
      " unet.decoders.7.0.conv_merged.weight torch.Size([640, 640, 3, 3])\n",
      " unet.decoders.7.0.conv_merged.bias torch.Size([640])\n",
      " unet.decoders.7.0.residual_layer.weight torch.Size([640, 1280, 1, 1])\n",
      " unet.decoders.7.0.residual_layer.bias torch.Size([640])\n",
      " unet.decoders.7.1.groupnorm.weight torch.Size([640])\n",
      " unet.decoders.7.1.groupnorm.bias torch.Size([640])\n",
      " unet.decoders.7.1.conv_input.weight torch.Size([640, 640, 1, 1])\n",
      " unet.decoders.7.1.conv_input.bias torch.Size([640])\n",
      " unet.decoders.7.1.layernorm_1.weight torch.Size([640])\n",
      " unet.decoders.7.1.layernorm_1.bias torch.Size([640])\n",
      " unet.decoders.7.1.attention_1.in_proj.originLinear.weight torch.Size([1920, 640])\n",
      " unet.decoders.7.1.attention_1.in_proj.loraA.weight torch.Size([8, 640])\n",
      " unet.decoders.7.1.attention_1.in_proj.loraB.weight torch.Size([1920, 8])\n",
      " unet.decoders.7.1.attention_1.out_proj.originLinear.weight torch.Size([640, 640])\n",
      " unet.decoders.7.1.attention_1.out_proj.originLinear.bias torch.Size([640])\n",
      " unet.decoders.7.1.attention_1.out_proj.loraA.weight torch.Size([8, 640])\n",
      " unet.decoders.7.1.attention_1.out_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.decoders.7.1.layernorm_2.weight torch.Size([640])\n",
      " unet.decoders.7.1.layernorm_2.bias torch.Size([640])\n",
      " unet.decoders.7.1.attention_2.q_proj.originLinear.weight torch.Size([640, 640])\n",
      " unet.decoders.7.1.attention_2.q_proj.loraA.weight torch.Size([8, 640])\n",
      " unet.decoders.7.1.attention_2.q_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.decoders.7.1.attention_2.k_proj.originLinear.weight torch.Size([640, 768])\n",
      " unet.decoders.7.1.attention_2.k_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.decoders.7.1.attention_2.k_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.decoders.7.1.attention_2.v_proj.originLinear.weight torch.Size([640, 768])\n",
      " unet.decoders.7.1.attention_2.v_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.decoders.7.1.attention_2.v_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.decoders.7.1.attention_2.out_proj.originLinear.weight torch.Size([640, 640])\n",
      " unet.decoders.7.1.attention_2.out_proj.originLinear.bias torch.Size([640])\n",
      " unet.decoders.7.1.attention_2.out_proj.loraA.weight torch.Size([8, 640])\n",
      " unet.decoders.7.1.attention_2.out_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.decoders.7.1.layernorm_3.weight torch.Size([640])\n",
      " unet.decoders.7.1.layernorm_3.bias torch.Size([640])\n",
      " unet.decoders.7.1.linear_geglu_1.weight torch.Size([5120, 640])\n",
      " unet.decoders.7.1.linear_geglu_1.bias torch.Size([5120])\n",
      " unet.decoders.7.1.linear_geglu_2.weight torch.Size([640, 2560])\n",
      " unet.decoders.7.1.linear_geglu_2.bias torch.Size([640])\n",
      " unet.decoders.7.1.conv_output.weight torch.Size([640, 640, 1, 1])\n",
      " unet.decoders.7.1.conv_output.bias torch.Size([640])\n",
      " unet.decoders.8.0.groupnorm_feature.weight torch.Size([960])\n",
      " unet.decoders.8.0.groupnorm_feature.bias torch.Size([960])\n",
      " unet.decoders.8.0.conv_feature.weight torch.Size([640, 960, 3, 3])\n",
      " unet.decoders.8.0.conv_feature.bias torch.Size([640])\n",
      " unet.decoders.8.0.linear_time.weight torch.Size([640, 1280])\n",
      " unet.decoders.8.0.linear_time.bias torch.Size([640])\n",
      " unet.decoders.8.0.groupnorm_merged.weight torch.Size([640])\n",
      " unet.decoders.8.0.groupnorm_merged.bias torch.Size([640])\n",
      " unet.decoders.8.0.conv_merged.weight torch.Size([640, 640, 3, 3])\n",
      " unet.decoders.8.0.conv_merged.bias torch.Size([640])\n",
      " unet.decoders.8.0.residual_layer.weight torch.Size([640, 960, 1, 1])\n",
      " unet.decoders.8.0.residual_layer.bias torch.Size([640])\n",
      " unet.decoders.8.1.groupnorm.weight torch.Size([640])\n",
      " unet.decoders.8.1.groupnorm.bias torch.Size([640])\n",
      " unet.decoders.8.1.conv_input.weight torch.Size([640, 640, 1, 1])\n",
      " unet.decoders.8.1.conv_input.bias torch.Size([640])\n",
      " unet.decoders.8.1.layernorm_1.weight torch.Size([640])\n",
      " unet.decoders.8.1.layernorm_1.bias torch.Size([640])\n",
      " unet.decoders.8.1.attention_1.in_proj.originLinear.weight torch.Size([1920, 640])\n",
      " unet.decoders.8.1.attention_1.in_proj.loraA.weight torch.Size([8, 640])\n",
      " unet.decoders.8.1.attention_1.in_proj.loraB.weight torch.Size([1920, 8])\n",
      " unet.decoders.8.1.attention_1.out_proj.originLinear.weight torch.Size([640, 640])\n",
      " unet.decoders.8.1.attention_1.out_proj.originLinear.bias torch.Size([640])\n",
      " unet.decoders.8.1.attention_1.out_proj.loraA.weight torch.Size([8, 640])\n",
      " unet.decoders.8.1.attention_1.out_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.decoders.8.1.layernorm_2.weight torch.Size([640])\n",
      " unet.decoders.8.1.layernorm_2.bias torch.Size([640])\n",
      " unet.decoders.8.1.attention_2.q_proj.originLinear.weight torch.Size([640, 640])\n",
      " unet.decoders.8.1.attention_2.q_proj.loraA.weight torch.Size([8, 640])\n",
      " unet.decoders.8.1.attention_2.q_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.decoders.8.1.attention_2.k_proj.originLinear.weight torch.Size([640, 768])\n",
      " unet.decoders.8.1.attention_2.k_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.decoders.8.1.attention_2.k_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.decoders.8.1.attention_2.v_proj.originLinear.weight torch.Size([640, 768])\n",
      " unet.decoders.8.1.attention_2.v_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.decoders.8.1.attention_2.v_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.decoders.8.1.attention_2.out_proj.originLinear.weight torch.Size([640, 640])\n",
      " unet.decoders.8.1.attention_2.out_proj.originLinear.bias torch.Size([640])\n",
      " unet.decoders.8.1.attention_2.out_proj.loraA.weight torch.Size([8, 640])\n",
      " unet.decoders.8.1.attention_2.out_proj.loraB.weight torch.Size([640, 8])\n",
      " unet.decoders.8.1.layernorm_3.weight torch.Size([640])\n",
      " unet.decoders.8.1.layernorm_3.bias torch.Size([640])\n",
      " unet.decoders.8.1.linear_geglu_1.weight torch.Size([5120, 640])\n",
      " unet.decoders.8.1.linear_geglu_1.bias torch.Size([5120])\n",
      " unet.decoders.8.1.linear_geglu_2.weight torch.Size([640, 2560])\n",
      " unet.decoders.8.1.linear_geglu_2.bias torch.Size([640])\n",
      " unet.decoders.8.1.conv_output.weight torch.Size([640, 640, 1, 1])\n",
      " unet.decoders.8.1.conv_output.bias torch.Size([640])\n",
      " unet.decoders.8.2.conv.weight torch.Size([640, 640, 3, 3])\n",
      " unet.decoders.8.2.conv.bias torch.Size([640])\n",
      " unet.decoders.9.0.groupnorm_feature.weight torch.Size([960])\n",
      " unet.decoders.9.0.groupnorm_feature.bias torch.Size([960])\n",
      " unet.decoders.9.0.conv_feature.weight torch.Size([320, 960, 3, 3])\n",
      " unet.decoders.9.0.conv_feature.bias torch.Size([320])\n",
      " unet.decoders.9.0.linear_time.weight torch.Size([320, 1280])\n",
      " unet.decoders.9.0.linear_time.bias torch.Size([320])\n",
      " unet.decoders.9.0.groupnorm_merged.weight torch.Size([320])\n",
      " unet.decoders.9.0.groupnorm_merged.bias torch.Size([320])\n",
      " unet.decoders.9.0.conv_merged.weight torch.Size([320, 320, 3, 3])\n",
      " unet.decoders.9.0.conv_merged.bias torch.Size([320])\n",
      " unet.decoders.9.0.residual_layer.weight torch.Size([320, 960, 1, 1])\n",
      " unet.decoders.9.0.residual_layer.bias torch.Size([320])\n",
      " unet.decoders.9.1.groupnorm.weight torch.Size([320])\n",
      " unet.decoders.9.1.groupnorm.bias torch.Size([320])\n",
      " unet.decoders.9.1.conv_input.weight torch.Size([320, 320, 1, 1])\n",
      " unet.decoders.9.1.conv_input.bias torch.Size([320])\n",
      " unet.decoders.9.1.layernorm_1.weight torch.Size([320])\n",
      " unet.decoders.9.1.layernorm_1.bias torch.Size([320])\n",
      " unet.decoders.9.1.attention_1.in_proj.originLinear.weight torch.Size([960, 320])\n",
      " unet.decoders.9.1.attention_1.in_proj.loraA.weight torch.Size([8, 320])\n",
      " unet.decoders.9.1.attention_1.in_proj.loraB.weight torch.Size([960, 8])\n",
      " unet.decoders.9.1.attention_1.out_proj.originLinear.weight torch.Size([320, 320])\n",
      " unet.decoders.9.1.attention_1.out_proj.originLinear.bias torch.Size([320])\n",
      " unet.decoders.9.1.attention_1.out_proj.loraA.weight torch.Size([8, 320])\n",
      " unet.decoders.9.1.attention_1.out_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.decoders.9.1.layernorm_2.weight torch.Size([320])\n",
      " unet.decoders.9.1.layernorm_2.bias torch.Size([320])\n",
      " unet.decoders.9.1.attention_2.q_proj.originLinear.weight torch.Size([320, 320])\n",
      " unet.decoders.9.1.attention_2.q_proj.loraA.weight torch.Size([8, 320])\n",
      " unet.decoders.9.1.attention_2.q_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.decoders.9.1.attention_2.k_proj.originLinear.weight torch.Size([320, 768])\n",
      " unet.decoders.9.1.attention_2.k_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.decoders.9.1.attention_2.k_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.decoders.9.1.attention_2.v_proj.originLinear.weight torch.Size([320, 768])\n",
      " unet.decoders.9.1.attention_2.v_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.decoders.9.1.attention_2.v_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.decoders.9.1.attention_2.out_proj.originLinear.weight torch.Size([320, 320])\n",
      " unet.decoders.9.1.attention_2.out_proj.originLinear.bias torch.Size([320])\n",
      " unet.decoders.9.1.attention_2.out_proj.loraA.weight torch.Size([8, 320])\n",
      " unet.decoders.9.1.attention_2.out_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.decoders.9.1.layernorm_3.weight torch.Size([320])\n",
      " unet.decoders.9.1.layernorm_3.bias torch.Size([320])\n",
      " unet.decoders.9.1.linear_geglu_1.weight torch.Size([2560, 320])\n",
      " unet.decoders.9.1.linear_geglu_1.bias torch.Size([2560])\n",
      " unet.decoders.9.1.linear_geglu_2.weight torch.Size([320, 1280])\n",
      " unet.decoders.9.1.linear_geglu_2.bias torch.Size([320])\n",
      " unet.decoders.9.1.conv_output.weight torch.Size([320, 320, 1, 1])\n",
      " unet.decoders.9.1.conv_output.bias torch.Size([320])\n",
      " unet.decoders.10.0.groupnorm_feature.weight torch.Size([640])\n",
      " unet.decoders.10.0.groupnorm_feature.bias torch.Size([640])\n",
      " unet.decoders.10.0.conv_feature.weight torch.Size([320, 640, 3, 3])\n",
      " unet.decoders.10.0.conv_feature.bias torch.Size([320])\n",
      " unet.decoders.10.0.linear_time.weight torch.Size([320, 1280])\n",
      " unet.decoders.10.0.linear_time.bias torch.Size([320])\n",
      " unet.decoders.10.0.groupnorm_merged.weight torch.Size([320])\n",
      " unet.decoders.10.0.groupnorm_merged.bias torch.Size([320])\n",
      " unet.decoders.10.0.conv_merged.weight torch.Size([320, 320, 3, 3])\n",
      " unet.decoders.10.0.conv_merged.bias torch.Size([320])\n",
      " unet.decoders.10.0.residual_layer.weight torch.Size([320, 640, 1, 1])\n",
      " unet.decoders.10.0.residual_layer.bias torch.Size([320])\n",
      " unet.decoders.10.1.groupnorm.weight torch.Size([320])\n",
      " unet.decoders.10.1.groupnorm.bias torch.Size([320])\n",
      " unet.decoders.10.1.conv_input.weight torch.Size([320, 320, 1, 1])\n",
      " unet.decoders.10.1.conv_input.bias torch.Size([320])\n",
      " unet.decoders.10.1.layernorm_1.weight torch.Size([320])\n",
      " unet.decoders.10.1.layernorm_1.bias torch.Size([320])\n",
      " unet.decoders.10.1.attention_1.in_proj.originLinear.weight torch.Size([960, 320])\n",
      " unet.decoders.10.1.attention_1.in_proj.loraA.weight torch.Size([8, 320])\n",
      " unet.decoders.10.1.attention_1.in_proj.loraB.weight torch.Size([960, 8])\n",
      " unet.decoders.10.1.attention_1.out_proj.originLinear.weight torch.Size([320, 320])\n",
      " unet.decoders.10.1.attention_1.out_proj.originLinear.bias torch.Size([320])\n",
      " unet.decoders.10.1.attention_1.out_proj.loraA.weight torch.Size([8, 320])\n",
      " unet.decoders.10.1.attention_1.out_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.decoders.10.1.layernorm_2.weight torch.Size([320])\n",
      " unet.decoders.10.1.layernorm_2.bias torch.Size([320])\n",
      " unet.decoders.10.1.attention_2.q_proj.originLinear.weight torch.Size([320, 320])\n",
      " unet.decoders.10.1.attention_2.q_proj.loraA.weight torch.Size([8, 320])\n",
      " unet.decoders.10.1.attention_2.q_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.decoders.10.1.attention_2.k_proj.originLinear.weight torch.Size([320, 768])\n",
      " unet.decoders.10.1.attention_2.k_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.decoders.10.1.attention_2.k_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.decoders.10.1.attention_2.v_proj.originLinear.weight torch.Size([320, 768])\n",
      " unet.decoders.10.1.attention_2.v_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.decoders.10.1.attention_2.v_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.decoders.10.1.attention_2.out_proj.originLinear.weight torch.Size([320, 320])\n",
      " unet.decoders.10.1.attention_2.out_proj.originLinear.bias torch.Size([320])\n",
      " unet.decoders.10.1.attention_2.out_proj.loraA.weight torch.Size([8, 320])\n",
      " unet.decoders.10.1.attention_2.out_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.decoders.10.1.layernorm_3.weight torch.Size([320])\n",
      " unet.decoders.10.1.layernorm_3.bias torch.Size([320])\n",
      " unet.decoders.10.1.linear_geglu_1.weight torch.Size([2560, 320])\n",
      " unet.decoders.10.1.linear_geglu_1.bias torch.Size([2560])\n",
      " unet.decoders.10.1.linear_geglu_2.weight torch.Size([320, 1280])\n",
      " unet.decoders.10.1.linear_geglu_2.bias torch.Size([320])\n",
      " unet.decoders.10.1.conv_output.weight torch.Size([320, 320, 1, 1])\n",
      " unet.decoders.10.1.conv_output.bias torch.Size([320])\n",
      " unet.decoders.11.0.groupnorm_feature.weight torch.Size([640])\n",
      " unet.decoders.11.0.groupnorm_feature.bias torch.Size([640])\n",
      " unet.decoders.11.0.conv_feature.weight torch.Size([320, 640, 3, 3])\n",
      " unet.decoders.11.0.conv_feature.bias torch.Size([320])\n",
      " unet.decoders.11.0.linear_time.weight torch.Size([320, 1280])\n",
      " unet.decoders.11.0.linear_time.bias torch.Size([320])\n",
      " unet.decoders.11.0.groupnorm_merged.weight torch.Size([320])\n",
      " unet.decoders.11.0.groupnorm_merged.bias torch.Size([320])\n",
      " unet.decoders.11.0.conv_merged.weight torch.Size([320, 320, 3, 3])\n",
      " unet.decoders.11.0.conv_merged.bias torch.Size([320])\n",
      " unet.decoders.11.0.residual_layer.weight torch.Size([320, 640, 1, 1])\n",
      " unet.decoders.11.0.residual_layer.bias torch.Size([320])\n",
      " unet.decoders.11.1.groupnorm.weight torch.Size([320])\n",
      " unet.decoders.11.1.groupnorm.bias torch.Size([320])\n",
      " unet.decoders.11.1.conv_input.weight torch.Size([320, 320, 1, 1])\n",
      " unet.decoders.11.1.conv_input.bias torch.Size([320])\n",
      " unet.decoders.11.1.layernorm_1.weight torch.Size([320])\n",
      " unet.decoders.11.1.layernorm_1.bias torch.Size([320])\n",
      " unet.decoders.11.1.attention_1.in_proj.originLinear.weight torch.Size([960, 320])\n",
      " unet.decoders.11.1.attention_1.in_proj.loraA.weight torch.Size([8, 320])\n",
      " unet.decoders.11.1.attention_1.in_proj.loraB.weight torch.Size([960, 8])\n",
      " unet.decoders.11.1.attention_1.out_proj.originLinear.weight torch.Size([320, 320])\n",
      " unet.decoders.11.1.attention_1.out_proj.originLinear.bias torch.Size([320])\n",
      " unet.decoders.11.1.attention_1.out_proj.loraA.weight torch.Size([8, 320])\n",
      " unet.decoders.11.1.attention_1.out_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.decoders.11.1.layernorm_2.weight torch.Size([320])\n",
      " unet.decoders.11.1.layernorm_2.bias torch.Size([320])\n",
      " unet.decoders.11.1.attention_2.q_proj.originLinear.weight torch.Size([320, 320])\n",
      " unet.decoders.11.1.attention_2.q_proj.loraA.weight torch.Size([8, 320])\n",
      " unet.decoders.11.1.attention_2.q_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.decoders.11.1.attention_2.k_proj.originLinear.weight torch.Size([320, 768])\n",
      " unet.decoders.11.1.attention_2.k_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.decoders.11.1.attention_2.k_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.decoders.11.1.attention_2.v_proj.originLinear.weight torch.Size([320, 768])\n",
      " unet.decoders.11.1.attention_2.v_proj.loraA.weight torch.Size([8, 768])\n",
      " unet.decoders.11.1.attention_2.v_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.decoders.11.1.attention_2.out_proj.originLinear.weight torch.Size([320, 320])\n",
      " unet.decoders.11.1.attention_2.out_proj.originLinear.bias torch.Size([320])\n",
      " unet.decoders.11.1.attention_2.out_proj.loraA.weight torch.Size([8, 320])\n",
      " unet.decoders.11.1.attention_2.out_proj.loraB.weight torch.Size([320, 8])\n",
      " unet.decoders.11.1.layernorm_3.weight torch.Size([320])\n",
      " unet.decoders.11.1.layernorm_3.bias torch.Size([320])\n",
      " unet.decoders.11.1.linear_geglu_1.weight torch.Size([2560, 320])\n",
      " unet.decoders.11.1.linear_geglu_1.bias torch.Size([2560])\n",
      " unet.decoders.11.1.linear_geglu_2.weight torch.Size([320, 1280])\n",
      " unet.decoders.11.1.linear_geglu_2.bias torch.Size([320])\n",
      " unet.decoders.11.1.conv_output.weight torch.Size([320, 320, 1, 1])\n",
      " unet.decoders.11.1.conv_output.bias torch.Size([320])\n",
      " final.groupnorm.weight torch.Size([320])\n",
      " final.groupnorm.bias torch.Size([320])\n",
      " final.conv.weight torch.Size([4, 320, 3, 3])\n",
      " final.conv.bias torch.Size([4])\n",
      "Parameter Name                                               | Status     | Shape\n",
      "-------------------------------------------------------------------------------------\n",
      "time_embedding.linear_1.weight                               |  FROZEN  | [1280, 320]\n",
      "time_embedding.linear_1.bias                                 |  FROZEN  | [1280]\n",
      "time_embedding.linear_2.weight                               |  FROZEN  | [1280, 1280]\n",
      "time_embedding.linear_2.bias                                 |  FROZEN  | [1280]\n",
      "unet.encoders.0.0.weight                                     |  FROZEN  | [320, 4, 3, 3]\n",
      "unet.encoders.0.0.bias                                       |  FROZEN  | [320]\n",
      "unet.encoders.1.0.groupnorm_feature.weight                   |  FROZEN  | [320]\n",
      "unet.encoders.1.0.groupnorm_feature.bias                     |  FROZEN  | [320]\n",
      "unet.encoders.1.0.conv_feature.weight                        |  FROZEN  | [320, 320, 3, 3]\n",
      "unet.encoders.1.0.conv_feature.bias                          |  FROZEN  | [320]\n",
      "unet.encoders.1.0.linear_time.weight                         |  FROZEN  | [320, 1280]\n",
      "unet.encoders.1.0.linear_time.bias                           |  FROZEN  | [320]\n",
      "unet.encoders.1.0.groupnorm_merged.weight                    |  FROZEN  | [320]\n",
      "unet.encoders.1.0.groupnorm_merged.bias                      |  FROZEN  | [320]\n",
      "unet.encoders.1.0.conv_merged.weight                         |  FROZEN  | [320, 320, 3, 3]\n",
      "unet.encoders.1.0.conv_merged.bias                           |  FROZEN  | [320]\n",
      "unet.encoders.1.1.groupnorm.weight                           |  FROZEN  | [320]\n",
      "unet.encoders.1.1.groupnorm.bias                             |  FROZEN  | [320]\n",
      "unet.encoders.1.1.conv_input.weight                          |  FROZEN  | [320, 320, 1, 1]\n",
      "unet.encoders.1.1.conv_input.bias                            |  FROZEN  | [320]\n",
      "unet.encoders.1.1.layernorm_1.weight                         |  FROZEN  | [320]\n",
      "unet.encoders.1.1.layernorm_1.bias                           |  FROZEN  | [320]\n",
      "unet.encoders.1.1.attention_1.in_proj.originLinear.weight    |  FROZEN  | [960, 320]\n",
      "unet.encoders.1.1.attention_1.in_proj.loraA.weight           |  TRAIN    | [8, 320]\n",
      "unet.encoders.1.1.attention_1.in_proj.loraB.weight           |  TRAIN    | [960, 8]\n",
      "unet.encoders.1.1.attention_1.out_proj.originLinear.weight   |  FROZEN  | [320, 320]\n",
      "unet.encoders.1.1.attention_1.out_proj.originLinear.bias     |  FROZEN  | [320]\n",
      "unet.encoders.1.1.attention_1.out_proj.loraA.weight          |  TRAIN    | [8, 320]\n",
      "unet.encoders.1.1.attention_1.out_proj.loraB.weight          |  TRAIN    | [320, 8]\n",
      "unet.encoders.1.1.layernorm_2.weight                         |  FROZEN  | [320]\n",
      "unet.encoders.1.1.layernorm_2.bias                           |  FROZEN  | [320]\n",
      "unet.encoders.1.1.attention_2.q_proj.originLinear.weight     |  FROZEN  | [320, 320]\n",
      "unet.encoders.1.1.attention_2.q_proj.loraA.weight            |  TRAIN    | [8, 320]\n",
      "unet.encoders.1.1.attention_2.q_proj.loraB.weight            |  TRAIN    | [320, 8]\n",
      "unet.encoders.1.1.attention_2.k_proj.originLinear.weight     |  FROZEN  | [320, 768]\n",
      "unet.encoders.1.1.attention_2.k_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.encoders.1.1.attention_2.k_proj.loraB.weight            |  TRAIN    | [320, 8]\n",
      "unet.encoders.1.1.attention_2.v_proj.originLinear.weight     |  FROZEN  | [320, 768]\n",
      "unet.encoders.1.1.attention_2.v_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.encoders.1.1.attention_2.v_proj.loraB.weight            |  TRAIN    | [320, 8]\n",
      "unet.encoders.1.1.attention_2.out_proj.originLinear.weight   |  FROZEN  | [320, 320]\n",
      "unet.encoders.1.1.attention_2.out_proj.originLinear.bias     |  FROZEN  | [320]\n",
      "unet.encoders.1.1.attention_2.out_proj.loraA.weight          |  TRAIN    | [8, 320]\n",
      "unet.encoders.1.1.attention_2.out_proj.loraB.weight          |  TRAIN    | [320, 8]\n",
      "unet.encoders.1.1.layernorm_3.weight                         |  FROZEN  | [320]\n",
      "unet.encoders.1.1.layernorm_3.bias                           |  FROZEN  | [320]\n",
      "unet.encoders.1.1.linear_geglu_1.weight                      |  FROZEN  | [2560, 320]\n",
      "unet.encoders.1.1.linear_geglu_1.bias                        |  FROZEN  | [2560]\n",
      "unet.encoders.1.1.linear_geglu_2.weight                      |  FROZEN  | [320, 1280]\n",
      "unet.encoders.1.1.linear_geglu_2.bias                        |  FROZEN  | [320]\n",
      "unet.encoders.1.1.conv_output.weight                         |  FROZEN  | [320, 320, 1, 1]\n",
      "unet.encoders.1.1.conv_output.bias                           |  FROZEN  | [320]\n",
      "unet.encoders.2.0.groupnorm_feature.weight                   |  FROZEN  | [320]\n",
      "unet.encoders.2.0.groupnorm_feature.bias                     |  FROZEN  | [320]\n",
      "unet.encoders.2.0.conv_feature.weight                        |  FROZEN  | [320, 320, 3, 3]\n",
      "unet.encoders.2.0.conv_feature.bias                          |  FROZEN  | [320]\n",
      "unet.encoders.2.0.linear_time.weight                         |  FROZEN  | [320, 1280]\n",
      "unet.encoders.2.0.linear_time.bias                           |  FROZEN  | [320]\n",
      "unet.encoders.2.0.groupnorm_merged.weight                    |  FROZEN  | [320]\n",
      "unet.encoders.2.0.groupnorm_merged.bias                      |  FROZEN  | [320]\n",
      "unet.encoders.2.0.conv_merged.weight                         |  FROZEN  | [320, 320, 3, 3]\n",
      "unet.encoders.2.0.conv_merged.bias                           |  FROZEN  | [320]\n",
      "unet.encoders.2.1.groupnorm.weight                           |  FROZEN  | [320]\n",
      "unet.encoders.2.1.groupnorm.bias                             |  FROZEN  | [320]\n",
      "unet.encoders.2.1.conv_input.weight                          |  FROZEN  | [320, 320, 1, 1]\n",
      "unet.encoders.2.1.conv_input.bias                            |  FROZEN  | [320]\n",
      "unet.encoders.2.1.layernorm_1.weight                         |  FROZEN  | [320]\n",
      "unet.encoders.2.1.layernorm_1.bias                           |  FROZEN  | [320]\n",
      "unet.encoders.2.1.attention_1.in_proj.originLinear.weight    |  FROZEN  | [960, 320]\n",
      "unet.encoders.2.1.attention_1.in_proj.loraA.weight           |  TRAIN    | [8, 320]\n",
      "unet.encoders.2.1.attention_1.in_proj.loraB.weight           |  TRAIN    | [960, 8]\n",
      "unet.encoders.2.1.attention_1.out_proj.originLinear.weight   |  FROZEN  | [320, 320]\n",
      "unet.encoders.2.1.attention_1.out_proj.originLinear.bias     |  FROZEN  | [320]\n",
      "unet.encoders.2.1.attention_1.out_proj.loraA.weight          |  TRAIN    | [8, 320]\n",
      "unet.encoders.2.1.attention_1.out_proj.loraB.weight          |  TRAIN    | [320, 8]\n",
      "unet.encoders.2.1.layernorm_2.weight                         |  FROZEN  | [320]\n",
      "unet.encoders.2.1.layernorm_2.bias                           |  FROZEN  | [320]\n",
      "unet.encoders.2.1.attention_2.q_proj.originLinear.weight     |  FROZEN  | [320, 320]\n",
      "unet.encoders.2.1.attention_2.q_proj.loraA.weight            |  TRAIN    | [8, 320]\n",
      "unet.encoders.2.1.attention_2.q_proj.loraB.weight            |  TRAIN    | [320, 8]\n",
      "unet.encoders.2.1.attention_2.k_proj.originLinear.weight     |  FROZEN  | [320, 768]\n",
      "unet.encoders.2.1.attention_2.k_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.encoders.2.1.attention_2.k_proj.loraB.weight            |  TRAIN    | [320, 8]\n",
      "unet.encoders.2.1.attention_2.v_proj.originLinear.weight     |  FROZEN  | [320, 768]\n",
      "unet.encoders.2.1.attention_2.v_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.encoders.2.1.attention_2.v_proj.loraB.weight            |  TRAIN    | [320, 8]\n",
      "unet.encoders.2.1.attention_2.out_proj.originLinear.weight   |  FROZEN  | [320, 320]\n",
      "unet.encoders.2.1.attention_2.out_proj.originLinear.bias     |  FROZEN  | [320]\n",
      "unet.encoders.2.1.attention_2.out_proj.loraA.weight          |  TRAIN    | [8, 320]\n",
      "unet.encoders.2.1.attention_2.out_proj.loraB.weight          |  TRAIN    | [320, 8]\n",
      "unet.encoders.2.1.layernorm_3.weight                         |  FROZEN  | [320]\n",
      "unet.encoders.2.1.layernorm_3.bias                           |  FROZEN  | [320]\n",
      "unet.encoders.2.1.linear_geglu_1.weight                      |  FROZEN  | [2560, 320]\n",
      "unet.encoders.2.1.linear_geglu_1.bias                        |  FROZEN  | [2560]\n",
      "unet.encoders.2.1.linear_geglu_2.weight                      |  FROZEN  | [320, 1280]\n",
      "unet.encoders.2.1.linear_geglu_2.bias                        |  FROZEN  | [320]\n",
      "unet.encoders.2.1.conv_output.weight                         |  FROZEN  | [320, 320, 1, 1]\n",
      "unet.encoders.2.1.conv_output.bias                           |  FROZEN  | [320]\n",
      "unet.encoders.3.0.weight                                     |  FROZEN  | [320, 320, 3, 3]\n",
      "unet.encoders.3.0.bias                                       |  FROZEN  | [320]\n",
      "unet.encoders.4.0.groupnorm_feature.weight                   |  FROZEN  | [320]\n",
      "unet.encoders.4.0.groupnorm_feature.bias                     |  FROZEN  | [320]\n",
      "unet.encoders.4.0.conv_feature.weight                        |  FROZEN  | [640, 320, 3, 3]\n",
      "unet.encoders.4.0.conv_feature.bias                          |  FROZEN  | [640]\n",
      "unet.encoders.4.0.linear_time.weight                         |  FROZEN  | [640, 1280]\n",
      "unet.encoders.4.0.linear_time.bias                           |  FROZEN  | [640]\n",
      "unet.encoders.4.0.groupnorm_merged.weight                    |  FROZEN  | [640]\n",
      "unet.encoders.4.0.groupnorm_merged.bias                      |  FROZEN  | [640]\n",
      "unet.encoders.4.0.conv_merged.weight                         |  FROZEN  | [640, 640, 3, 3]\n",
      "unet.encoders.4.0.conv_merged.bias                           |  FROZEN  | [640]\n",
      "unet.encoders.4.0.residual_layer.weight                      |  FROZEN  | [640, 320, 1, 1]\n",
      "unet.encoders.4.0.residual_layer.bias                        |  FROZEN  | [640]\n",
      "unet.encoders.4.1.groupnorm.weight                           |  FROZEN  | [640]\n",
      "unet.encoders.4.1.groupnorm.bias                             |  FROZEN  | [640]\n",
      "unet.encoders.4.1.conv_input.weight                          |  FROZEN  | [640, 640, 1, 1]\n",
      "unet.encoders.4.1.conv_input.bias                            |  FROZEN  | [640]\n",
      "unet.encoders.4.1.layernorm_1.weight                         |  FROZEN  | [640]\n",
      "unet.encoders.4.1.layernorm_1.bias                           |  FROZEN  | [640]\n",
      "unet.encoders.4.1.attention_1.in_proj.originLinear.weight    |  FROZEN  | [1920, 640]\n",
      "unet.encoders.4.1.attention_1.in_proj.loraA.weight           |  TRAIN    | [8, 640]\n",
      "unet.encoders.4.1.attention_1.in_proj.loraB.weight           |  TRAIN    | [1920, 8]\n",
      "unet.encoders.4.1.attention_1.out_proj.originLinear.weight   |  FROZEN  | [640, 640]\n",
      "unet.encoders.4.1.attention_1.out_proj.originLinear.bias     |  FROZEN  | [640]\n",
      "unet.encoders.4.1.attention_1.out_proj.loraA.weight          |  TRAIN    | [8, 640]\n",
      "unet.encoders.4.1.attention_1.out_proj.loraB.weight          |  TRAIN    | [640, 8]\n",
      "unet.encoders.4.1.layernorm_2.weight                         |  FROZEN  | [640]\n",
      "unet.encoders.4.1.layernorm_2.bias                           |  FROZEN  | [640]\n",
      "unet.encoders.4.1.attention_2.q_proj.originLinear.weight     |  FROZEN  | [640, 640]\n",
      "unet.encoders.4.1.attention_2.q_proj.loraA.weight            |  TRAIN    | [8, 640]\n",
      "unet.encoders.4.1.attention_2.q_proj.loraB.weight            |  TRAIN    | [640, 8]\n",
      "unet.encoders.4.1.attention_2.k_proj.originLinear.weight     |  FROZEN  | [640, 768]\n",
      "unet.encoders.4.1.attention_2.k_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.encoders.4.1.attention_2.k_proj.loraB.weight            |  TRAIN    | [640, 8]\n",
      "unet.encoders.4.1.attention_2.v_proj.originLinear.weight     |  FROZEN  | [640, 768]\n",
      "unet.encoders.4.1.attention_2.v_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.encoders.4.1.attention_2.v_proj.loraB.weight            |  TRAIN    | [640, 8]\n",
      "unet.encoders.4.1.attention_2.out_proj.originLinear.weight   |  FROZEN  | [640, 640]\n",
      "unet.encoders.4.1.attention_2.out_proj.originLinear.bias     |  FROZEN  | [640]\n",
      "unet.encoders.4.1.attention_2.out_proj.loraA.weight          |  TRAIN    | [8, 640]\n",
      "unet.encoders.4.1.attention_2.out_proj.loraB.weight          |  TRAIN    | [640, 8]\n",
      "unet.encoders.4.1.layernorm_3.weight                         |  FROZEN  | [640]\n",
      "unet.encoders.4.1.layernorm_3.bias                           |  FROZEN  | [640]\n",
      "unet.encoders.4.1.linear_geglu_1.weight                      |  FROZEN  | [5120, 640]\n",
      "unet.encoders.4.1.linear_geglu_1.bias                        |  FROZEN  | [5120]\n",
      "unet.encoders.4.1.linear_geglu_2.weight                      |  FROZEN  | [640, 2560]\n",
      "unet.encoders.4.1.linear_geglu_2.bias                        |  FROZEN  | [640]\n",
      "unet.encoders.4.1.conv_output.weight                         |  FROZEN  | [640, 640, 1, 1]\n",
      "unet.encoders.4.1.conv_output.bias                           |  FROZEN  | [640]\n",
      "unet.encoders.5.0.groupnorm_feature.weight                   |  FROZEN  | [640]\n",
      "unet.encoders.5.0.groupnorm_feature.bias                     |  FROZEN  | [640]\n",
      "unet.encoders.5.0.conv_feature.weight                        |  FROZEN  | [640, 640, 3, 3]\n",
      "unet.encoders.5.0.conv_feature.bias                          |  FROZEN  | [640]\n",
      "unet.encoders.5.0.linear_time.weight                         |  FROZEN  | [640, 1280]\n",
      "unet.encoders.5.0.linear_time.bias                           |  FROZEN  | [640]\n",
      "unet.encoders.5.0.groupnorm_merged.weight                    |  FROZEN  | [640]\n",
      "unet.encoders.5.0.groupnorm_merged.bias                      |  FROZEN  | [640]\n",
      "unet.encoders.5.0.conv_merged.weight                         |  FROZEN  | [640, 640, 3, 3]\n",
      "unet.encoders.5.0.conv_merged.bias                           |  FROZEN  | [640]\n",
      "unet.encoders.5.1.groupnorm.weight                           |  FROZEN  | [640]\n",
      "unet.encoders.5.1.groupnorm.bias                             |  FROZEN  | [640]\n",
      "unet.encoders.5.1.conv_input.weight                          |  FROZEN  | [640, 640, 1, 1]\n",
      "unet.encoders.5.1.conv_input.bias                            |  FROZEN  | [640]\n",
      "unet.encoders.5.1.layernorm_1.weight                         |  FROZEN  | [640]\n",
      "unet.encoders.5.1.layernorm_1.bias                           |  FROZEN  | [640]\n",
      "unet.encoders.5.1.attention_1.in_proj.originLinear.weight    |  FROZEN  | [1920, 640]\n",
      "unet.encoders.5.1.attention_1.in_proj.loraA.weight           |  TRAIN    | [8, 640]\n",
      "unet.encoders.5.1.attention_1.in_proj.loraB.weight           |  TRAIN    | [1920, 8]\n",
      "unet.encoders.5.1.attention_1.out_proj.originLinear.weight   |  FROZEN  | [640, 640]\n",
      "unet.encoders.5.1.attention_1.out_proj.originLinear.bias     |  FROZEN  | [640]\n",
      "unet.encoders.5.1.attention_1.out_proj.loraA.weight          |  TRAIN    | [8, 640]\n",
      "unet.encoders.5.1.attention_1.out_proj.loraB.weight          |  TRAIN    | [640, 8]\n",
      "unet.encoders.5.1.layernorm_2.weight                         |  FROZEN  | [640]\n",
      "unet.encoders.5.1.layernorm_2.bias                           |  FROZEN  | [640]\n",
      "unet.encoders.5.1.attention_2.q_proj.originLinear.weight     |  FROZEN  | [640, 640]\n",
      "unet.encoders.5.1.attention_2.q_proj.loraA.weight            |  TRAIN    | [8, 640]\n",
      "unet.encoders.5.1.attention_2.q_proj.loraB.weight            |  TRAIN    | [640, 8]\n",
      "unet.encoders.5.1.attention_2.k_proj.originLinear.weight     |  FROZEN  | [640, 768]\n",
      "unet.encoders.5.1.attention_2.k_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.encoders.5.1.attention_2.k_proj.loraB.weight            |  TRAIN    | [640, 8]\n",
      "unet.encoders.5.1.attention_2.v_proj.originLinear.weight     |  FROZEN  | [640, 768]\n",
      "unet.encoders.5.1.attention_2.v_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.encoders.5.1.attention_2.v_proj.loraB.weight            |  TRAIN    | [640, 8]\n",
      "unet.encoders.5.1.attention_2.out_proj.originLinear.weight   |  FROZEN  | [640, 640]\n",
      "unet.encoders.5.1.attention_2.out_proj.originLinear.bias     |  FROZEN  | [640]\n",
      "unet.encoders.5.1.attention_2.out_proj.loraA.weight          |  TRAIN    | [8, 640]\n",
      "unet.encoders.5.1.attention_2.out_proj.loraB.weight          |  TRAIN    | [640, 8]\n",
      "unet.encoders.5.1.layernorm_3.weight                         |  FROZEN  | [640]\n",
      "unet.encoders.5.1.layernorm_3.bias                           |  FROZEN  | [640]\n",
      "unet.encoders.5.1.linear_geglu_1.weight                      |  FROZEN  | [5120, 640]\n",
      "unet.encoders.5.1.linear_geglu_1.bias                        |  FROZEN  | [5120]\n",
      "unet.encoders.5.1.linear_geglu_2.weight                      |  FROZEN  | [640, 2560]\n",
      "unet.encoders.5.1.linear_geglu_2.bias                        |  FROZEN  | [640]\n",
      "unet.encoders.5.1.conv_output.weight                         |  FROZEN  | [640, 640, 1, 1]\n",
      "unet.encoders.5.1.conv_output.bias                           |  FROZEN  | [640]\n",
      "unet.encoders.6.0.weight                                     |  FROZEN  | [640, 640, 3, 3]\n",
      "unet.encoders.6.0.bias                                       |  FROZEN  | [640]\n",
      "unet.encoders.7.0.groupnorm_feature.weight                   |  FROZEN  | [640]\n",
      "unet.encoders.7.0.groupnorm_feature.bias                     |  FROZEN  | [640]\n",
      "unet.encoders.7.0.conv_feature.weight                        |  FROZEN  | [1280, 640, 3, 3]\n",
      "unet.encoders.7.0.conv_feature.bias                          |  FROZEN  | [1280]\n",
      "unet.encoders.7.0.linear_time.weight                         |  FROZEN  | [1280, 1280]\n",
      "unet.encoders.7.0.linear_time.bias                           |  FROZEN  | [1280]\n",
      "unet.encoders.7.0.groupnorm_merged.weight                    |  FROZEN  | [1280]\n",
      "unet.encoders.7.0.groupnorm_merged.bias                      |  FROZEN  | [1280]\n",
      "unet.encoders.7.0.conv_merged.weight                         |  FROZEN  | [1280, 1280, 3, 3]\n",
      "unet.encoders.7.0.conv_merged.bias                           |  FROZEN  | [1280]\n",
      "unet.encoders.7.0.residual_layer.weight                      |  FROZEN  | [1280, 640, 1, 1]\n",
      "unet.encoders.7.0.residual_layer.bias                        |  FROZEN  | [1280]\n",
      "unet.encoders.7.1.groupnorm.weight                           |  FROZEN  | [1280]\n",
      "unet.encoders.7.1.groupnorm.bias                             |  FROZEN  | [1280]\n",
      "unet.encoders.7.1.conv_input.weight                          |  FROZEN  | [1280, 1280, 1, 1]\n",
      "unet.encoders.7.1.conv_input.bias                            |  FROZEN  | [1280]\n",
      "unet.encoders.7.1.layernorm_1.weight                         |  FROZEN  | [1280]\n",
      "unet.encoders.7.1.layernorm_1.bias                           |  FROZEN  | [1280]\n",
      "unet.encoders.7.1.attention_1.in_proj.originLinear.weight    |  FROZEN  | [3840, 1280]\n",
      "unet.encoders.7.1.attention_1.in_proj.loraA.weight           |  TRAIN    | [8, 1280]\n",
      "unet.encoders.7.1.attention_1.in_proj.loraB.weight           |  TRAIN    | [3840, 8]\n",
      "unet.encoders.7.1.attention_1.out_proj.originLinear.weight   |  FROZEN  | [1280, 1280]\n",
      "unet.encoders.7.1.attention_1.out_proj.originLinear.bias     |  FROZEN  | [1280]\n",
      "unet.encoders.7.1.attention_1.out_proj.loraA.weight          |  TRAIN    | [8, 1280]\n",
      "unet.encoders.7.1.attention_1.out_proj.loraB.weight          |  TRAIN    | [1280, 8]\n",
      "unet.encoders.7.1.layernorm_2.weight                         |  FROZEN  | [1280]\n",
      "unet.encoders.7.1.layernorm_2.bias                           |  FROZEN  | [1280]\n",
      "unet.encoders.7.1.attention_2.q_proj.originLinear.weight     |  FROZEN  | [1280, 1280]\n",
      "unet.encoders.7.1.attention_2.q_proj.loraA.weight            |  TRAIN    | [8, 1280]\n",
      "unet.encoders.7.1.attention_2.q_proj.loraB.weight            |  TRAIN    | [1280, 8]\n",
      "unet.encoders.7.1.attention_2.k_proj.originLinear.weight     |  FROZEN  | [1280, 768]\n",
      "unet.encoders.7.1.attention_2.k_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.encoders.7.1.attention_2.k_proj.loraB.weight            |  TRAIN    | [1280, 8]\n",
      "unet.encoders.7.1.attention_2.v_proj.originLinear.weight     |  FROZEN  | [1280, 768]\n",
      "unet.encoders.7.1.attention_2.v_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.encoders.7.1.attention_2.v_proj.loraB.weight            |  TRAIN    | [1280, 8]\n",
      "unet.encoders.7.1.attention_2.out_proj.originLinear.weight   |  FROZEN  | [1280, 1280]\n",
      "unet.encoders.7.1.attention_2.out_proj.originLinear.bias     |  FROZEN  | [1280]\n",
      "unet.encoders.7.1.attention_2.out_proj.loraA.weight          |  TRAIN    | [8, 1280]\n",
      "unet.encoders.7.1.attention_2.out_proj.loraB.weight          |  TRAIN    | [1280, 8]\n",
      "unet.encoders.7.1.layernorm_3.weight                         |  FROZEN  | [1280]\n",
      "unet.encoders.7.1.layernorm_3.bias                           |  FROZEN  | [1280]\n",
      "unet.encoders.7.1.linear_geglu_1.weight                      |  FROZEN  | [10240, 1280]\n",
      "unet.encoders.7.1.linear_geglu_1.bias                        |  FROZEN  | [10240]\n",
      "unet.encoders.7.1.linear_geglu_2.weight                      |  FROZEN  | [1280, 5120]\n",
      "unet.encoders.7.1.linear_geglu_2.bias                        |  FROZEN  | [1280]\n",
      "unet.encoders.7.1.conv_output.weight                         |  FROZEN  | [1280, 1280, 1, 1]\n",
      "unet.encoders.7.1.conv_output.bias                           |  FROZEN  | [1280]\n",
      "unet.encoders.8.0.groupnorm_feature.weight                   |  FROZEN  | [1280]\n",
      "unet.encoders.8.0.groupnorm_feature.bias                     |  FROZEN  | [1280]\n",
      "unet.encoders.8.0.conv_feature.weight                        |  FROZEN  | [1280, 1280, 3, 3]\n",
      "unet.encoders.8.0.conv_feature.bias                          |  FROZEN  | [1280]\n",
      "unet.encoders.8.0.linear_time.weight                         |  FROZEN  | [1280, 1280]\n",
      "unet.encoders.8.0.linear_time.bias                           |  FROZEN  | [1280]\n",
      "unet.encoders.8.0.groupnorm_merged.weight                    |  FROZEN  | [1280]\n",
      "unet.encoders.8.0.groupnorm_merged.bias                      |  FROZEN  | [1280]\n",
      "unet.encoders.8.0.conv_merged.weight                         |  FROZEN  | [1280, 1280, 3, 3]\n",
      "unet.encoders.8.0.conv_merged.bias                           |  FROZEN  | [1280]\n",
      "unet.encoders.8.1.groupnorm.weight                           |  FROZEN  | [1280]\n",
      "unet.encoders.8.1.groupnorm.bias                             |  FROZEN  | [1280]\n",
      "unet.encoders.8.1.conv_input.weight                          |  FROZEN  | [1280, 1280, 1, 1]\n",
      "unet.encoders.8.1.conv_input.bias                            |  FROZEN  | [1280]\n",
      "unet.encoders.8.1.layernorm_1.weight                         |  FROZEN  | [1280]\n",
      "unet.encoders.8.1.layernorm_1.bias                           |  FROZEN  | [1280]\n",
      "unet.encoders.8.1.attention_1.in_proj.originLinear.weight    |  FROZEN  | [3840, 1280]\n",
      "unet.encoders.8.1.attention_1.in_proj.loraA.weight           |  TRAIN    | [8, 1280]\n",
      "unet.encoders.8.1.attention_1.in_proj.loraB.weight           |  TRAIN    | [3840, 8]\n",
      "unet.encoders.8.1.attention_1.out_proj.originLinear.weight   |  FROZEN  | [1280, 1280]\n",
      "unet.encoders.8.1.attention_1.out_proj.originLinear.bias     |  FROZEN  | [1280]\n",
      "unet.encoders.8.1.attention_1.out_proj.loraA.weight          |  TRAIN    | [8, 1280]\n",
      "unet.encoders.8.1.attention_1.out_proj.loraB.weight          |  TRAIN    | [1280, 8]\n",
      "unet.encoders.8.1.layernorm_2.weight                         |  FROZEN  | [1280]\n",
      "unet.encoders.8.1.layernorm_2.bias                           |  FROZEN  | [1280]\n",
      "unet.encoders.8.1.attention_2.q_proj.originLinear.weight     |  FROZEN  | [1280, 1280]\n",
      "unet.encoders.8.1.attention_2.q_proj.loraA.weight            |  TRAIN    | [8, 1280]\n",
      "unet.encoders.8.1.attention_2.q_proj.loraB.weight            |  TRAIN    | [1280, 8]\n",
      "unet.encoders.8.1.attention_2.k_proj.originLinear.weight     |  FROZEN  | [1280, 768]\n",
      "unet.encoders.8.1.attention_2.k_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.encoders.8.1.attention_2.k_proj.loraB.weight            |  TRAIN    | [1280, 8]\n",
      "unet.encoders.8.1.attention_2.v_proj.originLinear.weight     |  FROZEN  | [1280, 768]\n",
      "unet.encoders.8.1.attention_2.v_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.encoders.8.1.attention_2.v_proj.loraB.weight            |  TRAIN    | [1280, 8]\n",
      "unet.encoders.8.1.attention_2.out_proj.originLinear.weight   |  FROZEN  | [1280, 1280]\n",
      "unet.encoders.8.1.attention_2.out_proj.originLinear.bias     |  FROZEN  | [1280]\n",
      "unet.encoders.8.1.attention_2.out_proj.loraA.weight          |  TRAIN    | [8, 1280]\n",
      "unet.encoders.8.1.attention_2.out_proj.loraB.weight          |  TRAIN    | [1280, 8]\n",
      "unet.encoders.8.1.layernorm_3.weight                         |  FROZEN  | [1280]\n",
      "unet.encoders.8.1.layernorm_3.bias                           |  FROZEN  | [1280]\n",
      "unet.encoders.8.1.linear_geglu_1.weight                      |  FROZEN  | [10240, 1280]\n",
      "unet.encoders.8.1.linear_geglu_1.bias                        |  FROZEN  | [10240]\n",
      "unet.encoders.8.1.linear_geglu_2.weight                      |  FROZEN  | [1280, 5120]\n",
      "unet.encoders.8.1.linear_geglu_2.bias                        |  FROZEN  | [1280]\n",
      "unet.encoders.8.1.conv_output.weight                         |  FROZEN  | [1280, 1280, 1, 1]\n",
      "unet.encoders.8.1.conv_output.bias                           |  FROZEN  | [1280]\n",
      "unet.encoders.9.0.weight                                     |  FROZEN  | [1280, 1280, 3, 3]\n",
      "unet.encoders.9.0.bias                                       |  FROZEN  | [1280]\n",
      "unet.encoders.10.0.groupnorm_feature.weight                  |  FROZEN  | [1280]\n",
      "unet.encoders.10.0.groupnorm_feature.bias                    |  FROZEN  | [1280]\n",
      "unet.encoders.10.0.conv_feature.weight                       |  FROZEN  | [1280, 1280, 3, 3]\n",
      "unet.encoders.10.0.conv_feature.bias                         |  FROZEN  | [1280]\n",
      "unet.encoders.10.0.linear_time.weight                        |  FROZEN  | [1280, 1280]\n",
      "unet.encoders.10.0.linear_time.bias                          |  FROZEN  | [1280]\n",
      "unet.encoders.10.0.groupnorm_merged.weight                   |  FROZEN  | [1280]\n",
      "unet.encoders.10.0.groupnorm_merged.bias                     |  FROZEN  | [1280]\n",
      "unet.encoders.10.0.conv_merged.weight                        |  FROZEN  | [1280, 1280, 3, 3]\n",
      "unet.encoders.10.0.conv_merged.bias                          |  FROZEN  | [1280]\n",
      "unet.encoders.11.0.groupnorm_feature.weight                  |  FROZEN  | [1280]\n",
      "unet.encoders.11.0.groupnorm_feature.bias                    |  FROZEN  | [1280]\n",
      "unet.encoders.11.0.conv_feature.weight                       |  FROZEN  | [1280, 1280, 3, 3]\n",
      "unet.encoders.11.0.conv_feature.bias                         |  FROZEN  | [1280]\n",
      "unet.encoders.11.0.linear_time.weight                        |  FROZEN  | [1280, 1280]\n",
      "unet.encoders.11.0.linear_time.bias                          |  FROZEN  | [1280]\n",
      "unet.encoders.11.0.groupnorm_merged.weight                   |  FROZEN  | [1280]\n",
      "unet.encoders.11.0.groupnorm_merged.bias                     |  FROZEN  | [1280]\n",
      "unet.encoders.11.0.conv_merged.weight                        |  FROZEN  | [1280, 1280, 3, 3]\n",
      "unet.encoders.11.0.conv_merged.bias                          |  FROZEN  | [1280]\n",
      "unet.bottleneck.0.groupnorm_feature.weight                   |  FROZEN  | [1280]\n",
      "unet.bottleneck.0.groupnorm_feature.bias                     |  FROZEN  | [1280]\n",
      "unet.bottleneck.0.conv_feature.weight                        |  FROZEN  | [1280, 1280, 3, 3]\n",
      "unet.bottleneck.0.conv_feature.bias                          |  FROZEN  | [1280]\n",
      "unet.bottleneck.0.linear_time.weight                         |  FROZEN  | [1280, 1280]\n",
      "unet.bottleneck.0.linear_time.bias                           |  FROZEN  | [1280]\n",
      "unet.bottleneck.0.groupnorm_merged.weight                    |  FROZEN  | [1280]\n",
      "unet.bottleneck.0.groupnorm_merged.bias                      |  FROZEN  | [1280]\n",
      "unet.bottleneck.0.conv_merged.weight                         |  FROZEN  | [1280, 1280, 3, 3]\n",
      "unet.bottleneck.0.conv_merged.bias                           |  FROZEN  | [1280]\n",
      "unet.bottleneck.1.groupnorm.weight                           |  FROZEN  | [1280]\n",
      "unet.bottleneck.1.groupnorm.bias                             |  FROZEN  | [1280]\n",
      "unet.bottleneck.1.conv_input.weight                          |  FROZEN  | [1280, 1280, 1, 1]\n",
      "unet.bottleneck.1.conv_input.bias                            |  FROZEN  | [1280]\n",
      "unet.bottleneck.1.layernorm_1.weight                         |  FROZEN  | [1280]\n",
      "unet.bottleneck.1.layernorm_1.bias                           |  FROZEN  | [1280]\n",
      "unet.bottleneck.1.attention_1.in_proj.originLinear.weight    |  FROZEN  | [3840, 1280]\n",
      "unet.bottleneck.1.attention_1.in_proj.loraA.weight           |  TRAIN    | [8, 1280]\n",
      "unet.bottleneck.1.attention_1.in_proj.loraB.weight           |  TRAIN    | [3840, 8]\n",
      "unet.bottleneck.1.attention_1.out_proj.originLinear.weight   |  FROZEN  | [1280, 1280]\n",
      "unet.bottleneck.1.attention_1.out_proj.originLinear.bias     |  FROZEN  | [1280]\n",
      "unet.bottleneck.1.attention_1.out_proj.loraA.weight          |  TRAIN    | [8, 1280]\n",
      "unet.bottleneck.1.attention_1.out_proj.loraB.weight          |  TRAIN    | [1280, 8]\n",
      "unet.bottleneck.1.layernorm_2.weight                         |  FROZEN  | [1280]\n",
      "unet.bottleneck.1.layernorm_2.bias                           |  FROZEN  | [1280]\n",
      "unet.bottleneck.1.attention_2.q_proj.originLinear.weight     |  FROZEN  | [1280, 1280]\n",
      "unet.bottleneck.1.attention_2.q_proj.loraA.weight            |  TRAIN    | [8, 1280]\n",
      "unet.bottleneck.1.attention_2.q_proj.loraB.weight            |  TRAIN    | [1280, 8]\n",
      "unet.bottleneck.1.attention_2.k_proj.originLinear.weight     |  FROZEN  | [1280, 768]\n",
      "unet.bottleneck.1.attention_2.k_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.bottleneck.1.attention_2.k_proj.loraB.weight            |  TRAIN    | [1280, 8]\n",
      "unet.bottleneck.1.attention_2.v_proj.originLinear.weight     |  FROZEN  | [1280, 768]\n",
      "unet.bottleneck.1.attention_2.v_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.bottleneck.1.attention_2.v_proj.loraB.weight            |  TRAIN    | [1280, 8]\n",
      "unet.bottleneck.1.attention_2.out_proj.originLinear.weight   |  FROZEN  | [1280, 1280]\n",
      "unet.bottleneck.1.attention_2.out_proj.originLinear.bias     |  FROZEN  | [1280]\n",
      "unet.bottleneck.1.attention_2.out_proj.loraA.weight          |  TRAIN    | [8, 1280]\n",
      "unet.bottleneck.1.attention_2.out_proj.loraB.weight          |  TRAIN    | [1280, 8]\n",
      "unet.bottleneck.1.layernorm_3.weight                         |  FROZEN  | [1280]\n",
      "unet.bottleneck.1.layernorm_3.bias                           |  FROZEN  | [1280]\n",
      "unet.bottleneck.1.linear_geglu_1.weight                      |  FROZEN  | [10240, 1280]\n",
      "unet.bottleneck.1.linear_geglu_1.bias                        |  FROZEN  | [10240]\n",
      "unet.bottleneck.1.linear_geglu_2.weight                      |  FROZEN  | [1280, 5120]\n",
      "unet.bottleneck.1.linear_geglu_2.bias                        |  FROZEN  | [1280]\n",
      "unet.bottleneck.1.conv_output.weight                         |  FROZEN  | [1280, 1280, 1, 1]\n",
      "unet.bottleneck.1.conv_output.bias                           |  FROZEN  | [1280]\n",
      "unet.bottleneck.2.groupnorm_feature.weight                   |  FROZEN  | [1280]\n",
      "unet.bottleneck.2.groupnorm_feature.bias                     |  FROZEN  | [1280]\n",
      "unet.bottleneck.2.conv_feature.weight                        |  FROZEN  | [1280, 1280, 3, 3]\n",
      "unet.bottleneck.2.conv_feature.bias                          |  FROZEN  | [1280]\n",
      "unet.bottleneck.2.linear_time.weight                         |  FROZEN  | [1280, 1280]\n",
      "unet.bottleneck.2.linear_time.bias                           |  FROZEN  | [1280]\n",
      "unet.bottleneck.2.groupnorm_merged.weight                    |  FROZEN  | [1280]\n",
      "unet.bottleneck.2.groupnorm_merged.bias                      |  FROZEN  | [1280]\n",
      "unet.bottleneck.2.conv_merged.weight                         |  FROZEN  | [1280, 1280, 3, 3]\n",
      "unet.bottleneck.2.conv_merged.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.0.0.groupnorm_feature.weight                   |  FROZEN  | [2560]\n",
      "unet.decoders.0.0.groupnorm_feature.bias                     |  FROZEN  | [2560]\n",
      "unet.decoders.0.0.conv_feature.weight                        |  FROZEN  | [1280, 2560, 3, 3]\n",
      "unet.decoders.0.0.conv_feature.bias                          |  FROZEN  | [1280]\n",
      "unet.decoders.0.0.linear_time.weight                         |  FROZEN  | [1280, 1280]\n",
      "unet.decoders.0.0.linear_time.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.0.0.groupnorm_merged.weight                    |  FROZEN  | [1280]\n",
      "unet.decoders.0.0.groupnorm_merged.bias                      |  FROZEN  | [1280]\n",
      "unet.decoders.0.0.conv_merged.weight                         |  FROZEN  | [1280, 1280, 3, 3]\n",
      "unet.decoders.0.0.conv_merged.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.0.0.residual_layer.weight                      |  FROZEN  | [1280, 2560, 1, 1]\n",
      "unet.decoders.0.0.residual_layer.bias                        |  FROZEN  | [1280]\n",
      "unet.decoders.1.0.groupnorm_feature.weight                   |  FROZEN  | [2560]\n",
      "unet.decoders.1.0.groupnorm_feature.bias                     |  FROZEN  | [2560]\n",
      "unet.decoders.1.0.conv_feature.weight                        |  FROZEN  | [1280, 2560, 3, 3]\n",
      "unet.decoders.1.0.conv_feature.bias                          |  FROZEN  | [1280]\n",
      "unet.decoders.1.0.linear_time.weight                         |  FROZEN  | [1280, 1280]\n",
      "unet.decoders.1.0.linear_time.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.1.0.groupnorm_merged.weight                    |  FROZEN  | [1280]\n",
      "unet.decoders.1.0.groupnorm_merged.bias                      |  FROZEN  | [1280]\n",
      "unet.decoders.1.0.conv_merged.weight                         |  FROZEN  | [1280, 1280, 3, 3]\n",
      "unet.decoders.1.0.conv_merged.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.1.0.residual_layer.weight                      |  FROZEN  | [1280, 2560, 1, 1]\n",
      "unet.decoders.1.0.residual_layer.bias                        |  FROZEN  | [1280]\n",
      "unet.decoders.2.0.groupnorm_feature.weight                   |  FROZEN  | [2560]\n",
      "unet.decoders.2.0.groupnorm_feature.bias                     |  FROZEN  | [2560]\n",
      "unet.decoders.2.0.conv_feature.weight                        |  FROZEN  | [1280, 2560, 3, 3]\n",
      "unet.decoders.2.0.conv_feature.bias                          |  FROZEN  | [1280]\n",
      "unet.decoders.2.0.linear_time.weight                         |  FROZEN  | [1280, 1280]\n",
      "unet.decoders.2.0.linear_time.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.2.0.groupnorm_merged.weight                    |  FROZEN  | [1280]\n",
      "unet.decoders.2.0.groupnorm_merged.bias                      |  FROZEN  | [1280]\n",
      "unet.decoders.2.0.conv_merged.weight                         |  FROZEN  | [1280, 1280, 3, 3]\n",
      "unet.decoders.2.0.conv_merged.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.2.0.residual_layer.weight                      |  FROZEN  | [1280, 2560, 1, 1]\n",
      "unet.decoders.2.0.residual_layer.bias                        |  FROZEN  | [1280]\n",
      "unet.decoders.2.1.conv.weight                                |  FROZEN  | [1280, 1280, 3, 3]\n",
      "unet.decoders.2.1.conv.bias                                  |  FROZEN  | [1280]\n",
      "unet.decoders.3.0.groupnorm_feature.weight                   |  FROZEN  | [2560]\n",
      "unet.decoders.3.0.groupnorm_feature.bias                     |  FROZEN  | [2560]\n",
      "unet.decoders.3.0.conv_feature.weight                        |  FROZEN  | [1280, 2560, 3, 3]\n",
      "unet.decoders.3.0.conv_feature.bias                          |  FROZEN  | [1280]\n",
      "unet.decoders.3.0.linear_time.weight                         |  FROZEN  | [1280, 1280]\n",
      "unet.decoders.3.0.linear_time.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.3.0.groupnorm_merged.weight                    |  FROZEN  | [1280]\n",
      "unet.decoders.3.0.groupnorm_merged.bias                      |  FROZEN  | [1280]\n",
      "unet.decoders.3.0.conv_merged.weight                         |  FROZEN  | [1280, 1280, 3, 3]\n",
      "unet.decoders.3.0.conv_merged.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.3.0.residual_layer.weight                      |  FROZEN  | [1280, 2560, 1, 1]\n",
      "unet.decoders.3.0.residual_layer.bias                        |  FROZEN  | [1280]\n",
      "unet.decoders.3.1.groupnorm.weight                           |  FROZEN  | [1280]\n",
      "unet.decoders.3.1.groupnorm.bias                             |  FROZEN  | [1280]\n",
      "unet.decoders.3.1.conv_input.weight                          |  FROZEN  | [1280, 1280, 1, 1]\n",
      "unet.decoders.3.1.conv_input.bias                            |  FROZEN  | [1280]\n",
      "unet.decoders.3.1.layernorm_1.weight                         |  FROZEN  | [1280]\n",
      "unet.decoders.3.1.layernorm_1.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.3.1.attention_1.in_proj.originLinear.weight    |  FROZEN  | [3840, 1280]\n",
      "unet.decoders.3.1.attention_1.in_proj.loraA.weight           |  TRAIN    | [8, 1280]\n",
      "unet.decoders.3.1.attention_1.in_proj.loraB.weight           |  TRAIN    | [3840, 8]\n",
      "unet.decoders.3.1.attention_1.out_proj.originLinear.weight   |  FROZEN  | [1280, 1280]\n",
      "unet.decoders.3.1.attention_1.out_proj.originLinear.bias     |  FROZEN  | [1280]\n",
      "unet.decoders.3.1.attention_1.out_proj.loraA.weight          |  TRAIN    | [8, 1280]\n",
      "unet.decoders.3.1.attention_1.out_proj.loraB.weight          |  TRAIN    | [1280, 8]\n",
      "unet.decoders.3.1.layernorm_2.weight                         |  FROZEN  | [1280]\n",
      "unet.decoders.3.1.layernorm_2.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.3.1.attention_2.q_proj.originLinear.weight     |  FROZEN  | [1280, 1280]\n",
      "unet.decoders.3.1.attention_2.q_proj.loraA.weight            |  TRAIN    | [8, 1280]\n",
      "unet.decoders.3.1.attention_2.q_proj.loraB.weight            |  TRAIN    | [1280, 8]\n",
      "unet.decoders.3.1.attention_2.k_proj.originLinear.weight     |  FROZEN  | [1280, 768]\n",
      "unet.decoders.3.1.attention_2.k_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.decoders.3.1.attention_2.k_proj.loraB.weight            |  TRAIN    | [1280, 8]\n",
      "unet.decoders.3.1.attention_2.v_proj.originLinear.weight     |  FROZEN  | [1280, 768]\n",
      "unet.decoders.3.1.attention_2.v_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.decoders.3.1.attention_2.v_proj.loraB.weight            |  TRAIN    | [1280, 8]\n",
      "unet.decoders.3.1.attention_2.out_proj.originLinear.weight   |  FROZEN  | [1280, 1280]\n",
      "unet.decoders.3.1.attention_2.out_proj.originLinear.bias     |  FROZEN  | [1280]\n",
      "unet.decoders.3.1.attention_2.out_proj.loraA.weight          |  TRAIN    | [8, 1280]\n",
      "unet.decoders.3.1.attention_2.out_proj.loraB.weight          |  TRAIN    | [1280, 8]\n",
      "unet.decoders.3.1.layernorm_3.weight                         |  FROZEN  | [1280]\n",
      "unet.decoders.3.1.layernorm_3.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.3.1.linear_geglu_1.weight                      |  FROZEN  | [10240, 1280]\n",
      "unet.decoders.3.1.linear_geglu_1.bias                        |  FROZEN  | [10240]\n",
      "unet.decoders.3.1.linear_geglu_2.weight                      |  FROZEN  | [1280, 5120]\n",
      "unet.decoders.3.1.linear_geglu_2.bias                        |  FROZEN  | [1280]\n",
      "unet.decoders.3.1.conv_output.weight                         |  FROZEN  | [1280, 1280, 1, 1]\n",
      "unet.decoders.3.1.conv_output.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.4.0.groupnorm_feature.weight                   |  FROZEN  | [2560]\n",
      "unet.decoders.4.0.groupnorm_feature.bias                     |  FROZEN  | [2560]\n",
      "unet.decoders.4.0.conv_feature.weight                        |  FROZEN  | [1280, 2560, 3, 3]\n",
      "unet.decoders.4.0.conv_feature.bias                          |  FROZEN  | [1280]\n",
      "unet.decoders.4.0.linear_time.weight                         |  FROZEN  | [1280, 1280]\n",
      "unet.decoders.4.0.linear_time.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.4.0.groupnorm_merged.weight                    |  FROZEN  | [1280]\n",
      "unet.decoders.4.0.groupnorm_merged.bias                      |  FROZEN  | [1280]\n",
      "unet.decoders.4.0.conv_merged.weight                         |  FROZEN  | [1280, 1280, 3, 3]\n",
      "unet.decoders.4.0.conv_merged.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.4.0.residual_layer.weight                      |  FROZEN  | [1280, 2560, 1, 1]\n",
      "unet.decoders.4.0.residual_layer.bias                        |  FROZEN  | [1280]\n",
      "unet.decoders.4.1.groupnorm.weight                           |  FROZEN  | [1280]\n",
      "unet.decoders.4.1.groupnorm.bias                             |  FROZEN  | [1280]\n",
      "unet.decoders.4.1.conv_input.weight                          |  FROZEN  | [1280, 1280, 1, 1]\n",
      "unet.decoders.4.1.conv_input.bias                            |  FROZEN  | [1280]\n",
      "unet.decoders.4.1.layernorm_1.weight                         |  FROZEN  | [1280]\n",
      "unet.decoders.4.1.layernorm_1.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.4.1.attention_1.in_proj.originLinear.weight    |  FROZEN  | [3840, 1280]\n",
      "unet.decoders.4.1.attention_1.in_proj.loraA.weight           |  TRAIN    | [8, 1280]\n",
      "unet.decoders.4.1.attention_1.in_proj.loraB.weight           |  TRAIN    | [3840, 8]\n",
      "unet.decoders.4.1.attention_1.out_proj.originLinear.weight   |  FROZEN  | [1280, 1280]\n",
      "unet.decoders.4.1.attention_1.out_proj.originLinear.bias     |  FROZEN  | [1280]\n",
      "unet.decoders.4.1.attention_1.out_proj.loraA.weight          |  TRAIN    | [8, 1280]\n",
      "unet.decoders.4.1.attention_1.out_proj.loraB.weight          |  TRAIN    | [1280, 8]\n",
      "unet.decoders.4.1.layernorm_2.weight                         |  FROZEN  | [1280]\n",
      "unet.decoders.4.1.layernorm_2.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.4.1.attention_2.q_proj.originLinear.weight     |  FROZEN  | [1280, 1280]\n",
      "unet.decoders.4.1.attention_2.q_proj.loraA.weight            |  TRAIN    | [8, 1280]\n",
      "unet.decoders.4.1.attention_2.q_proj.loraB.weight            |  TRAIN    | [1280, 8]\n",
      "unet.decoders.4.1.attention_2.k_proj.originLinear.weight     |  FROZEN  | [1280, 768]\n",
      "unet.decoders.4.1.attention_2.k_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.decoders.4.1.attention_2.k_proj.loraB.weight            |  TRAIN    | [1280, 8]\n",
      "unet.decoders.4.1.attention_2.v_proj.originLinear.weight     |  FROZEN  | [1280, 768]\n",
      "unet.decoders.4.1.attention_2.v_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.decoders.4.1.attention_2.v_proj.loraB.weight            |  TRAIN    | [1280, 8]\n",
      "unet.decoders.4.1.attention_2.out_proj.originLinear.weight   |  FROZEN  | [1280, 1280]\n",
      "unet.decoders.4.1.attention_2.out_proj.originLinear.bias     |  FROZEN  | [1280]\n",
      "unet.decoders.4.1.attention_2.out_proj.loraA.weight          |  TRAIN    | [8, 1280]\n",
      "unet.decoders.4.1.attention_2.out_proj.loraB.weight          |  TRAIN    | [1280, 8]\n",
      "unet.decoders.4.1.layernorm_3.weight                         |  FROZEN  | [1280]\n",
      "unet.decoders.4.1.layernorm_3.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.4.1.linear_geglu_1.weight                      |  FROZEN  | [10240, 1280]\n",
      "unet.decoders.4.1.linear_geglu_1.bias                        |  FROZEN  | [10240]\n",
      "unet.decoders.4.1.linear_geglu_2.weight                      |  FROZEN  | [1280, 5120]\n",
      "unet.decoders.4.1.linear_geglu_2.bias                        |  FROZEN  | [1280]\n",
      "unet.decoders.4.1.conv_output.weight                         |  FROZEN  | [1280, 1280, 1, 1]\n",
      "unet.decoders.4.1.conv_output.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.5.0.groupnorm_feature.weight                   |  FROZEN  | [1920]\n",
      "unet.decoders.5.0.groupnorm_feature.bias                     |  FROZEN  | [1920]\n",
      "unet.decoders.5.0.conv_feature.weight                        |  FROZEN  | [1280, 1920, 3, 3]\n",
      "unet.decoders.5.0.conv_feature.bias                          |  FROZEN  | [1280]\n",
      "unet.decoders.5.0.linear_time.weight                         |  FROZEN  | [1280, 1280]\n",
      "unet.decoders.5.0.linear_time.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.5.0.groupnorm_merged.weight                    |  FROZEN  | [1280]\n",
      "unet.decoders.5.0.groupnorm_merged.bias                      |  FROZEN  | [1280]\n",
      "unet.decoders.5.0.conv_merged.weight                         |  FROZEN  | [1280, 1280, 3, 3]\n",
      "unet.decoders.5.0.conv_merged.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.5.0.residual_layer.weight                      |  FROZEN  | [1280, 1920, 1, 1]\n",
      "unet.decoders.5.0.residual_layer.bias                        |  FROZEN  | [1280]\n",
      "unet.decoders.5.1.groupnorm.weight                           |  FROZEN  | [1280]\n",
      "unet.decoders.5.1.groupnorm.bias                             |  FROZEN  | [1280]\n",
      "unet.decoders.5.1.conv_input.weight                          |  FROZEN  | [1280, 1280, 1, 1]\n",
      "unet.decoders.5.1.conv_input.bias                            |  FROZEN  | [1280]\n",
      "unet.decoders.5.1.layernorm_1.weight                         |  FROZEN  | [1280]\n",
      "unet.decoders.5.1.layernorm_1.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.5.1.attention_1.in_proj.originLinear.weight    |  FROZEN  | [3840, 1280]\n",
      "unet.decoders.5.1.attention_1.in_proj.loraA.weight           |  TRAIN    | [8, 1280]\n",
      "unet.decoders.5.1.attention_1.in_proj.loraB.weight           |  TRAIN    | [3840, 8]\n",
      "unet.decoders.5.1.attention_1.out_proj.originLinear.weight   |  FROZEN  | [1280, 1280]\n",
      "unet.decoders.5.1.attention_1.out_proj.originLinear.bias     |  FROZEN  | [1280]\n",
      "unet.decoders.5.1.attention_1.out_proj.loraA.weight          |  TRAIN    | [8, 1280]\n",
      "unet.decoders.5.1.attention_1.out_proj.loraB.weight          |  TRAIN    | [1280, 8]\n",
      "unet.decoders.5.1.layernorm_2.weight                         |  FROZEN  | [1280]\n",
      "unet.decoders.5.1.layernorm_2.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.5.1.attention_2.q_proj.originLinear.weight     |  FROZEN  | [1280, 1280]\n",
      "unet.decoders.5.1.attention_2.q_proj.loraA.weight            |  TRAIN    | [8, 1280]\n",
      "unet.decoders.5.1.attention_2.q_proj.loraB.weight            |  TRAIN    | [1280, 8]\n",
      "unet.decoders.5.1.attention_2.k_proj.originLinear.weight     |  FROZEN  | [1280, 768]\n",
      "unet.decoders.5.1.attention_2.k_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.decoders.5.1.attention_2.k_proj.loraB.weight            |  TRAIN    | [1280, 8]\n",
      "unet.decoders.5.1.attention_2.v_proj.originLinear.weight     |  FROZEN  | [1280, 768]\n",
      "unet.decoders.5.1.attention_2.v_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.decoders.5.1.attention_2.v_proj.loraB.weight            |  TRAIN    | [1280, 8]\n",
      "unet.decoders.5.1.attention_2.out_proj.originLinear.weight   |  FROZEN  | [1280, 1280]\n",
      "unet.decoders.5.1.attention_2.out_proj.originLinear.bias     |  FROZEN  | [1280]\n",
      "unet.decoders.5.1.attention_2.out_proj.loraA.weight          |  TRAIN    | [8, 1280]\n",
      "unet.decoders.5.1.attention_2.out_proj.loraB.weight          |  TRAIN    | [1280, 8]\n",
      "unet.decoders.5.1.layernorm_3.weight                         |  FROZEN  | [1280]\n",
      "unet.decoders.5.1.layernorm_3.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.5.1.linear_geglu_1.weight                      |  FROZEN  | [10240, 1280]\n",
      "unet.decoders.5.1.linear_geglu_1.bias                        |  FROZEN  | [10240]\n",
      "unet.decoders.5.1.linear_geglu_2.weight                      |  FROZEN  | [1280, 5120]\n",
      "unet.decoders.5.1.linear_geglu_2.bias                        |  FROZEN  | [1280]\n",
      "unet.decoders.5.1.conv_output.weight                         |  FROZEN  | [1280, 1280, 1, 1]\n",
      "unet.decoders.5.1.conv_output.bias                           |  FROZEN  | [1280]\n",
      "unet.decoders.5.2.conv.weight                                |  FROZEN  | [1280, 1280, 3, 3]\n",
      "unet.decoders.5.2.conv.bias                                  |  FROZEN  | [1280]\n",
      "unet.decoders.6.0.groupnorm_feature.weight                   |  FROZEN  | [1920]\n",
      "unet.decoders.6.0.groupnorm_feature.bias                     |  FROZEN  | [1920]\n",
      "unet.decoders.6.0.conv_feature.weight                        |  FROZEN  | [640, 1920, 3, 3]\n",
      "unet.decoders.6.0.conv_feature.bias                          |  FROZEN  | [640]\n",
      "unet.decoders.6.0.linear_time.weight                         |  FROZEN  | [640, 1280]\n",
      "unet.decoders.6.0.linear_time.bias                           |  FROZEN  | [640]\n",
      "unet.decoders.6.0.groupnorm_merged.weight                    |  FROZEN  | [640]\n",
      "unet.decoders.6.0.groupnorm_merged.bias                      |  FROZEN  | [640]\n",
      "unet.decoders.6.0.conv_merged.weight                         |  FROZEN  | [640, 640, 3, 3]\n",
      "unet.decoders.6.0.conv_merged.bias                           |  FROZEN  | [640]\n",
      "unet.decoders.6.0.residual_layer.weight                      |  FROZEN  | [640, 1920, 1, 1]\n",
      "unet.decoders.6.0.residual_layer.bias                        |  FROZEN  | [640]\n",
      "unet.decoders.6.1.groupnorm.weight                           |  FROZEN  | [640]\n",
      "unet.decoders.6.1.groupnorm.bias                             |  FROZEN  | [640]\n",
      "unet.decoders.6.1.conv_input.weight                          |  FROZEN  | [640, 640, 1, 1]\n",
      "unet.decoders.6.1.conv_input.bias                            |  FROZEN  | [640]\n",
      "unet.decoders.6.1.layernorm_1.weight                         |  FROZEN  | [640]\n",
      "unet.decoders.6.1.layernorm_1.bias                           |  FROZEN  | [640]\n",
      "unet.decoders.6.1.attention_1.in_proj.originLinear.weight    |  FROZEN  | [1920, 640]\n",
      "unet.decoders.6.1.attention_1.in_proj.loraA.weight           |  TRAIN    | [8, 640]\n",
      "unet.decoders.6.1.attention_1.in_proj.loraB.weight           |  TRAIN    | [1920, 8]\n",
      "unet.decoders.6.1.attention_1.out_proj.originLinear.weight   |  FROZEN  | [640, 640]\n",
      "unet.decoders.6.1.attention_1.out_proj.originLinear.bias     |  FROZEN  | [640]\n",
      "unet.decoders.6.1.attention_1.out_proj.loraA.weight          |  TRAIN    | [8, 640]\n",
      "unet.decoders.6.1.attention_1.out_proj.loraB.weight          |  TRAIN    | [640, 8]\n",
      "unet.decoders.6.1.layernorm_2.weight                         |  FROZEN  | [640]\n",
      "unet.decoders.6.1.layernorm_2.bias                           |  FROZEN  | [640]\n",
      "unet.decoders.6.1.attention_2.q_proj.originLinear.weight     |  FROZEN  | [640, 640]\n",
      "unet.decoders.6.1.attention_2.q_proj.loraA.weight            |  TRAIN    | [8, 640]\n",
      "unet.decoders.6.1.attention_2.q_proj.loraB.weight            |  TRAIN    | [640, 8]\n",
      "unet.decoders.6.1.attention_2.k_proj.originLinear.weight     |  FROZEN  | [640, 768]\n",
      "unet.decoders.6.1.attention_2.k_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.decoders.6.1.attention_2.k_proj.loraB.weight            |  TRAIN    | [640, 8]\n",
      "unet.decoders.6.1.attention_2.v_proj.originLinear.weight     |  FROZEN  | [640, 768]\n",
      "unet.decoders.6.1.attention_2.v_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.decoders.6.1.attention_2.v_proj.loraB.weight            |  TRAIN    | [640, 8]\n",
      "unet.decoders.6.1.attention_2.out_proj.originLinear.weight   |  FROZEN  | [640, 640]\n",
      "unet.decoders.6.1.attention_2.out_proj.originLinear.bias     |  FROZEN  | [640]\n",
      "unet.decoders.6.1.attention_2.out_proj.loraA.weight          |  TRAIN    | [8, 640]\n",
      "unet.decoders.6.1.attention_2.out_proj.loraB.weight          |  TRAIN    | [640, 8]\n",
      "unet.decoders.6.1.layernorm_3.weight                         |  FROZEN  | [640]\n",
      "unet.decoders.6.1.layernorm_3.bias                           |  FROZEN  | [640]\n",
      "unet.decoders.6.1.linear_geglu_1.weight                      |  FROZEN  | [5120, 640]\n",
      "unet.decoders.6.1.linear_geglu_1.bias                        |  FROZEN  | [5120]\n",
      "unet.decoders.6.1.linear_geglu_2.weight                      |  FROZEN  | [640, 2560]\n",
      "unet.decoders.6.1.linear_geglu_2.bias                        |  FROZEN  | [640]\n",
      "unet.decoders.6.1.conv_output.weight                         |  FROZEN  | [640, 640, 1, 1]\n",
      "unet.decoders.6.1.conv_output.bias                           |  FROZEN  | [640]\n",
      "unet.decoders.7.0.groupnorm_feature.weight                   |  FROZEN  | [1280]\n",
      "unet.decoders.7.0.groupnorm_feature.bias                     |  FROZEN  | [1280]\n",
      "unet.decoders.7.0.conv_feature.weight                        |  FROZEN  | [640, 1280, 3, 3]\n",
      "unet.decoders.7.0.conv_feature.bias                          |  FROZEN  | [640]\n",
      "unet.decoders.7.0.linear_time.weight                         |  FROZEN  | [640, 1280]\n",
      "unet.decoders.7.0.linear_time.bias                           |  FROZEN  | [640]\n",
      "unet.decoders.7.0.groupnorm_merged.weight                    |  FROZEN  | [640]\n",
      "unet.decoders.7.0.groupnorm_merged.bias                      |  FROZEN  | [640]\n",
      "unet.decoders.7.0.conv_merged.weight                         |  FROZEN  | [640, 640, 3, 3]\n",
      "unet.decoders.7.0.conv_merged.bias                           |  FROZEN  | [640]\n",
      "unet.decoders.7.0.residual_layer.weight                      |  FROZEN  | [640, 1280, 1, 1]\n",
      "unet.decoders.7.0.residual_layer.bias                        |  FROZEN  | [640]\n",
      "unet.decoders.7.1.groupnorm.weight                           |  FROZEN  | [640]\n",
      "unet.decoders.7.1.groupnorm.bias                             |  FROZEN  | [640]\n",
      "unet.decoders.7.1.conv_input.weight                          |  FROZEN  | [640, 640, 1, 1]\n",
      "unet.decoders.7.1.conv_input.bias                            |  FROZEN  | [640]\n",
      "unet.decoders.7.1.layernorm_1.weight                         |  FROZEN  | [640]\n",
      "unet.decoders.7.1.layernorm_1.bias                           |  FROZEN  | [640]\n",
      "unet.decoders.7.1.attention_1.in_proj.originLinear.weight    |  FROZEN  | [1920, 640]\n",
      "unet.decoders.7.1.attention_1.in_proj.loraA.weight           |  TRAIN    | [8, 640]\n",
      "unet.decoders.7.1.attention_1.in_proj.loraB.weight           |  TRAIN    | [1920, 8]\n",
      "unet.decoders.7.1.attention_1.out_proj.originLinear.weight   |  FROZEN  | [640, 640]\n",
      "unet.decoders.7.1.attention_1.out_proj.originLinear.bias     |  FROZEN  | [640]\n",
      "unet.decoders.7.1.attention_1.out_proj.loraA.weight          |  TRAIN    | [8, 640]\n",
      "unet.decoders.7.1.attention_1.out_proj.loraB.weight          |  TRAIN    | [640, 8]\n",
      "unet.decoders.7.1.layernorm_2.weight                         |  FROZEN  | [640]\n",
      "unet.decoders.7.1.layernorm_2.bias                           |  FROZEN  | [640]\n",
      "unet.decoders.7.1.attention_2.q_proj.originLinear.weight     |  FROZEN  | [640, 640]\n",
      "unet.decoders.7.1.attention_2.q_proj.loraA.weight            |  TRAIN    | [8, 640]\n",
      "unet.decoders.7.1.attention_2.q_proj.loraB.weight            |  TRAIN    | [640, 8]\n",
      "unet.decoders.7.1.attention_2.k_proj.originLinear.weight     |  FROZEN  | [640, 768]\n",
      "unet.decoders.7.1.attention_2.k_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.decoders.7.1.attention_2.k_proj.loraB.weight            |  TRAIN    | [640, 8]\n",
      "unet.decoders.7.1.attention_2.v_proj.originLinear.weight     |  FROZEN  | [640, 768]\n",
      "unet.decoders.7.1.attention_2.v_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.decoders.7.1.attention_2.v_proj.loraB.weight            |  TRAIN    | [640, 8]\n",
      "unet.decoders.7.1.attention_2.out_proj.originLinear.weight   |  FROZEN  | [640, 640]\n",
      "unet.decoders.7.1.attention_2.out_proj.originLinear.bias     |  FROZEN  | [640]\n",
      "unet.decoders.7.1.attention_2.out_proj.loraA.weight          |  TRAIN    | [8, 640]\n",
      "unet.decoders.7.1.attention_2.out_proj.loraB.weight          |  TRAIN    | [640, 8]\n",
      "unet.decoders.7.1.layernorm_3.weight                         |  FROZEN  | [640]\n",
      "unet.decoders.7.1.layernorm_3.bias                           |  FROZEN  | [640]\n",
      "unet.decoders.7.1.linear_geglu_1.weight                      |  FROZEN  | [5120, 640]\n",
      "unet.decoders.7.1.linear_geglu_1.bias                        |  FROZEN  | [5120]\n",
      "unet.decoders.7.1.linear_geglu_2.weight                      |  FROZEN  | [640, 2560]\n",
      "unet.decoders.7.1.linear_geglu_2.bias                        |  FROZEN  | [640]\n",
      "unet.decoders.7.1.conv_output.weight                         |  FROZEN  | [640, 640, 1, 1]\n",
      "unet.decoders.7.1.conv_output.bias                           |  FROZEN  | [640]\n",
      "unet.decoders.8.0.groupnorm_feature.weight                   |  FROZEN  | [960]\n",
      "unet.decoders.8.0.groupnorm_feature.bias                     |  FROZEN  | [960]\n",
      "unet.decoders.8.0.conv_feature.weight                        |  FROZEN  | [640, 960, 3, 3]\n",
      "unet.decoders.8.0.conv_feature.bias                          |  FROZEN  | [640]\n",
      "unet.decoders.8.0.linear_time.weight                         |  FROZEN  | [640, 1280]\n",
      "unet.decoders.8.0.linear_time.bias                           |  FROZEN  | [640]\n",
      "unet.decoders.8.0.groupnorm_merged.weight                    |  FROZEN  | [640]\n",
      "unet.decoders.8.0.groupnorm_merged.bias                      |  FROZEN  | [640]\n",
      "unet.decoders.8.0.conv_merged.weight                         |  FROZEN  | [640, 640, 3, 3]\n",
      "unet.decoders.8.0.conv_merged.bias                           |  FROZEN  | [640]\n",
      "unet.decoders.8.0.residual_layer.weight                      |  FROZEN  | [640, 960, 1, 1]\n",
      "unet.decoders.8.0.residual_layer.bias                        |  FROZEN  | [640]\n",
      "unet.decoders.8.1.groupnorm.weight                           |  FROZEN  | [640]\n",
      "unet.decoders.8.1.groupnorm.bias                             |  FROZEN  | [640]\n",
      "unet.decoders.8.1.conv_input.weight                          |  FROZEN  | [640, 640, 1, 1]\n",
      "unet.decoders.8.1.conv_input.bias                            |  FROZEN  | [640]\n",
      "unet.decoders.8.1.layernorm_1.weight                         |  FROZEN  | [640]\n",
      "unet.decoders.8.1.layernorm_1.bias                           |  FROZEN  | [640]\n",
      "unet.decoders.8.1.attention_1.in_proj.originLinear.weight    |  FROZEN  | [1920, 640]\n",
      "unet.decoders.8.1.attention_1.in_proj.loraA.weight           |  TRAIN    | [8, 640]\n",
      "unet.decoders.8.1.attention_1.in_proj.loraB.weight           |  TRAIN    | [1920, 8]\n",
      "unet.decoders.8.1.attention_1.out_proj.originLinear.weight   |  FROZEN  | [640, 640]\n",
      "unet.decoders.8.1.attention_1.out_proj.originLinear.bias     |  FROZEN  | [640]\n",
      "unet.decoders.8.1.attention_1.out_proj.loraA.weight          |  TRAIN    | [8, 640]\n",
      "unet.decoders.8.1.attention_1.out_proj.loraB.weight          |  TRAIN    | [640, 8]\n",
      "unet.decoders.8.1.layernorm_2.weight                         |  FROZEN  | [640]\n",
      "unet.decoders.8.1.layernorm_2.bias                           |  FROZEN  | [640]\n",
      "unet.decoders.8.1.attention_2.q_proj.originLinear.weight     |  FROZEN  | [640, 640]\n",
      "unet.decoders.8.1.attention_2.q_proj.loraA.weight            |  TRAIN    | [8, 640]\n",
      "unet.decoders.8.1.attention_2.q_proj.loraB.weight            |  TRAIN    | [640, 8]\n",
      "unet.decoders.8.1.attention_2.k_proj.originLinear.weight     |  FROZEN  | [640, 768]\n",
      "unet.decoders.8.1.attention_2.k_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.decoders.8.1.attention_2.k_proj.loraB.weight            |  TRAIN    | [640, 8]\n",
      "unet.decoders.8.1.attention_2.v_proj.originLinear.weight     |  FROZEN  | [640, 768]\n",
      "unet.decoders.8.1.attention_2.v_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.decoders.8.1.attention_2.v_proj.loraB.weight            |  TRAIN    | [640, 8]\n",
      "unet.decoders.8.1.attention_2.out_proj.originLinear.weight   |  FROZEN  | [640, 640]\n",
      "unet.decoders.8.1.attention_2.out_proj.originLinear.bias     |  FROZEN  | [640]\n",
      "unet.decoders.8.1.attention_2.out_proj.loraA.weight          |  TRAIN    | [8, 640]\n",
      "unet.decoders.8.1.attention_2.out_proj.loraB.weight          |  TRAIN    | [640, 8]\n",
      "unet.decoders.8.1.layernorm_3.weight                         |  FROZEN  | [640]\n",
      "unet.decoders.8.1.layernorm_3.bias                           |  FROZEN  | [640]\n",
      "unet.decoders.8.1.linear_geglu_1.weight                      |  FROZEN  | [5120, 640]\n",
      "unet.decoders.8.1.linear_geglu_1.bias                        |  FROZEN  | [5120]\n",
      "unet.decoders.8.1.linear_geglu_2.weight                      |  FROZEN  | [640, 2560]\n",
      "unet.decoders.8.1.linear_geglu_2.bias                        |  FROZEN  | [640]\n",
      "unet.decoders.8.1.conv_output.weight                         |  FROZEN  | [640, 640, 1, 1]\n",
      "unet.decoders.8.1.conv_output.bias                           |  FROZEN  | [640]\n",
      "unet.decoders.8.2.conv.weight                                |  FROZEN  | [640, 640, 3, 3]\n",
      "unet.decoders.8.2.conv.bias                                  |  FROZEN  | [640]\n",
      "unet.decoders.9.0.groupnorm_feature.weight                   |  FROZEN  | [960]\n",
      "unet.decoders.9.0.groupnorm_feature.bias                     |  FROZEN  | [960]\n",
      "unet.decoders.9.0.conv_feature.weight                        |  FROZEN  | [320, 960, 3, 3]\n",
      "unet.decoders.9.0.conv_feature.bias                          |  FROZEN  | [320]\n",
      "unet.decoders.9.0.linear_time.weight                         |  FROZEN  | [320, 1280]\n",
      "unet.decoders.9.0.linear_time.bias                           |  FROZEN  | [320]\n",
      "unet.decoders.9.0.groupnorm_merged.weight                    |  FROZEN  | [320]\n",
      "unet.decoders.9.0.groupnorm_merged.bias                      |  FROZEN  | [320]\n",
      "unet.decoders.9.0.conv_merged.weight                         |  FROZEN  | [320, 320, 3, 3]\n",
      "unet.decoders.9.0.conv_merged.bias                           |  FROZEN  | [320]\n",
      "unet.decoders.9.0.residual_layer.weight                      |  FROZEN  | [320, 960, 1, 1]\n",
      "unet.decoders.9.0.residual_layer.bias                        |  FROZEN  | [320]\n",
      "unet.decoders.9.1.groupnorm.weight                           |  FROZEN  | [320]\n",
      "unet.decoders.9.1.groupnorm.bias                             |  FROZEN  | [320]\n",
      "unet.decoders.9.1.conv_input.weight                          |  FROZEN  | [320, 320, 1, 1]\n",
      "unet.decoders.9.1.conv_input.bias                            |  FROZEN  | [320]\n",
      "unet.decoders.9.1.layernorm_1.weight                         |  FROZEN  | [320]\n",
      "unet.decoders.9.1.layernorm_1.bias                           |  FROZEN  | [320]\n",
      "unet.decoders.9.1.attention_1.in_proj.originLinear.weight    |  FROZEN  | [960, 320]\n",
      "unet.decoders.9.1.attention_1.in_proj.loraA.weight           |  TRAIN    | [8, 320]\n",
      "unet.decoders.9.1.attention_1.in_proj.loraB.weight           |  TRAIN    | [960, 8]\n",
      "unet.decoders.9.1.attention_1.out_proj.originLinear.weight   |  FROZEN  | [320, 320]\n",
      "unet.decoders.9.1.attention_1.out_proj.originLinear.bias     |  FROZEN  | [320]\n",
      "unet.decoders.9.1.attention_1.out_proj.loraA.weight          |  TRAIN    | [8, 320]\n",
      "unet.decoders.9.1.attention_1.out_proj.loraB.weight          |  TRAIN    | [320, 8]\n",
      "unet.decoders.9.1.layernorm_2.weight                         |  FROZEN  | [320]\n",
      "unet.decoders.9.1.layernorm_2.bias                           |  FROZEN  | [320]\n",
      "unet.decoders.9.1.attention_2.q_proj.originLinear.weight     |  FROZEN  | [320, 320]\n",
      "unet.decoders.9.1.attention_2.q_proj.loraA.weight            |  TRAIN    | [8, 320]\n",
      "unet.decoders.9.1.attention_2.q_proj.loraB.weight            |  TRAIN    | [320, 8]\n",
      "unet.decoders.9.1.attention_2.k_proj.originLinear.weight     |  FROZEN  | [320, 768]\n",
      "unet.decoders.9.1.attention_2.k_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.decoders.9.1.attention_2.k_proj.loraB.weight            |  TRAIN    | [320, 8]\n",
      "unet.decoders.9.1.attention_2.v_proj.originLinear.weight     |  FROZEN  | [320, 768]\n",
      "unet.decoders.9.1.attention_2.v_proj.loraA.weight            |  TRAIN    | [8, 768]\n",
      "unet.decoders.9.1.attention_2.v_proj.loraB.weight            |  TRAIN    | [320, 8]\n",
      "unet.decoders.9.1.attention_2.out_proj.originLinear.weight   |  FROZEN  | [320, 320]\n",
      "unet.decoders.9.1.attention_2.out_proj.originLinear.bias     |  FROZEN  | [320]\n",
      "unet.decoders.9.1.attention_2.out_proj.loraA.weight          |  TRAIN    | [8, 320]\n",
      "unet.decoders.9.1.attention_2.out_proj.loraB.weight          |  TRAIN    | [320, 8]\n",
      "unet.decoders.9.1.layernorm_3.weight                         |  FROZEN  | [320]\n",
      "unet.decoders.9.1.layernorm_3.bias                           |  FROZEN  | [320]\n",
      "unet.decoders.9.1.linear_geglu_1.weight                      |  FROZEN  | [2560, 320]\n",
      "unet.decoders.9.1.linear_geglu_1.bias                        |  FROZEN  | [2560]\n",
      "unet.decoders.9.1.linear_geglu_2.weight                      |  FROZEN  | [320, 1280]\n",
      "unet.decoders.9.1.linear_geglu_2.bias                        |  FROZEN  | [320]\n",
      "unet.decoders.9.1.conv_output.weight                         |  FROZEN  | [320, 320, 1, 1]\n",
      "unet.decoders.9.1.conv_output.bias                           |  FROZEN  | [320]\n",
      "unet.decoders.10.0.groupnorm_feature.weight                  |  FROZEN  | [640]\n",
      "unet.decoders.10.0.groupnorm_feature.bias                    |  FROZEN  | [640]\n",
      "unet.decoders.10.0.conv_feature.weight                       |  FROZEN  | [320, 640, 3, 3]\n",
      "unet.decoders.10.0.conv_feature.bias                         |  FROZEN  | [320]\n",
      "unet.decoders.10.0.linear_time.weight                        |  FROZEN  | [320, 1280]\n",
      "unet.decoders.10.0.linear_time.bias                          |  FROZEN  | [320]\n",
      "unet.decoders.10.0.groupnorm_merged.weight                   |  FROZEN  | [320]\n",
      "unet.decoders.10.0.groupnorm_merged.bias                     |  FROZEN  | [320]\n",
      "unet.decoders.10.0.conv_merged.weight                        |  FROZEN  | [320, 320, 3, 3]\n",
      "unet.decoders.10.0.conv_merged.bias                          |  FROZEN  | [320]\n",
      "unet.decoders.10.0.residual_layer.weight                     |  FROZEN  | [320, 640, 1, 1]\n",
      "unet.decoders.10.0.residual_layer.bias                       |  FROZEN  | [320]\n",
      "unet.decoders.10.1.groupnorm.weight                          |  FROZEN  | [320]\n",
      "unet.decoders.10.1.groupnorm.bias                            |  FROZEN  | [320]\n",
      "unet.decoders.10.1.conv_input.weight                         |  FROZEN  | [320, 320, 1, 1]\n",
      "unet.decoders.10.1.conv_input.bias                           |  FROZEN  | [320]\n",
      "unet.decoders.10.1.layernorm_1.weight                        |  FROZEN  | [320]\n",
      "unet.decoders.10.1.layernorm_1.bias                          |  FROZEN  | [320]\n",
      "unet.decoders.10.1.attention_1.in_proj.originLinear.weight   |  FROZEN  | [960, 320]\n",
      "unet.decoders.10.1.attention_1.in_proj.loraA.weight          |  TRAIN    | [8, 320]\n",
      "unet.decoders.10.1.attention_1.in_proj.loraB.weight          |  TRAIN    | [960, 8]\n",
      "unet.decoders.10.1.attention_1.out_proj.originLinear.weight  |  FROZEN  | [320, 320]\n",
      "unet.decoders.10.1.attention_1.out_proj.originLinear.bias    |  FROZEN  | [320]\n",
      "unet.decoders.10.1.attention_1.out_proj.loraA.weight         |  TRAIN    | [8, 320]\n",
      "unet.decoders.10.1.attention_1.out_proj.loraB.weight         |  TRAIN    | [320, 8]\n",
      "unet.decoders.10.1.layernorm_2.weight                        |  FROZEN  | [320]\n",
      "unet.decoders.10.1.layernorm_2.bias                          |  FROZEN  | [320]\n",
      "unet.decoders.10.1.attention_2.q_proj.originLinear.weight    |  FROZEN  | [320, 320]\n",
      "unet.decoders.10.1.attention_2.q_proj.loraA.weight           |  TRAIN    | [8, 320]\n",
      "unet.decoders.10.1.attention_2.q_proj.loraB.weight           |  TRAIN    | [320, 8]\n",
      "unet.decoders.10.1.attention_2.k_proj.originLinear.weight    |  FROZEN  | [320, 768]\n",
      "unet.decoders.10.1.attention_2.k_proj.loraA.weight           |  TRAIN    | [8, 768]\n",
      "unet.decoders.10.1.attention_2.k_proj.loraB.weight           |  TRAIN    | [320, 8]\n",
      "unet.decoders.10.1.attention_2.v_proj.originLinear.weight    |  FROZEN  | [320, 768]\n",
      "unet.decoders.10.1.attention_2.v_proj.loraA.weight           |  TRAIN    | [8, 768]\n",
      "unet.decoders.10.1.attention_2.v_proj.loraB.weight           |  TRAIN    | [320, 8]\n",
      "unet.decoders.10.1.attention_2.out_proj.originLinear.weight  |  FROZEN  | [320, 320]\n",
      "unet.decoders.10.1.attention_2.out_proj.originLinear.bias    |  FROZEN  | [320]\n",
      "unet.decoders.10.1.attention_2.out_proj.loraA.weight         |  TRAIN    | [8, 320]\n",
      "unet.decoders.10.1.attention_2.out_proj.loraB.weight         |  TRAIN    | [320, 8]\n",
      "unet.decoders.10.1.layernorm_3.weight                        |  FROZEN  | [320]\n",
      "unet.decoders.10.1.layernorm_3.bias                          |  FROZEN  | [320]\n",
      "unet.decoders.10.1.linear_geglu_1.weight                     |  FROZEN  | [2560, 320]\n",
      "unet.decoders.10.1.linear_geglu_1.bias                       |  FROZEN  | [2560]\n",
      "unet.decoders.10.1.linear_geglu_2.weight                     |  FROZEN  | [320, 1280]\n",
      "unet.decoders.10.1.linear_geglu_2.bias                       |  FROZEN  | [320]\n",
      "unet.decoders.10.1.conv_output.weight                        |  FROZEN  | [320, 320, 1, 1]\n",
      "unet.decoders.10.1.conv_output.bias                          |  FROZEN  | [320]\n",
      "unet.decoders.11.0.groupnorm_feature.weight                  |  FROZEN  | [640]\n",
      "unet.decoders.11.0.groupnorm_feature.bias                    |  FROZEN  | [640]\n",
      "unet.decoders.11.0.conv_feature.weight                       |  FROZEN  | [320, 640, 3, 3]\n",
      "unet.decoders.11.0.conv_feature.bias                         |  FROZEN  | [320]\n",
      "unet.decoders.11.0.linear_time.weight                        |  FROZEN  | [320, 1280]\n",
      "unet.decoders.11.0.linear_time.bias                          |  FROZEN  | [320]\n",
      "unet.decoders.11.0.groupnorm_merged.weight                   |  FROZEN  | [320]\n",
      "unet.decoders.11.0.groupnorm_merged.bias                     |  FROZEN  | [320]\n",
      "unet.decoders.11.0.conv_merged.weight                        |  FROZEN  | [320, 320, 3, 3]\n",
      "unet.decoders.11.0.conv_merged.bias                          |  FROZEN  | [320]\n",
      "unet.decoders.11.0.residual_layer.weight                     |  FROZEN  | [320, 640, 1, 1]\n",
      "unet.decoders.11.0.residual_layer.bias                       |  FROZEN  | [320]\n",
      "unet.decoders.11.1.groupnorm.weight                          |  FROZEN  | [320]\n",
      "unet.decoders.11.1.groupnorm.bias                            |  FROZEN  | [320]\n",
      "unet.decoders.11.1.conv_input.weight                         |  FROZEN  | [320, 320, 1, 1]\n",
      "unet.decoders.11.1.conv_input.bias                           |  FROZEN  | [320]\n",
      "unet.decoders.11.1.layernorm_1.weight                        |  FROZEN  | [320]\n",
      "unet.decoders.11.1.layernorm_1.bias                          |  FROZEN  | [320]\n",
      "unet.decoders.11.1.attention_1.in_proj.originLinear.weight   |  FROZEN  | [960, 320]\n",
      "unet.decoders.11.1.attention_1.in_proj.loraA.weight          |  TRAIN    | [8, 320]\n",
      "unet.decoders.11.1.attention_1.in_proj.loraB.weight          |  TRAIN    | [960, 8]\n",
      "unet.decoders.11.1.attention_1.out_proj.originLinear.weight  |  FROZEN  | [320, 320]\n",
      "unet.decoders.11.1.attention_1.out_proj.originLinear.bias    |  FROZEN  | [320]\n",
      "unet.decoders.11.1.attention_1.out_proj.loraA.weight         |  TRAIN    | [8, 320]\n",
      "unet.decoders.11.1.attention_1.out_proj.loraB.weight         |  TRAIN    | [320, 8]\n",
      "unet.decoders.11.1.layernorm_2.weight                        |  FROZEN  | [320]\n",
      "unet.decoders.11.1.layernorm_2.bias                          |  FROZEN  | [320]\n",
      "unet.decoders.11.1.attention_2.q_proj.originLinear.weight    |  FROZEN  | [320, 320]\n",
      "unet.decoders.11.1.attention_2.q_proj.loraA.weight           |  TRAIN    | [8, 320]\n",
      "unet.decoders.11.1.attention_2.q_proj.loraB.weight           |  TRAIN    | [320, 8]\n",
      "unet.decoders.11.1.attention_2.k_proj.originLinear.weight    |  FROZEN  | [320, 768]\n",
      "unet.decoders.11.1.attention_2.k_proj.loraA.weight           |  TRAIN    | [8, 768]\n",
      "unet.decoders.11.1.attention_2.k_proj.loraB.weight           |  TRAIN    | [320, 8]\n",
      "unet.decoders.11.1.attention_2.v_proj.originLinear.weight    |  FROZEN  | [320, 768]\n",
      "unet.decoders.11.1.attention_2.v_proj.loraA.weight           |  TRAIN    | [8, 768]\n",
      "unet.decoders.11.1.attention_2.v_proj.loraB.weight           |  TRAIN    | [320, 8]\n",
      "unet.decoders.11.1.attention_2.out_proj.originLinear.weight  |  FROZEN  | [320, 320]\n",
      "unet.decoders.11.1.attention_2.out_proj.originLinear.bias    |  FROZEN  | [320]\n",
      "unet.decoders.11.1.attention_2.out_proj.loraA.weight         |  TRAIN    | [8, 320]\n",
      "unet.decoders.11.1.attention_2.out_proj.loraB.weight         |  TRAIN    | [320, 8]\n",
      "unet.decoders.11.1.layernorm_3.weight                        |  FROZEN  | [320]\n",
      "unet.decoders.11.1.layernorm_3.bias                          |  FROZEN  | [320]\n",
      "unet.decoders.11.1.linear_geglu_1.weight                     |  FROZEN  | [2560, 320]\n",
      "unet.decoders.11.1.linear_geglu_1.bias                       |  FROZEN  | [2560]\n",
      "unet.decoders.11.1.linear_geglu_2.weight                     |  FROZEN  | [320, 1280]\n",
      "unet.decoders.11.1.linear_geglu_2.bias                       |  FROZEN  | [320]\n",
      "unet.decoders.11.1.conv_output.weight                        |  FROZEN  | [320, 320, 1, 1]\n",
      "unet.decoders.11.1.conv_output.bias                          |  FROZEN  | [320]\n",
      "final.groupnorm.weight                                       |  FROZEN  | [320]\n",
      "final.groupnorm.bias                                         |  FROZEN  | [320]\n",
      "final.conv.weight                                            |  FROZEN  | [4, 320, 3, 3]\n",
      "final.conv.bias                                              |  FROZEN  | [4]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import StableDiffusion.LoraLayer\n",
    "import StableDiffusion.LoraUtils\n",
    "import importlib\n",
    "importlib.reload(StableDiffusion.LoraLayer)\n",
    "importlib.reload(StableDiffusion.LoraUtils)\n",
    "from StableDiffusion.LoraLayer import LoraLayer\n",
    "from StableDiffusion.LoraUtils import *\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "injectLora(diffusionProcess,rank = 8,alpha=16,\n",
    "           filterTuple=('in_proj','out_proj','k_proj','v_proj','q_proj'),\n",
    "           device=device)\n",
    "freezeModelWeights(clipEncoder)\n",
    "freezeModelWeights(vaeDecoder)\n",
    "freezeModelWeights(vaeDecoder)\n",
    "freezeModelWeights(diffusionProcess)\n",
    "checkModuleStatus(diffusionProcess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: VAE Encoder        | Memory: 130.32 MB\n",
      "Model: VAE Decoder        | Memory: 188.79 MB\n",
      "Model: CLIP Encoder       | Memory: 469.44 MB\n",
      "Model: UNet/Diffusion     | Memory: 3278.81 MB\n"
     ]
    }
   ],
   "source": [
    "def check_model_memory(model, name):\n",
    "    # Calculate bytes (params * size of float32/4 bytes)\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    total_size_mb = (param_size + buffer_size) / (1024**2)\n",
    "    print(f\"Model: {name:18} | Memory: {total_size_mb:.2f} MB\")\n",
    "\n",
    "check_model_memory(vaeEncoder, \"VAE Encoder\")\n",
    "check_model_memory(vaeDecoder, \"VAE Decoder\")\n",
    "check_model_memory(clipEncoder, \"CLIP Encoder\")\n",
    "check_model_memory(diffusionProcess, \"UNet/Diffusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([170, 478, 870, 790], device='cuda:0')\n",
      "[170 478 870 790]\n",
      "[[170]\n",
      " [478]\n",
      " [870]\n",
      " [790]]\n"
     ]
    }
   ],
   "source": [
    "timeSteps =torch.randint(0,1000,(4,),device=device)\n",
    "print(timeSteps)\n",
    "timeSteps = timeSteps.cpu().numpy()\n",
    "print(timeSteps)\n",
    "BatchSize = timeSteps.shape[0]\n",
    "t = timeSteps\n",
    "t = t.reshape(BatchSize,1)\n",
    "print(t)\n",
    "#time = sampler.tensor2Numpy(timeSteps)\n",
    "#timeEmbedding320 = Utils.getTimeEmbedding(time)    \n",
    "#timeEmbedding320= sampler.numpy2Tensor(timeEmbedding320,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n",
      "(4, 160)\n",
      "(4, 320)\n"
     ]
    }
   ],
   "source": [
    "def getTimeEmbedding(timeStep:torch.Tensor)->np.ndarray:   \n",
    "    t = timeStep.cpu().numpy()\n",
    "    t = t[:,None]\n",
    "    #print(t.shape)\n",
    "    frequency = np.arange(0,160.0) \n",
    "    frequency = -frequency/160.0\n",
    "    frequency = 10000**frequency    \n",
    "    xk = t *frequency \n",
    "    #print(xk.shape)\n",
    "    timeEmbeddingCos = np.cos(xk)\n",
    "    timeEmbeddingSin = np.sin(xk)\n",
    "    timeEmbedding = np.concatenate([timeEmbeddingCos,timeEmbeddingSin],axis=1)\n",
    "    return timeEmbedding\n",
    "\n",
    "\n",
    "timeSteps =torch.randint(0,1000,(4,),device=device)\n",
    "timeEmb320 = getTimeEmbedding(timeSteps)\n",
    "print(timeEmb320.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512, 512]) torch.Size([77]) torch.Size([77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vae encoder input noise is none  use zeros\n",
      "token embedding shape torch.Size([2, 77, 768])\n",
      "torch.Size([2]) tensor([208, 775], device='cuda:0')\n",
      "torch.Size([2, 4, 64, 64]) torch.Size([2, 77, 768]) torch.Size([2, 320])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vae encoder input noise is none  use zeros\n",
      "token embedding shape torch.Size([2, 77, 768])\n",
      "torch.Size([2]) tensor([577, 431], device='cuda:0')\n",
      "torch.Size([2, 4, 64, 64]) torch.Size([2, 77, 768]) torch.Size([2, 320])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vae encoder input noise is none  use zeros\n",
      "token embedding shape torch.Size([2, 77, 768])\n",
      "torch.Size([2]) tensor([84, 95], device='cuda:0')\n",
      "torch.Size([2, 4, 64, 64]) torch.Size([2, 77, 768]) torch.Size([2, 320])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vae encoder input noise is none  use zeros\n",
      "token embedding shape torch.Size([2, 77, 768])\n",
      "torch.Size([2]) tensor([ 16, 371], device='cuda:0')\n",
      "torch.Size([2, 4, 64, 64]) torch.Size([2, 77, 768]) torch.Size([2, 320])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vae encoder input noise is none  use zeros\n",
      "token embedding shape torch.Size([2, 77, 768])\n",
      "torch.Size([2]) tensor([529,  33], device='cuda:0')\n",
      "torch.Size([2, 4, 64, 64]) torch.Size([2, 77, 768]) torch.Size([2, 320])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vae encoder input noise is none  use zeros\n",
      "token embedding shape torch.Size([2, 77, 768])\n",
      "torch.Size([2]) tensor([467, 128], device='cuda:0')\n",
      "torch.Size([2, 4, 64, 64]) torch.Size([2, 77, 768]) torch.Size([2, 320])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vae encoder input noise is none  use zeros\n",
      "token embedding shape torch.Size([2, 77, 768])\n",
      "torch.Size([2]) tensor([228, 615], device='cuda:0')\n",
      "torch.Size([2, 4, 64, 64]) torch.Size([2, 77, 768]) torch.Size([2, 320])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vae encoder input noise is none  use zeros\n",
      "token embedding shape torch.Size([2, 77, 768])\n",
      "torch.Size([2]) tensor([221, 402], device='cuda:0')\n",
      "torch.Size([2, 4, 64, 64]) torch.Size([2, 77, 768]) torch.Size([2, 320])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vae encoder input noise is none  use zeros\n",
      "token embedding shape torch.Size([2, 77, 768])\n",
      "torch.Size([2]) tensor([742, 294], device='cuda:0')\n",
      "torch.Size([2, 4, 64, 64]) torch.Size([2, 77, 768]) torch.Size([2, 320])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vae encoder input noise is none  use zeros\n",
      "token embedding shape torch.Size([2, 77, 768])\n",
      "torch.Size([2]) tensor([578, 914], device='cuda:0')\n",
      "torch.Size([2, 4, 64, 64]) torch.Size([2, 77, 768]) torch.Size([2, 320])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vae encoder input noise is none  use zeros\n",
      "token embedding shape torch.Size([2, 77, 768])\n",
      "torch.Size([2]) tensor([763, 787], device='cuda:0')\n",
      "torch.Size([2, 4, 64, 64]) torch.Size([2, 77, 768]) torch.Size([2, 320])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 12/12 [00:06<00:00,  1.84it/s]\n",
      "Epoch: 100%|| 1/1 [00:06<00:00,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vae encoder input noise is none  use zeros\n",
      "token embedding shape torch.Size([2, 77, 768])\n",
      "torch.Size([2]) tensor([28,  6], device='cuda:0')\n",
      "torch.Size([2, 4, 64, 64]) torch.Size([2, 77, 768]) torch.Size([2, 320])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import StableDiffusion.LoraDataSet\n",
    "import StableDiffusion.DdpmSamplerTorch\n",
    "import StableDiffusion.Utils\n",
    "import importlib\n",
    "importlib.reload(StableDiffusion.DdpmSamplerTorch)\n",
    "importlib.reload(StableDiffusion.LoraDataSet)\n",
    "importlib.reload(StableDiffusion.Utils)\n",
    "from StableDiffusion.Utils import Utils\n",
    "from StableDiffusion.DdpmSamplerTorch import DdpmSamplerTorch\n",
    "from StableDiffusion.LoraDataSet import LoraDataSet\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sampler = DdpmSamplerTorch()\n",
    "sampler.to(device)\n",
    "loraDataSet = LoraDataSet()\n",
    "loraDataLoader = DataLoader(loraDataSet,batch_size=2,\n",
    "                                shuffle=True,num_workers=0,drop_last=True,collate_fn=None)\n",
    "loraDataSet.__len__()\n",
    "img,promtTokens,attentionMask = loraDataSet[0]\n",
    "print(img.shape,promtTokens.shape,attentionMask.shape)\n",
    "\n",
    "vaeEncoder.eval()\n",
    "vaeDecoder.eval()\n",
    "clipEncoder.eval()\n",
    "diffusionProcess.train()\n",
    "EpochNum = 1\n",
    "for epoch in tqdm(range(EpochNum),desc='Epoch'):\n",
    "    for i,data in enumerate(tqdm(loraDataLoader)):\n",
    "        img,promptTokens,attentionMask = data\n",
    "        with torch.no_grad():\n",
    "            latentImg = vaeEncoder(img)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            clipOutputsPositive = clipEncoder(promptTokens,attentionMask)\n",
    "        \n",
    "        \n",
    "        BatchSize,Channel,Height,Width = latentImg.shape\n",
    "\n",
    "        noiseLatent = torch.randn(BatchSize,Channel,Height,Width,dtype=torch.float32,device=device)\n",
    "        timeSteps =torch.randint(0,1000,(BatchSize,),device=device)\n",
    "        print(timeSteps.shape,timeSteps)\n",
    "        latentImgNoised = sampler.addNoiseBatchTrain(latentImg,noiseLatent,timeSteps)\n",
    "        timeEmb320 =Utils.getTimeEmbeddingBatch(timeSteps)\n",
    "        timeEmb320 = sampler.numpy2Tensor(timeEmb320)\n",
    "        print(latentImgNoised.shape,clipOutputsPositive.shape,timeEmb320.shape)\n",
    "        predictedNoise= diffusionProcess(latentImgNoised,clipOutputsPositive,timeEmb320)\n",
    "        with torch.no_grad():        \n",
    "            noisedImg =vaeDecoder(latentImgNoised)\n",
    "            \n",
    "        #Utils.showBatchImage(img[:1])\n",
    "        #Utils.showBatchImage(latentImg[:1,:3])\n",
    "        #Utils.showBatchImage(noisedImg[:1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 512, 512])\n",
      "tensor([[[[ 0.8588,  0.8510,  0.9608,  ...,  0.5608,  0.3176,  0.2078],\n",
      "          [ 0.8902,  0.8745,  0.9686,  ...,  0.4902,  0.3569,  0.2941],\n",
      "          [ 0.8980,  0.8824,  0.9451,  ...,  0.4196,  0.2863,  0.2235],\n",
      "          ...,\n",
      "          [ 0.2157,  0.1608,  0.1373,  ..., -0.3020, -0.2078, -0.2078],\n",
      "          [ 0.2078,  0.1765,  0.1294,  ..., -0.2549, -0.1686, -0.1686],\n",
      "          [ 0.1137,  0.1529,  0.1216,  ..., -0.3020, -0.2392, -0.2471]],\n",
      "\n",
      "         [[ 0.7804,  0.7725,  0.8824,  ...,  0.3882,  0.1608,  0.0510],\n",
      "          [ 0.8118,  0.7961,  0.8902,  ...,  0.3176,  0.2000,  0.1373],\n",
      "          [ 0.8196,  0.8039,  0.8667,  ...,  0.2471,  0.1294,  0.0667],\n",
      "          ...,\n",
      "          [ 0.1529,  0.0980,  0.0745,  ..., -0.2314, -0.1373, -0.1373],\n",
      "          [ 0.1451,  0.1137,  0.0667,  ..., -0.1843, -0.0980, -0.0980],\n",
      "          [ 0.0510,  0.0902,  0.0588,  ..., -0.2314, -0.1686, -0.1765]],\n",
      "\n",
      "         [[ 0.7020,  0.6941,  0.8039,  ...,  0.2078, -0.0275, -0.1373],\n",
      "          [ 0.7333,  0.7176,  0.8118,  ...,  0.1373,  0.0118, -0.0510],\n",
      "          [ 0.7412,  0.7255,  0.7882,  ...,  0.0667, -0.0588, -0.1216],\n",
      "          ...,\n",
      "          [ 0.2549,  0.2000,  0.1765,  ..., -0.0039,  0.0902,  0.0902],\n",
      "          [ 0.2471,  0.2157,  0.1686,  ...,  0.0431,  0.1294,  0.1294],\n",
      "          [ 0.1529,  0.1922,  0.1608,  ..., -0.0039,  0.0588,  0.0510]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3020,  0.2235,  0.1451,  ..., -0.0667, -0.1137, -0.1608],\n",
      "          [ 0.0118,  0.3490,  0.1922,  ..., -0.0588, -0.1137, -0.1529],\n",
      "          [ 0.2784,  0.2471,  0.2471,  ..., -0.0510, -0.0980, -0.1373],\n",
      "          ...,\n",
      "          [-0.7490, -0.7569, -0.7569,  ..., -0.5373, -0.6941, -0.7804],\n",
      "          [-0.7490, -0.7490, -0.7569,  ..., -0.3647, -0.6078, -0.7569],\n",
      "          [-0.7490, -0.7490, -0.7490,  ..., -0.2706, -0.5765, -0.8196]],\n",
      "\n",
      "         [[ 0.1765,  0.0980,  0.0353,  ..., -0.0118, -0.0588, -0.1059],\n",
      "          [-0.1137,  0.2235,  0.0824,  ..., -0.0039, -0.0588, -0.0980],\n",
      "          [ 0.1686,  0.1373,  0.1373,  ...,  0.0039, -0.0431, -0.0824],\n",
      "          ...,\n",
      "          [-0.7569, -0.7647, -0.7647,  ..., -0.5137, -0.6706, -0.7569],\n",
      "          [-0.7569, -0.7569, -0.7647,  ..., -0.3412, -0.5843, -0.7333],\n",
      "          [-0.7569, -0.7569, -0.7569,  ..., -0.2471, -0.5529, -0.7961]],\n",
      "\n",
      "         [[ 0.1765,  0.0980,  0.0275,  ...,  0.0510,  0.0039, -0.0431],\n",
      "          [-0.1137,  0.2235,  0.0745,  ...,  0.0588,  0.0039, -0.0353],\n",
      "          [ 0.1608,  0.1294,  0.1294,  ...,  0.0667,  0.0196, -0.0196],\n",
      "          ...,\n",
      "          [-0.7725, -0.7804, -0.7804,  ..., -0.4431, -0.6000, -0.6863],\n",
      "          [-0.7725, -0.7725, -0.7804,  ..., -0.2863, -0.5294, -0.6784],\n",
      "          [-0.7725, -0.7725, -0.7725,  ..., -0.1922, -0.4980, -0.7412]]]],\n",
      "       device='cuda:0')\n",
      "tensor([[49406,  2467,  7313, 23441,  2467,  2704,   267, 13734,   267,   272,\n",
      "          1611,   267,  5797,   267,  1538,  2225,   267,  1312,   536, 15061,\n",
      "           267, 12386,   267,  2523,   267,  1449,  2225,   267,  2225, 23409,\n",
      "           267,  4919,   267,  1228, 11075,   267,  6066,  3422,   267,  8774,\n",
      "           267,  4658,  9289,   267, 11775,  3575,   267, 12518,  3575,   267,\n",
      "          1746,  6066,  3422,   267, 16157,   267, 12679, 49407, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407, 49407, 49407, 49407],\n",
      "        [49406,  2467,  7313, 23441,  2467,  2704,   267,  3658,   267, 13734,\n",
      "           267,   272,  1611,   267,  5797,   267,  1538,  2225,   267, 12386,\n",
      "           267,  1449,  2225,   267,  4040,   267,  4668,  9861,   267,  1228,\n",
      "         11075,   267,  6164,   267, 10469,   267,  2368,  9861,   267,  1573,\n",
      "           267,  3365,   267, 23291,   267, 11775,  3575,   267, 23291, 12386,\n",
      "           267, 24318,   267,  2307,   267, 16157, 49407, 49407, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407, 49407, 49407, 49407]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]], device='cuda:0')\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([77])\n",
      "torch.Size([77])\n"
     ]
    }
   ],
   "source": [
    "#print(imgBatch.shape)\n",
    "print(img.shape)\n",
    "print(img)\n",
    "print(promptTokens)\n",
    "print(attentionMask)\n",
    "\n",
    "print(img[0].shape)\n",
    "print(promptTokens[0].shape)\n",
    "print(attentionMask[0].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('MyEnvForSD': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "647a70d9ee79436e1ee2ec7ca1cc35c4dc1bf6626b891860add3bc52abf89db9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
